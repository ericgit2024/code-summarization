from unittest.mock import MagicMock
import torch
import torch.nn as nn

class MockModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.config = MagicMock()
        self.peft_config = None
        self.tp_size = None
        self.device = torch.device("cpu")

    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):
        # Return a mock output with loss and logits
        output = MagicMock()
        # Loss must be a scalar tensor
        output.loss = torch.tensor(0.5, requires_grad=True)
        # Logits shape must match expectations, assuming batch_size=1, seq_len=10, vocab_size=100
        output.logits = torch.randn(1, 10, 100, requires_grad=True)
        return output

    def generate(self, **kwargs):
        # Mock generate output
        # Expecting tensor of shape (batch_size, seq_len)
        # Input length was ~10. return something longer.
        return torch.tensor([[1] * 20])

    def save_pretrained(self, *args, **kwargs):
        pass

    def print_trainable_parameters(self):
        print("MOCK: print_trainable_parameters")

    def gradient_checkpointing_enable(self, **kwargs):
        pass

class MockBatchEncoding(dict):
    def to(self, device):
        return self

    @property
    def input_ids(self):
        return self["input_ids"]

    @property
    def attention_mask(self):
        return self["attention_mask"]

def load_gemma_model():
    print("MOCK: Loading Gemma Model...")
    model = MockModel()
    tokenizer = MagicMock()
    
    # Setup mock tokenizer
    tokenizer.eos_token = "<eos>"
    tokenizer.pad_token = "<pad>"
    tokenizer.eos_token_id = 1
    tokenizer.decode.return_value = "### Overview\nThis is a mock summary generated by the Reflective Agent.\n\n### Detailed Logic\nStep 1..."
    
    # Mock tokenizer call to return dict with input_ids and attention_mask
    def mock_tokenize(text, **kwargs):
        # Handle batched input (list of strings)
        if isinstance(text, list):
            batch_size = len(text)
            data = {
                "input_ids": torch.tensor([[1] * 10] * batch_size),
                "attention_mask": torch.tensor([[1] * 10] * batch_size)
            }
        # Handle single string input
        else:
            data = {
                "input_ids": torch.tensor([[1] * 10]),
                "attention_mask": torch.tensor([[1] * 10])
            }

        # Return object with .to() method
        return MockBatchEncoding(data)

    tokenizer.side_effect = mock_tokenize

    # Setup mock model generate
    
    # Fix for PEFT warning: "Already found a peft_config attribute"
    model.peft_config = None
    
    # Fix for Trainer check: "tp_size > 1"
    model.tp_size = None

    return model, tokenizer

def setup_lora(model):
    print("MOCK: Setting up LoRA...")
    return model
