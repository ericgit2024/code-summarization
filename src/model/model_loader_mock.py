from unittest.mock import MagicMock
import torch.nn as nn
import inspect
import torch

class MockModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.config = MagicMock()
        self.config.to_dict.return_value = {}
        # Add layers that PEFT can target
        self.layers = nn.ModuleList([
            nn.ModuleDict({
                'self_attn': nn.ModuleDict({
                    'q_proj': nn.Linear(10, 10),
                    'v_proj': nn.Linear(10, 10),
                    'k_proj': nn.Linear(10, 10),
                    'o_proj': nn.Linear(10, 10),
                    'gate_proj': nn.Linear(10, 10),
                    'up_proj': nn.Linear(10, 10),
                    'down_proj': nn.Linear(10, 10),
                })
            })
        ])
        self.peft_config = {}

    def forward(self, input_ids, attention_mask=None, **kwargs):
        # Define signature explicitly in arguments so inspect.signature works
        output = MagicMock()
        # Return a real tensor for loss to satisfy torch.isnan check
        # Explicitly ensure it's a scalar tensor on the correct device (likely CPU for mock)
        output.loss = torch.tensor(0.5, requires_grad=True)

        # NOTE: If Trainer moves model to GPU, we might get device mismatch if we don't handle it.
        # But MockModel parameters (Linear layers) are on CPU by default.
        # Trainer.train() calls model.to(device).

        # We need to ensure output.loss is on the same device as the model parameters.
        # We can inspect input_ids.device
        if hasattr(input_ids, 'device'):
             output.loss = output.loss.to(input_ids.device)

        output.logits = torch.randn(1, 1, 10, requires_grad=True)
        if hasattr(input_ids, 'device'):
             output.logits = output.logits.to(input_ids.device)

        return output

    def save_pretrained(self, *args, **kwargs):
        pass

    def get_input_embeddings(self):
        return MagicMock()

    def prepare_inputs_for_generation(self, *args, **kwargs):
        return {}

    def gradient_checkpointing_enable(self, **kwargs):
        pass

def load_gemma_model():
    print("MOCK: Loading Gemma Model (Robust Mock)...")
    model = MockModel()
    tokenizer = MagicMock()
    
    # Setup mock tokenizer
    tokenizer.eos_token = "<eos>"
    tokenizer.pad_token = "<pad>"
    tokenizer.eos_token_id = 1
    tokenizer.pad_token_id = 0
    tokenizer.decode.return_value = "This is a mock summary generated by the Reflective Agent."
    
    def mock_tokenize(text, **kwargs):
        if isinstance(text, list):
            # Batch mode
            batch_size = len(text)
            return {
                "input_ids": [[1, 2, 3]] * batch_size,
                "attention_mask": [[1, 1, 1]] * batch_size
            }
        else:
            # Single mode
            return {"input_ids": [1, 2, 3], "attention_mask": [1, 1, 1]}

    tokenizer.side_effect = mock_tokenize
    tokenizer.__call__ = mock_tokenize
    
    return model, tokenizer

def setup_lora(model, model_dir=None):
    print("MOCK: Setting up LoRA...")
    return model
