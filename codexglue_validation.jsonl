{"code": "def intersection(self, other):\n        \"\"\"\n        Return the intersection of this RDD and another one. The output will\n        not contain any duplicate elements, even if the input RDDs did.\n\n        .. note:: This method performs a shuffle internally.\n\n        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n        >>> rdd1.intersection(rdd2).collect()\n        [1, 2, 3]\n        \"\"\"\n        return self.map(lambda v: (v, None)) \\\n            .cogroup(other.map(lambda v: (v, None))) \\\n            .filter(lambda k_vs: all(k_vs[1])) \\\n            .keys()", "summary": "Return the intersection of this RDD and another one. The output will\n        not contain any duplicate elements, even if the input RDDs did.\n\n        .. note:: This method performs a shuffle internally.\n\n        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n        >>> rdd1.intersection(rdd2).collect()\n        [1, 2, 3]", "name": "intersection", "complexity": 1, "num_dependencies": 1, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L576-L591", "code_lines": 14, "summary_length": 376}
{"code": "def asDict(self, recursive=False):\n        \"\"\"\n        Return as an dict\n\n        :param recursive: turns the nested Row as dict (default: False).\n\n        >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n        True\n        >>> row = Row(key=1, value=Row(name='a', age=2))\n        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}\n        True\n        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n        True\n        \"\"\"\n        if not hasattr(self, \"__fields__\"):\n            raise TypeError(\"Cannot convert a Row class into dict\")\n\n        if recursive:\n            def conv(obj):\n                if isinstance(obj, Row):\n                    return obj.asDict(True)\n                elif isinstance(obj, list):\n                    return [conv(o) for o in obj]\n                elif isinstance(obj, dict):\n                    return dict((k, conv(v)) for k, v in obj.items())\n                else:\n                    return obj\n            return dict(zip(self.__fields__, (conv(o) for o in self)))\n        else:\n            return dict(zip(self.__fields__, self))", "summary": "Return as an dict\n\n        :param recursive: turns the nested Row as dict (default: False).\n\n        >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n        True\n        >>> row = Row(key=1, value=Row(name='a', age=2))\n        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}\n        True\n        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n        True", "name": "asDict", "complexity": 6, "num_dependencies": 23, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1463-L1492", "code_lines": 27, "summary_length": 414}
{"code": "def _recursive_merged_items(self, index):\n        \"\"\"\n        merge the partitioned items and return the as iterator\n\n        If one partition can not be fit in memory, then them will be\n        partitioned and merged recursively.\n        \"\"\"\n        subdirs = [os.path.join(d, \"parts\", str(index)) for d in self.localdirs]\n        m = ExternalMerger(self.agg, self.memory_limit, self.serializer, subdirs,\n                           self.scale * self.partitions, self.partitions, self.batch)\n        m.pdata = [{} for _ in range(self.partitions)]\n        limit = self._next_limit()\n\n        for j in range(self.spills):\n            path = self._get_spill_dir(j)\n            p = os.path.join(path, str(index))\n            with open(p, 'rb') as f:\n                m.mergeCombiners(self.serializer.load_stream(f), 0)\n\n            if get_used_memory() > limit:\n                m._spill()\n                limit = self._next_limit()\n\n        return m._external_items()", "summary": "merge the partitioned items and return the as iterator\n\n        If one partition can not be fit in memory, then them will be\n        partitioned and merged recursively.", "name": "_recursive_merged_items", "complexity": 3, "num_dependencies": 16, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L386-L409", "code_lines": 20, "summary_length": 168}
{"code": "def unpersist(self, blocking=False):\n        \"\"\"\n        Mark the RDD as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. versionchanged:: 3.0.0\n           Added optional argument `blocking` to specify whether to block until all\n           blocks are deleted.\n        \"\"\"\n        self.is_cached = False\n        self._jrdd.unpersist(blocking)\n        return self", "summary": "Mark the RDD as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. versionchanged:: 3.0.0\n           Added optional argument `blocking` to specify whether to block until all\n           blocks are deleted.", "name": "unpersist", "complexity": 1, "num_dependencies": 1, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L247-L258", "code_lines": 11, "summary_length": 240}
{"code": "def mapPartitionsWithSplit(self, f, preservesPartitioning=False):\n        \"\"\"\n        Deprecated: use mapPartitionsWithIndex instead.\n\n        Return a new RDD by applying a function to each partition of this RDD,\n        while tracking the index of the original partition.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n        >>> def f(splitIndex, iterator): yield splitIndex\n        >>> rdd.mapPartitionsWithSplit(f).sum()\n        6\n        \"\"\"\n        warnings.warn(\"mapPartitionsWithSplit is deprecated; \"\n                      \"use mapPartitionsWithIndex instead\", DeprecationWarning, stacklevel=2)\n        return self.mapPartitionsWithIndex(f, preservesPartitioning)", "summary": "Deprecated: use mapPartitionsWithIndex instead.\n\n        Return a new RDD by applying a function to each partition of this RDD,\n        while tracking the index of the original partition.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n        >>> def f(splitIndex, iterator): yield splitIndex\n        >>> rdd.mapPartitionsWithSplit(f).sum()\n        6", "name": "mapPartitionsWithSplit", "complexity": 1, "num_dependencies": 2, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L369-L383", "code_lines": 13, "summary_length": 354}
{"code": "def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None):\n        \"\"\"\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n        converted for output using either user specified converters or, by default,\n        L{org.apache.spark.api.python.JavaToWritableConverter}.\n\n        :param conf: Hadoop job configuration, passed in as a dict\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)\n        \"\"\"\n        jconf = self.ctx._dictToJavaMap(conf)\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,\n                                                    keyConverter, valueConverter, True)", "summary": "Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n        converted for output using either user specified converters or, by default,\n        L{org.apache.spark.api.python.JavaToWritableConverter}.\n\n        :param conf: Hadoop job configuration, passed in as a dict\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)", "name": "saveAsNewAPIHadoopDataset", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1398-L1412", "code_lines": 14, "summary_length": 485}
{"code": "def take(self, num):\n        \"\"\"\n        Take the first num elements of the RDD.\n\n        It works by first scanning one partition, and use the results from\n        that partition to estimate the number of additional partitions needed\n        to satisfy the limit.\n\n        Translated from the Scala implementation in RDD#take().\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n        [2, 3]\n        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n        [2, 3, 4, 5, 6]\n        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n        [91, 92, 93]\n        \"\"\"\n        items = []\n        totalParts = self.getNumPartitions()\n        partsScanned = 0\n\n        while len(items) < num and partsScanned < totalParts:\n            # The number of partitions to try in this iteration.\n            # It is ok for this number to be greater than totalParts because\n            # we actually cap it at totalParts in runJob.\n            numPartsToTry = 1\n            if partsScanned > 0:\n                # If we didn't find any rows after the previous iteration,\n                # quadruple and retry.  Otherwise, interpolate the number of\n                # partitions we need to try, but overestimate it by 50%.\n                # We also cap the estimation in the end.\n                if len(items) == 0:\n                    numPartsToTry = partsScanned * 4\n                else:\n                    # the first parameter of max is >=1 whenever partsScanned >= 2\n                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned\n                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)\n\n            left = num - len(items)\n\n            def takeUpToNumLeft(iterator):\n                iterator = iter(iterator)\n                taken = 0\n                while taken < left:\n                    try:\n                        yield next(iterator)\n                    except StopIteration:\n                        return\n                    taken += 1\n\n            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))\n            res = self.context.runJob(self, takeUpToNumLeft, p)\n\n            items += res\n            partsScanned += numPartsToTry\n\n        return items[:num]", "summary": "Take the first num elements of the RDD.\n\n        It works by first scanning one partition, and use the results from\n        that partition to estimate the number of additional partitions needed\n        to satisfy the limit.\n\n        Translated from the Scala implementation in RDD#take().\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n        [2, 3]\n        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n        [2, 3, 4, 5, 6]\n        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n        [91, 92, 93]", "name": "take", "complexity": 7, "num_dependencies": 15, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1308-L1367", "code_lines": 50, "summary_length": 702}
{"code": "def _infer_schema(row, names=None):\n    \"\"\"Infer the schema from dict/namedtuple/object\"\"\"\n    if isinstance(row, dict):\n        items = sorted(row.items())\n\n    elif isinstance(row, (tuple, list)):\n        if hasattr(row, \"__fields__\"):  # Row\n            items = zip(row.__fields__, tuple(row))\n        elif hasattr(row, \"_fields\"):  # namedtuple\n            items = zip(row._fields, tuple(row))\n        else:\n            if names is None:\n                names = ['_%d' % i for i in range(1, len(row) + 1)]\n            elif len(names) < len(row):\n                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))\n            items = zip(names, row)\n\n    elif hasattr(row, \"__dict__\"):  # object\n        items = sorted(row.__dict__.items())\n\n    else:\n        raise TypeError(\"Can not infer schema for type: %s\" % type(row))\n\n    fields = [StructField(k, _infer_type(v), True) for k, v in items]\n    return StructType(fields)", "summary": "Infer the schema from dict/namedtuple/object", "name": "_infer_schema", "complexity": 8, "num_dependencies": 27, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1041-L1065", "code_lines": 21, "summary_length": 44}
{"code": "def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' objects of found face locations\n    \"\"\"\n    if model == \"cnn\":\n        return cnn_face_detector(img, number_of_times_to_upsample)\n    else:\n        return face_detector(img, number_of_times_to_upsample)", "summary": "Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' objects of found face locations", "name": "_raw_face_locations", "complexity": 2, "num_dependencies": 2, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L92-L105", "code_lines": 13, "summary_length": 530}
{"code": "def foreachPartition(self, f):\n        \"\"\"\n        Applies a function to each partition of this RDD.\n\n        >>> def f(iterator):\n        ...     for x in iterator:\n        ...          print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n        \"\"\"\n        def func(it):\n            r = f(it)\n            try:\n                return iter(r)\n            except TypeError:\n                return iter([])\n        self.mapPartitions(func).count()", "summary": "Applies a function to each partition of this RDD.\n\n        >>> def f(iterator):\n        ...     for x in iterator:\n        ...          print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)", "name": "foreachPartition", "complexity": 2, "num_dependencies": 7, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L793-L808", "code_lines": 15, "summary_length": 208}
