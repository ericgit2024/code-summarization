{"code": "def _compute_vulnerability_score(risk_level, impact_factor):\n    return risk_level * impact_factor * 10\n\ndef _alert_critical_vulnerability(score, threshold=70):\n    return score >= threshold\n\ndef assess_security_risk(risk_input, impact_input):\n    # Dependency 1: Calculate the final score\n    security_score = _compute_vulnerability_score(risk_input, impact_input)\n    \n    # Dependency 2: Check if the score is above the critical threshold\n    is_critical = _alert_critical_vulnerability(security_score)\n    \n    return {'score': security_score, 'critical_alert': is_critical}", "summary": "Assesses a security risk by first calculating a **`security_score`** based on risk level and impact factor using **`_compute_vulnerability_score`** (defined in **`assess_security_risk.py`**), and then checking if the result exceeds a critical threshold using **`_alert_critical_vulnerability`** (defined in **`assess_security_risk.py`**)."}
{"code": "def _generate_otp_code(length=6):\n    import random\n    return ''.join(random.choices('0123456789', k=length))\n\ndef _send_sms_notification(phone_number, otp):\n    # Mock SMS gateway call\n    print(f\"SMS sent to {phone_number} with OTP: {otp}\")\n\ndef issue_one_time_password(target_phone):\n    # Dependency 1: Generate the random OTP code\n    otp = _generate_otp_code()\n    \n    # Dependency 2: Send the OTP via SMS\n    _send_sms_notification(target_phone, otp)\n    \n    return otp", "summary": "Generates a random One-Time Password (OTP) and sends it to the **`target_phone`**. The function uses **`_generate_otp_code`** (defined in **`issue_one_time_password.py`**) (relying on the **`random`** library) to create the code, and then calls **`_send_sms_notification`** (defined in **`issue_one_time_password.py`**) (a mock) to deliver it."}
{"code": "def _find_max_value(data_list):\n    return max(data_list) if data_list else None\n\ndef _normalize_value(value, max_val):\n    if max_val is None or max_val == 0: return 0\n    return value / max_val\n\ndef normalize_dataset(data_series):\n    # Dependency 1: Find the maximum value in the series\n    max_val = _find_max_value(data_series)\n    \n    # Dependency 2: Normalize each value against the maximum\n    normalized_series = [_normalize_value(val, max_val) for val in data_series]\n    \n    return normalized_series", "summary": "Normalizes a **`data_series`** by scaling all values between 0 and 1 (Min-Max normalization). It first calls **`_find_max_value`** (defined in **`normalize_dataset.py`**) to identify the maximum element, and then uses **`_normalize_value`** (defined in **`normalize_dataset.py`**) to transform each element using that maximum."}
{"code": "def _extract_host_port(url):\n    from urllib.parse import urlparse\n    parsed = urlparse(url)\n    return parsed.hostname, parsed.port\n\ndef _check_default_port(port, default=80):\n    return port is None or port == default\n\ndef analyze_url_endpoint(input_url):\n    # Dependency 1: Parse the URL to get host and port\n    host, port = _extract_host_port(input_url)\n    \n    # Dependency 2: Determine if the extracted port is the default HTTP port\n    uses_default_port = _check_default_port(port)\n    \n    return {'hostname': host, 'port': port, 'is_default_http': uses_default_port}", "summary": "Analyzes an **`input_url`** to extract the hostname, port, and determine if the port is the default HTTP port (80). It calls **`_extract_host_port`** (defined in **`analyze_url_endpoint.py`**) (relying on **`urllib.parse`**) to parse the URL, and then uses **`_check_default_port`** (defined in **`analyze_url_endpoint.py`**) to check the port value."}
{"code": "def _process_image_resize(image_path, new_width, new_height):\n    # Mock image processing library call\n    return f\"Resized {image_path} to {new_width}x{new_height}\"\n\ndef _log_transformation(operation):\n    print(f\"[IMG_TRANSFORM] {operation}\")\n\ndef resize_and_log_image(file_path, w, h):\n    # Dependency 1: Perform the image resizing operation\n    result = _process_image_resize(file_path, w, h)\n    \n    # Dependency 2: Log the result of the operation\n    _log_transformation(result)\n    \n    return result", "summary": "Resizes an image specified by **`file_path`** to new dimensions and logs the successful operation. It calls the mock function **`_process_image_resize`** (defined in **`resize_and_log_image.py`**) to perform the transformation, and then uses **`_log_transformation`** (defined in **`resize_and_log_image.py`**) to record the action."}
{"code": "def _reverse_string(text):\n    return text[::-1]\n\ndef _count_occurrences(main_string, sub_string):\n    return main_string.count(sub_string)\n\ndef analyze_reversed_string(input_text, sub_text):\n    # Dependency 1: Reverse the main input string\n    reversed_text = _reverse_string(input_text)\n    \n    # Dependency 2: Count how many times the sub_text appears in the reversed string\n    count = _count_occurrences(reversed_text, sub_text)\n    \n    return {'reversed': reversed_text, 'count_in_reversed': count}", "summary": "Reverses the **`input_text`** and then counts the number of occurrences of **`sub_text`** within the reversed result. This process relies on **`_reverse_string`** (defined in **`analyze_reversed_string.py`**) to perform the reversal, followed by **`_count_occurrences`** (defined in **`analyze_reversed_string.py`**) for the count."}
{"code": "def _get_system_platform():\n    import platform\n    return platform.system()\n\ndef _check_for_linux(system_name):\n    return system_name == 'Linux'\n\ndef check_current_os():\n    # Dependency 1: Get the underlying operating system name\n    os_name = _get_system_platform()\n    \n    # Dependency 2: Check if the system is Linux\n    is_linux = _check_for_linux(os_name)\n    \n    return {'os_name': os_name, 'is_linux': is_linux}", "summary": "Determines the current operating system name and checks specifically if the system is Linux. It uses **`_get_system_platform`** (defined in **`check_current_os.py`**) (relying on the external **`platform`** library) to retrieve the OS name, and then calls **`_check_for_linux`** (defined in **`check_current_os.py`**) for the specific check."}
{"code": "def _fetch_exchange_rate(base, target):\n    # Mock external API call for rates\n    if base == 'USD' and target == 'EUR': return 0.92\n    return 1.0\n\ndef _apply_rate(amount, rate):\n    return amount * rate\n\ndef convert_currency(amount_usd, target_currency):\n    # Dependency 1: Fetch the conversion rate from a mock API\n    rate = _fetch_exchange_rate('USD', target_currency)\n    \n    # Dependency 2: Apply the rate to the amount\n    converted_amount = _apply_rate(amount_usd, rate)\n    \n    return converted_amount", "summary": "Converts an **`amount_usd`** to a **`target_currency`** by fetching an exchange rate and applying it. The function relies on **`_fetch_exchange_rate`** (defined in **`convert_currency.py`**) to retrieve the conversion factor and **`_apply_rate`** (defined in **`convert_currency.py`**) to perform the multiplication."}
{"code": "def _check_type_equality(a, b):\n    return type(a) is type(b)\n\ndef _is_type_primitive(data_type):\n    # Checks if type is int, float, str, or bool\n    return data_type in [int, float, str, bool]\n\ndef analyze_type_and_primitive(value1, value2):\n    # Dependency 1: Check if the types of the two values are the same\n    types_match = _check_type_equality(value1, value2)\n    \n    # Dependency 2: Check if the type of the first value is a primitive\n    is_primitive = _is_type_primitive(type(value1))\n    \n    return {'types_match': types_match, 'is_primitive': is_primitive}", "summary": "Analyzes two input values to determine if their data types are identical and whether the first value's type is considered a primitive type (int, float, str, bool). It uses **`_check_type_equality`** (defined in **`analyze_type_and_primitive.py`**) for the comparison and **`_is_type_primitive`** (defined in **`analyze_type_and_primitive.py`**) for the classification."}
{"code": "def _load_template(template_name):\n    # Mock file read\n    return f\"Hello {{user}}, your balance is {{balance}}.\"\n\ndef _render_template(template_string, context):\n    for key, value in context.items():\n        template_string = template_string.replace('{{' + key + '}}', str(value))\n    return template_string\n\ndef generate_email_body(template_id, data_context):\n    # Dependency 1: Load the raw template string\n    template = _load_template(template_id)\n    \n    # Dependency 2: Populate the template with context data\n    rendered_body = _render_template(template, data_context)\n    \n    return rendered_body", "summary": "Generates a final email body by first loading a template string specified by **`template_id`** and then substituting placeholders with values from the **`data_context`** dictionary. The process relies on **`_load_template`** (defined in **`generate_email_body.py`**) followed by **`_render_template`** (defined in **`generate_email_body.py`**)."}
{"code": "def _compute_geometric_mean(numbers):\n    import numpy\n    # Ensure positive numbers for geometric mean\n    positive_numbers = [n for n in numbers if n > 0]\n    if not positive_numbers: return 0.0\n    # External numpy dependency for the calculation\n    return numpy.prod(positive_numbers)**(1/len(positive_numbers))\n\ndef _is_list_homogeneous(numbers):\n    return len(set(numbers)) == 1\n\ndef analyze_number_distribution(data_list):\n    # Dependency 1: Calculate the geometric mean\n    g_mean = _compute_geometric_mean(data_list)\n    \n    # Dependency 2: Check if all numbers in the list are the same\n    is_same = _is_list_homogeneous(data_list)\n    \n    return {'geometric_mean': g_mean, 'is_homogeneous': is_same}", "summary": "Analyzes a list of numbers by calculating their Geometric Mean and determining if the list is homogeneous (all elements are equal). It uses **`_compute_geometric_mean`** (defined in **`analyze_number_distribution.py`**) (requiring the external **`numpy`** library for the product function) and **`_is_list_homogeneous`** (defined in **`analyze_number_distribution.py`**) for the equality check."}
{"code": "def _encrypt_data_aes(plaintext, key):\n    # Mock AES encryption\n    return f\"[AES]{key}:{plaintext[::-1]}\"\n\ndef _store_in_vault(encrypted_data):\n    # Mock secure storage service\n    return f\"Stored successfully: {encrypted_data[:15]}...\"\n\ndef secure_data_storage(data_payload, encryption_key):\n    # Dependency 1: Encrypt the sensitive payload\n    encrypted_payload = _encrypt_data_aes(data_payload, encryption_key)\n    \n    # Dependency 2: Store the encrypted result in a mock vault\n    storage_result = _store_in_vault(encrypted_payload)\n    \n    return storage_result", "summary": "Encrypts a **`data_payload`** using a provided **`encryption_key`** and then stores the resulting ciphertext in a mock secure vault. The function relies on **`_encrypt_data_aes`** (defined in **`secure_data_storage.py`**) for the encryption and **`_store_in_vault`** (defined in **`secure_data_storage.py`**) for the storage mock."}
{"code": "def _get_list_difference(list1, list2):\n    set1 = set(list1)\n    set2 = set(list2)\n    # Elements in list1 but not in list2\n    return list(set1 - set2)\n\ndef _log_difference_count(count):\n    print(f\"Difference calculated. Unique elements found: {count}\")\n\ndef compare_two_lists(list_a, list_b):\n    # Dependency 1: Find elements unique to list_a\n    unique_elements = _get_list_difference(list_a, list_b)\n    \n    # Dependency 2: Log the count of unique elements\n    _log_difference_count(len(unique_elements))\n    \n    return unique_elements", "summary": "Compares two lists to find and return the elements present in **`list_a`** but not in **`list_b`**, and also logs the count of these unique elements. It uses **`_get_list_difference`** (defined in **`compare_two_lists.py`**) for the set difference calculation and **`_log_difference_count`** (defined in **`compare_two_lists.py`**) to report the count."}
{"code": "def _generate_bar_chart_data(x_labels, y_values):\n    # Mock function to generate data structure for plotting\n    return {'type': 'bar', 'x': x_labels, 'y': y_values}\n\ndef _send_data_to_plotting_service(plot_data):\n    # Mock external plotting service call\n    print(f\"Plotting service received data for {plot_data['type']} chart.\")\n\ndef create_and_render_chart(labels, values):\n    # Dependency 1: Generate the structured data needed for the chart\n    chart_data = _generate_bar_chart_data(labels, values)\n    \n    # Dependency 2: Send the structured data to the plotting service\n    _send_data_to_plotting_service(chart_data)\n    \n    return chart_data", "summary": "Creates structured data for a bar chart using provided **`labels`** and **`values`** and then sends this data to a mock external plotting service for rendering. The function calls **`_generate_bar_chart_data`** (defined in **`create_and_render_chart.py`**) to organize the input, followed by **`_send_data_to_plotting_service`** (defined in **`create_and_render_chart.py`**) for the mock API transmission."}
{"code": "def _validate_json_schema(data, schema):\n    import jsonschema\n    try:\n        jsonschema.validate(instance=data, schema=schema)\n        return True\n    except jsonschema.exceptions.ValidationError:\n        return False\n\ndef _log_validation_result(is_valid):\n    print(f\"Schema validation result: {'SUCCESS' if is_valid else 'FAILURE'}\")\n\ndef process_data_with_schema(payload, validation_schema):\n    # Dependency 1: Validate the payload against the JSON schema\n    is_valid = _validate_json_schema(payload, validation_schema)\n    \n    # Dependency 2: Log the outcome of the validation\n    _log_validation_result(is_valid)\n    \n    return is_valid", "summary": "Validates a data **`payload`** against a **`validation_schema`** and logs the outcome. It relies on **`_validate_json_schema`** (defined in **`process_data_with_schema.py`**) (which uses the external **`jsonschema`** library) to perform the structural check, and then calls **`_log_validation_result`** (defined in **`process_data_with_schema.py`**)."}
{"code": "def _calculate_moving_median(data, window):\n    # Mock implementation of a moving median\n    import statistics\n    if len(data) < window: return []\n    return [statistics.median(data[i:i+window]) for i in range(len(data) - window + 1)]\n\ndef _filter_outliers_by_median(data, median_series):\n    # Mock outlier check against the moving median\n    return [d for d in data if abs(d - statistics.median(median_series)) < 5]\n\ndef smooth_and_filter_data(series, window_size):\n    import statistics\n    # Dependency 1: Calculate the moving median to smooth the series\n    m_median = _calculate_moving_median(series, window_size)\n    \n    # Dependency 2: Filter the original series using the calculated median trend\n    filtered_series = _filter_outliers_by_median(series, m_median)\n    \n    return filtered_series", "summary": "Smooths a data **`series`** by calculating a moving median and then uses this trend to filter outliers from the original data. It uses **`_calculate_moving_median`** (defined in **`smooth_and_filter_data.py`**) (requiring the **`statistics`** library) for smoothing and **`_filter_outliers_by_median`** (defined in **`smooth_and_filter_data.py`**) (also using **`statistics`**) for cleaning the data."}
{"code": "def _get_config_value(key, defaults):\n    import os\n    # Environment variable overrides defaults\n    return os.environ.get(key) or defaults.get(key)\n\ndef _check_if_enabled(config_value):\n    return str(config_value).lower() == 'true'\n\ndef check_feature_flag_status(flag_name, default_config):\n    # Dependency 1: Retrieve the configuration value for the flag\n    flag_value = _get_config_value(flag_name, default_config)\n    \n    # Dependency 2: Determine if the value translates to 'enabled'\n    is_enabled = _check_if_enabled(flag_value)\n    \n    return is_enabled", "summary": "Checks the status of a feature flag by first attempting to load the value from environment variables, falling back to **`default_config`**. It uses **`_get_config_value`** (defined in **`check_feature_flag_status.py`**) (relying on the **`os`** library) for retrieval and **`_check_if_enabled`** (defined in **`check_feature_flag_status.py`**) for the boolean interpretation."}
{"code": "def _convert_to_roman(number):\n    # Mock simple conversion\n    if number == 1: return 'I'\n    if number == 5: return 'V'\n    return str(number)\n\ndef _append_symbol(roman_numeral, symbol='.'):\n    return f'{roman_numeral}{symbol}'\n\ndef format_number_as_roman_and_symbol(integer_input):\n    # Dependency 1: Convert the integer to its Roman numeral representation\n    roman = _convert_to_roman(integer_input)\n    \n    # Dependency 2: Append a standard symbol to the result\n    final_format = _append_symbol(roman)\n    \n    return final_format", "summary": "Formats an **`integer_input`** by converting it to a Roman numeral representation (mock) and then appending a standard symbol. The process involves calling **`_convert_to_roman`** (defined in **`format_number_as_roman_and_symbol.py`**) followed by **`_append_symbol`** (defined in **`format_number_as_roman_and_symbol.py`**)."}
{"code": "def _get_time_of_day(hour):\n    if 6 <= hour < 12: return 'Morning'\n    if 12 <= hour < 18: return 'Afternoon'\n    return 'Evening'\n\ndef _generate_greeting(time_of_day, user_name):\n    return f\"Good {time_of_day}, {user_name}!\"\n\ndef create_personalized_greeting(current_hour, username):\n    # Dependency 1: Determine the time of day based on the hour\n    time_of_day = _get_time_of_day(current_hour)\n    \n    # Dependency 2: Generate the greeting string\n    greeting = _generate_greeting(time_of_day, username)\n    \n    return greeting", "summary": "Creates a personalized greeting string based on the **`current_hour`** and **`username`**. It uses **`_get_time_of_day`** (defined in **`create_personalized_greeting.py`**) to classify the hour, and then passes the classification and username to **`_generate_greeting`** (defined in **`create_personalized_greeting.py`**)."}
{"code": "def _tokenize_by_regex(text, pattern=r'\\w+'):\n    import re\n    return re.findall(pattern, text.lower())\n\ndef _count_token_types(tokens):\n    return len(set(tokens))\n\ndef measure_vocabulary_size(document_text):\n    # Dependency 1: Extract all tokens using a regex pattern\n    tokens = _tokenize_by_regex(document_text)\n    \n    # Dependency 2: Count the number of unique tokens (vocabulary size)\n    vocab_size = _count_token_types(tokens)\n    \n    return {'total_tokens': len(tokens), 'vocabulary_size': vocab_size}", "summary": "Measures the vocabulary size (unique word count) of a **`document_text`** by first extracting all word tokens using a regex pattern, and then counting the unique elements. It relies on **`_tokenize_by_regex`** (defined in **`measure_vocabulary_size.py`**) (which uses the **`re`** library) and **`_count_token_types`** (defined in **`measure_vocabulary_size.py`**)."}
{"code": "def _fetch_record_from_cache(record_key):\n    # Mock cache lookup\n    return {'key': record_key, 'data': 'cached_value', 'ttl': 300}\n\ndef _check_cache_validity(record):\n    # Mock check: always valid for this example\n    return record is not None\n\ndef retrieve_user_record(user_identifier):\n    # Dependency 1: Attempt to fetch the record from the cache\n    cached_record = _fetch_record_from_cache(user_identifier)\n    \n    # Dependency 2: Check the validity/existence of the cache entry\n    is_valid = _check_cache_validity(cached_record)\n    \n    return {'data': cached_record, 'is_valid': is_valid}", "summary": "Retrieves a user record by attempting a lookup in a mock cache and then checking the validity of the retrieved entry. The function calls **`_fetch_record_from_cache`** (defined in **`retrieve_user_record.py`**) for data retrieval, followed by **`_check_cache_validity`** (defined in **`retrieve_user_record.py`**)."}
{"code": "def _encode_to_base64(data_bytes):\n    import base64\n    return base64.b64encode(data_bytes).decode('utf-8')\n\ndef _wrap_in_mime_format(base64_string, mime_type='text/plain'):\n    return f\"data:{mime_type};base64,{base64_string}\"\n\ndef format_data_uri(raw_bytes, data_mime_type):\n    # Dependency 1: Encode the raw data bytes into a Base64 string\n    encoded_string = _encode_to_base64(raw_bytes)\n    \n    # Dependency 2: Wrap the Base64 string in the complete Data URI format\n    data_uri = _wrap_in_mime_format(encoded_string, data_mime_type)\n    \n    return data_uri", "summary": "Formats raw bytes into a Data URI string by first encoding the bytes into a Base64 string and then prepending the MIME type and Data URI header. It uses **`_encode_to_base64`** (defined in **`format_data_uri.py`**) (relying on the **`base64`** library) and **`_wrap_in_mime_format`** (defined in **`format_data_uri.py`**) for string assembly."}
{"code": "def _get_random_elements(data_list, count):\n    import random\n    return random.sample(data_list, count)\n\ndef _sort_alphabetically(element_list):\n    return sorted(element_list)\n\ndef select_and_sort_sample(source_list, sample_size):\n    # Dependency 1: Select a random sample of elements\n    random_sample = _get_random_elements(source_list, sample_size)\n    \n    # Dependency 2: Sort the selected sample alphabetically\n    sorted_sample = _sort_alphabetically(random_sample)\n    \n    return sorted_sample", "summary": "Selects a random sample of elements from a **`source_list`** and then returns the sample sorted alphabetically. It relies on **`_get_random_elements`** (defined in **`select_and_sort_sample.py`**) (which uses the **`random`** library) for selection and **`_sort_alphabetically`** (defined in **`select_and_sort_sample.py`**) for ordering."}
{"code": "def _calculate_running_total(numbers):\n    running_sum = 0\n    results = []\n    for num in numbers:\n        running_sum += num\n        results.append(running_sum)\n    return results\n\ndef _get_final_sum(running_totals):\n    return running_totals[-1] if running_totals else 0\n\ndef compute_cumulative_sum(input_data):\n    # Dependency 1: Calculate the running total for the entire list\n    cumulative_series = _calculate_running_total(input_data)\n    \n    # Dependency 2: Extract the final sum (last element of the running total)\n    final_sum = _get_final_sum(cumulative_series)\n    \n    return {'series': cumulative_series, 'total': final_sum}", "summary": "Computes the cumulative sum series for a list of **`input_data`** and extracts the final total. The function uses **`_calculate_running_total`** (defined in **`compute_cumulative_sum.py`**) to generate the series and **`_get_final_sum`** (defined in **`compute_cumulative_sum.py`**) to retrieve the final sum from that series."}
{"code": "def _generate_report_title(entity_name, date_str):\n    return f\"Performance Report for {entity_name} ({date_str})\"\n\ndef _render_to_pdf(title, content):\n    # Mock PDF generation library call\n    return f\"PDF created: {title}\"\n\ndef generate_pdf_report(entity, current_date, report_content):\n    # Dependency 1: Create a standardized title for the report\n    report_title = _generate_report_title(entity, current_date)\n    \n    # Dependency 2: Render the content into a PDF document using the title\n    pdf_output = _render_to_pdf(report_title, report_content)\n    \n    return pdf_output", "summary": "Generates a PDF report by first creating a standardized title using **`_generate_report_title`** (defined in **`generate_pdf_report.py`**) and then rendering the **`report_content`** into a mock PDF document using the generated title via **`_render_to_pdf`** (defined in **`generate_pdf_report.py`**)."}
{"code": "def _load_data_from_remote(api_url):\n    # Mock HTTP request to fetch JSON\n    return [{'id': 1, 'active': True}, {'id': 2, 'active': False}]\n\ndef _filter_by_key_value(data_list, key, value):\n    return [item for item in data_list if item.get(key) == value]\n\ndef fetch_and_filter_active_records(api_endpoint, filter_key, filter_value=True):\n    # Dependency 1: Load the raw data from the API endpoint\n    raw_data = _load_data_from_remote(api_endpoint)\n    \n    # Dependency 2: Filter the data based on a key-value pair\n    filtered_data = _filter_by_key_value(raw_data, filter_key, filter_value)\n    \n    return filtered_data", "summary": "Fetches a raw data list from an **`api_endpoint`** mock and then filters the resulting records to include only those matching a specific **`filter_key`** and **`filter_value`**. The function calls **`_load_data_from_remote`** (defined in **`fetch_and_filter_active_records.py`**) for data acquisition, followed by **`_filter_by_key_value`** (defined in **`fetch_and_filter_active_records.py`**)."}
{"code": "def _calculate_sha1(input_string):\n    import hashlib\n    return hashlib.sha1(input_string.encode()).hexdigest()\n\ndef _truncate_hash(full_hash, length=8):\n    return full_hash[:length]\n\ndef generate_short_content_id(content_data):\n    # Dependency 1: Calculate the full SHA1 hash of the content\n    full_hash = _calculate_sha1(content_data)\n    \n    # Dependency 2: Truncate the full hash to create a short ID\n    short_id = _truncate_hash(full_hash)\n    \n    return short_id", "summary": "Generates a short content identifier by first calculating the full SHA1 hash of the **`content_data`** and then truncating the result to 8 characters. It uses **`_calculate_sha1`** (defined in **`generate_short_content_id.py`**) (relying on the **`hashlib`** library) and **`_truncate_hash`** (defined in **`generate_short_content_id.py`**)."}
{"code": "def _validate_list_contains_only_integers(input_list):\n    return all(isinstance(x, int) for x in input_list)\n\ndef _calculate_sum_and_count(input_list):\n    return sum(input_list), len(input_list)\n\ndef analyze_integer_list(data):\n    # Dependency 1: Validate that all elements are integers\n    if not _validate_list_contains_only_integers(data):\n        raise TypeError(\"List must contain only integers.\")\n        \n    # Dependency 2: Calculate the sum and count\n    total_sum, count = _calculate_sum_and_count(data)\n    \n    return {'sum': total_sum, 'count': count}", "summary": "Analyzes a list by first ensuring all elements are integers and then calculating the total sum and the element count. The function uses **`_validate_list_contains_only_integers`** (defined in **`analyze_integer_list.py`**) for type checking and **`_calculate_sum_and_count`** (defined in **`analyze_integer_list.py`**) for metric generation."}
{"code": "def _normalize_path(path):\n    import os\n    return os.path.normpath(path)\n\ndef _check_if_absolute(normalized_path):\n    import os\n    return os.path.isabs(normalized_path)\n\ndef analyze_file_path(raw_path):\n    # Dependency 1: Normalize the raw path string (e.g., removing redundant separators)\n    normalized = _normalize_path(raw_path)\n    \n    # Dependency 2: Check if the normalized path is an absolute path\n    is_absolute = _check_if_absolute(normalized)\n    \n    return {'normalized': normalized, 'is_absolute': is_absolute}", "summary": "Analyzes a **`raw_path`** string by normalizing it and then determining if the result is an absolute path. It uses **`_normalize_path`** (defined in **`analyze_file_path.py`**) and **`_check_if_absolute`** (defined in **`analyze_file_path.py`**) (both relying on the **`os`** library) to perform these steps."}
{"code": "def _read_data_from_db(sql_query):\n    # Mock database interaction\n    return [('user1', 25), ('user2', 30), ('user3', 25)]\n\ndef _aggregate_by_value(data_tuples, index=1):\n    counts = {}\n    for row in data_tuples:\n        key = row[index]\n        counts[key] = counts.get(key, 0) + 1\n    return counts\n\ndef get_count_by_age(query_string):\n    # Dependency 1: Execute the query and fetch raw data\n    raw_data = _read_data_from_db(query_string)\n    \n    # Dependency 2: Group and count the data based on the second column (age)\n    age_counts = _aggregate_by_value(raw_data)\n    \n    return age_counts", "summary": "Retrieves raw data from a mock database using an **`query_string`** and then aggregates the results by counting the occurrences of values in the second column (age). It calls **`_read_data_from_db`** (defined in **`get_count_by_age.py`**) to fetch the data and **`_aggregate_by_value`** (defined in **`get_count_by_age.py`**) for the counting operation."}
{"code": "def _encode_url_safe(data):\n    import base64\n    # Base64 encode, then convert to URL-safe variants\n    encoded = base64.b64encode(data.encode('utf-8')).decode('utf-8')\n    return encoded.replace('+', '-').replace('/', '_').rstrip('=')\n\ndef _generate_token_signature(encoded_data):\n    # Mock HMAC signature\n    return f\"SIG-{encoded_data[-5:]}\"\n\ndef create_secure_token(user_payload):\n    # Dependency 1: Encode the payload using a URL-safe Base64 variant\n    encoded_payload = _encode_url_safe(user_payload)\n    \n    # Dependency 2: Generate a signature for the encoded payload\n    signature = _generate_token_signature(encoded_payload)\n    \n    return f\"{encoded_payload}.{signature}\"", "summary": "Creates a secure token by first URL-safe Base64 encoding the **`user_payload`** and then generating a signature for the encoded result. It uses **`_encode_url_safe`** (defined in **`create_secure_token.py`**) (relying on the **`base64`** library) and **`_generate_token_signature`** (defined in **`create_secure_token.py`**)."}
{"code": "def _check_if_writable(file_path):\n    import os\n    return os.access(file_path, os.W_OK)\n\ndef _get_file_owner_id(file_path):\n    import os\n    return os.stat(file_path).st_uid\n\ndef analyze_file_permissions(target_file):\n    # Dependency 1: Check if the current process has write permission\n    is_writable = _check_if_writable(target_file)\n    \n    # Dependency 2: Get the user ID of the file owner\n    owner_id = _get_file_owner_id(target_file)\n    \n    return {'can_write': is_writable, 'owner_uid': owner_id}", "summary": "Analyzes a file's permissions by checking if the current process has write access and retrieving the file's owner ID. It uses **`_check_if_writable`** (defined in **`analyze_file_permissions.py`**) and **`_get_file_owner_id`** (defined in **`analyze_file_permissions.py`**) (both relying on the **`os`** library) to perform these checks."}
{"code": "def _validate_credit_card_format(card_number):\n    import re\n    # Simple mock check for 16 digits\n    return bool(re.match(r'^\\d{16}$', card_number.replace(' ', '')))\n\ndef _mask_card_number(card_number):\n    return 'XXXX-XXXX-XXXX-' + card_number[-4:]\n\ndef process_card_for_storage(raw_card_num):\n    # Dependency 1: Validate the card number format\n    if not _validate_credit_card_format(raw_card_num):\n        raise ValueError(\"Invalid card format.\")\n        \n    # Dependency 2: Mask all but the last four digits for storage\n    masked_num = _mask_card_number(raw_card_num)\n    \n    return masked_num", "summary": "Processes a raw credit card number by first validating its format and then masking all but the last four digits for secure storage. It uses **`_validate_credit_card_format`** (defined in **`process_card_for_storage.py`**) (relying on the **`re`** library) for validation, followed by **`_mask_card_number`** (defined in **`process_card_for_storage.py`**)."}
{"code": "def _get_api_version():\n    # Mock retrieval of version from a global config\n    return \"v1.2.3\"\n\ndef _format_endpoint(version, resource):\n    return f\"/{version}/{resource}\"\n\ndef build_api_url(resource_name):\n    # Dependency 1: Retrieve the current API version\n    version = _get_api_version()\n    \n    # Dependency 2: Format the endpoint using the version and resource name\n    endpoint = _format_endpoint(version, resource_name)\n    \n    return endpoint", "summary": "Constructs a full API endpoint URL by first retrieving the current API version and then combining it with the given **`resource_name`**. The function calls **`_get_api_version`** (defined in **`build_api_url.py`**) to fetch the version string, and then uses **`_format_endpoint`** (defined in **`build_api_url.py`**) for path assembly."}
{"code": "def _calculate_mode(data_list):\n    import collections\n    if not data_list: return None\n    counts = collections.Counter(data_list)\n    max_count = max(counts.values())\n    return [key for key, value in counts.items() if value == max_count]\n\ndef _get_range(data_list):\n    if not data_list: return 0\n    return max(data_list) - min(data_list)\n\ndef analyze_data_centrality(data):\n    # Dependency 1: Calculate the mode of the data set\n    mode = _calculate_mode(data)\n    \n    # Dependency 2: Calculate the range of the data set\n    data_range = _get_range(data)\n    \n    return {'mode': mode, 'range': data_range}", "summary": "Analyzes a dataset by calculating two statistical properties: the mode (most frequent value) and the range (maximum value minus minimum value). It uses **`_calculate_mode`** (defined in **`analyze_data_centrality.py`**) (relying on the **`collections`** library) and **`_get_range`** (defined in **`analyze_data_centrality.py`**)."}
{"code": "def _get_max_recursion_depth():\n    import sys\n    return sys.getrecursionlimit()\n\ndef _check_if_safe_depth(current_depth, max_depth):\n    return current_depth < max_depth * 0.9\n\ndef check_recursion_safety(current_recursion_level):\n    # Dependency 1: Get the system's maximum allowed recursion depth\n    max_depth = _get_max_recursion_depth()\n    \n    # Dependency 2: Check if the current level is approaching the limit\n    is_safe = _check_if_safe_depth(current_recursion_level, max_depth)\n    \n    return {'max_limit': max_depth, 'is_safe': is_safe}", "summary": "Checks the current recursion level against the system's maximum allowed depth to determine if the process is safe. It uses **`_get_max_recursion_depth`** (defined in **`check_recursion_safety.py`**) (relying on the **`sys`** library) and **`_check_if_safe_depth`** (defined in **`check_recursion_safety.py`**) for the safety evaluation."}
{"code": "def _get_database_connection(config):\n    # Mock database connection establishment\n    return f\"Connection established to {config['host']}\"\n\ndef _execute_cleanup_script(connection_handle):\n    # Mock SQL execution\n    print(f\"Executing cleanup on {connection_handle}\")\n\ndef run_database_cleanup(db_config):\n    # Dependency 1: Establish a database connection\n    conn = _get_database_connection(db_config)\n    \n    # Dependency 2: Execute the cleanup script using the connection handle\n    _execute_cleanup_script(conn)\n    \n    return True", "summary": "Executes a database cleanup routine by first establishing a connection to the database defined in **`db_config`** and then running a cleanup script using that connection. It uses **`_get_database_connection`** (defined in **`run_database_cleanup.py`**) followed by **`_execute_cleanup_script`** (defined in **`run_database_cleanup.py`**)."}
{"code": "def _generate_jwt_payload(user_id, role):\n    import time\n    return {'sub': user_id, 'role': role, 'iat': int(time.time())}\n\ndef _sign_jwt(payload, secret):\n    # Mock signing process\n    return f\"HEADER.{payload['sub']}.SIGNATURE_{secret}\"\n\ndef create_signed_token(id, user_role, jwt_secret):\n    # Dependency 1: Generate the JWT payload with claims and timestamp\n    payload = _generate_jwt_payload(id, user_role)\n    \n    # Dependency 2: Sign the token using the secret key\n    signed_token = _sign_jwt(payload, jwt_secret)\n    \n    return signed_token", "summary": "Creates a signed JSON Web Token (JWT) by first generating a payload containing the user ID, role, and a timestamp, and then signing the token using a secret key. It relies on **`_generate_jwt_payload`** (defined in **`create_signed_token.py`**) (which uses the **`time`** library) and **`_sign_jwt`** (defined in **`create_signed_token.py`**)."}
{"code": "def _filter_dict_by_keys(data_dict, keys_to_keep):\n    return {k: v for k, v in data_dict.items() if k in keys_to_keep}\n\ndef _serialize_to_yaml(data):\n    import yaml\n    return yaml.dump(data)\n\ndef extract_and_serialize_config(full_config, required_keys):\n    # Dependency 1: Filter the dictionary to keep only the required keys\n    filtered_config = _filter_dict_by_keys(full_config, required_keys)\n    \n    # Dependency 2: Serialize the filtered dictionary into YAML format\n    yaml_output = _serialize_to_yaml(filtered_config)\n    \n    return yaml_output", "summary": "Extracts a subset of key-value pairs from a **`full_config`** dictionary and serializes the resulting configuration into a YAML string. It uses **`_filter_dict_by_keys`** (defined in **`extract_and_serialize_config.py`**) to select the data, and then calls **`_serialize_to_yaml`** (defined in **`extract_and_serialize_config.py`**) (relying on the external **`yaml`** library) for output formatting."}
{"code": "def _check_if_port_is_open(host, port):\n    import socket\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(1)\n    result = sock.connect_ex((host, port))\n    sock.close()\n    return result == 0\n\ndef _log_port_status(host, port, is_open):\n    status = 'OPEN' if is_open else 'CLOSED'\n    print(f\"[NETWORK] Port {port} on {host} is {status}\")\n\ndef scan_network_port(target_host, target_port):\n    # Dependency 1: Perform the socket connection attempt\n    is_port_open = _check_if_port_is_open(target_host, target_port)\n    \n    # Dependency 2: Log the result of the port check\n    _log_port_status(target_host, target_port, is_port_open)\n    \n    return is_port_open", "summary": "Scans a network port on a **`target_host`** to determine if it is open and logs the result. It relies on **`_check_if_port_is_open`** (defined in **`scan_network_port.py`**) (which uses the external **`socket`** library) to perform the connection attempt, and then uses **`_log_port_status`** (defined in **`scan_network_port.py`**) to report the finding."}
{"code": "def _get_data_type(value):\n    return str(type(value))\n\ndef _wrap_in_xml_tag(tag, value):\n    return f\"<{tag} type='{_get_data_type(value)}'>{value}</{tag}>\"\n\ndef format_data_for_xml(field_name, field_value):\n    # Dependency 1: Wrap the value in an XML tag that includes the type\n    xml_element = _wrap_in_xml_tag(field_name, field_value)\n    \n    return xml_element", "summary": "Formats a given **`field_value`** into a basic XML tag named after **`field_name`**, including the data type as an attribute. The function relies on **`_get_data_type`** (defined in **`format_data_for_xml.py`**) to determine the type string, which is then used by **`_wrap_in_xml_tag`** (defined in **`format_data_for_xml.py`**) to construct the XML element."}
{"code": "def _execute_shell_command(command):\n    import subprocess\n    result = subprocess.run(command, capture_output=True, text=True, shell=True)\n    return result.stdout.strip(), result.stderr.strip()\n\ndef _check_for_errors(stderr):\n    return bool(stderr)\n\ndef run_system_command_and_check(command_string):\n    # Dependency 1: Execute the command and capture output\n    stdout, stderr = _execute_shell_command(command_string)\n    \n    # Dependency 2: Check if any error output was captured\n    error_detected = _check_for_errors(stderr)\n    \n    return {'stdout': stdout, 'stderr': stderr, 'error': error_detected}", "summary": "Executes a shell command, captures both standard output and standard error, and checks if any errors were reported. It uses **`_execute_shell_command`** (defined in **`run_system_command_and_check.py`**) (relying on the external **`subprocess`** library) to run the command, and then uses **`_check_for_errors`** (defined in **`run_system_command_and_check.py`**) to check the error stream."}
{"code": "def _apply_linear_transformation(x, slope, intercept):\n    return slope * x + intercept\n\ndef _apply_threshold(y, limit):\n    return y > limit\n\ndef analyze_linear_data(input_x, m, c, threshold):\n    # Dependency 1: Apply the linear transformation to the input\n    transformed_y = _apply_linear_transformation(input_x, m, c)\n    \n    # Dependency 2: Check if the transformed result exceeds the threshold\n    is_above_limit = _apply_threshold(transformed_y, threshold)\n    \n    return {'transformed_y': transformed_y, 'exceeds_threshold': is_above_limit}", "summary": "Applies a linear transformation ($y = mx + c$) to an **`input_x`** value and determines if the resulting $y$-value exceeds a specified **`threshold`**. The calculation involves **`_apply_linear_transformation`** (defined in **`analyze_linear_data.py`**) and the subsequent check uses **`_apply_threshold`** (defined in **`analyze_linear_data.py`**)."}
{"code": "def _count_total_characters(text):\n    return len(text)\n\ndef _count_alphabetic_characters(text):\n    return sum(1 for char in text if char.isalpha())\n\ndef analyze_text_character_counts(input_string):\n    # Dependency 1: Count the total number of characters\n    total_chars = _count_total_characters(input_string)\n    \n    # Dependency 2: Count the number of strictly alphabetic characters\n    alpha_chars = _count_alphabetic_characters(input_string)\n    \n    return {'total_chars': total_chars, 'alpha_chars': alpha_chars}", "summary": "Analyzes an **`input_string`** to determine two character counts: the total number of characters and the count of strictly alphabetic characters. The function sequentially calls **`_count_total_characters`** (defined in **`analyze_text_character_counts.py`**) and **`_count_alphabetic_characters`** (defined in **`analyze_text_character_counts.py`**)."}
{"code": "def _check_string_suffix(text, suffix_list):\n    return any(text.endswith(s) for s in suffix_list)\n\ndef _remove_suffix(text, suffix_list):\n    for suffix in suffix_list:\n        if text.endswith(suffix):\n            return text[:-len(suffix)]\n    return text\n\ndef clean_filename_suffix(filename, suffixes_to_remove):\n    # Dependency 1: Check if the filename ends with one of the target suffixes\n    if _check_string_suffix(filename, suffixes_to_remove):\n        # Dependency 2: Remove the matching suffix\n        cleaned_name = _remove_suffix(filename, suffixes_to_remove)\n        return cleaned_name\n    return filename", "summary": "Cleans a **`filename`** by checking if it ends with any of the provided **`suffixes_to_remove`**, and if so, removing the matching suffix. The process uses **`_check_string_suffix`** (defined in **`clean_filename_suffix.py`**) for validation before applying **`_remove_suffix`** (defined in **`clean_filename_suffix.py`**)."}
{"code": "def _get_data_median(data_list):\n    import statistics\n    return statistics.median(data_list) if data_list else None\n\ndef _classify_deviation(value, median, threshold=10):\n    if median is None: return 'N/A'\n    return 'High Deviation' if abs(value - median) > threshold else 'Normal'\n\ndef check_value_deviation(data_set, test_value):\n    # Dependency 1: Calculate the median of the data set\n    median = _get_data_median(data_set)\n    \n    # Dependency 2: Classify the test value's deviation from the median\n    deviation_class = _classify_deviation(test_value, median)\n    \n    return {'median': median, 'deviation_status': deviation_class}", "summary": "Analyzes a **`test_value`** against a **`data_set`** by first calculating the dataset's median and then classifying the test value's deviation from that median. It uses **`_get_data_median`** (defined in **`check_value_deviation.py`**) (relying on the **`statistics`** library) and **`_classify_deviation`** (defined in **`check_value_deviation.py`**)."}
{"code": "def _sort_list_by_key(data_list, sort_key):\n    import operator\n    return sorted(data_list, key=operator.itemgetter(sort_key))\n\ndef _limit_list_size(sorted_list, limit):\n    return sorted_list[:limit]\n\ndef get_top_n_records(records, key, n):\n    # Dependency 1: Sort the list of records based on the specified key\n    sorted_records = _sort_list_by_key(records, key)\n    \n    # Dependency 2: Take only the top N records from the sorted list\n    top_n = _limit_list_size(sorted_records, n)\n    \n    return top_n", "summary": "Retrieves the top $N$ records from a list of dictionaries by first sorting them based on a specified **`key`** and then truncating the result. It uses **`_sort_list_by_key`** (defined in **`get_top_n_records.py`**) (relying on the **`operator`** library) and **`_limit_list_size`** (defined in **`get_top_n_records.py`**)."}
{"code": "def _fetch_user_groups(user_id):\n    # Mock external service call\n    return ['finance', 'engineering', 'hr']\n\ndef _check_group_membership(groups, required_group):\n    return required_group in groups\n\ndef authorize_access_by_group(user, required_group_name):\n    # Dependency 1: Fetch all groups the user belongs to\n    user_groups = _fetch_user_groups(user)\n    \n    # Dependency 2: Check if the user is a member of the required group\n    is_authorized = _check_group_membership(user_groups, required_group_name)\n    \n    return is_authorized", "summary": "Authorizes user access by first retrieving all groups the **`user`** belongs to and then checking if the required group is present in that list. It uses **`_fetch_user_groups`** (defined in **`authorize_access_by_group.py`**) to get the list and **`_check_group_membership`** (defined in **`authorize_access_by_group.py`**) for the authorization check."}
{"code": "def _get_utc_now():\n    import datetime\n    return datetime.datetime.utcnow().isoformat()\n\ndef _log_event_details(event_name, timestamp):\n    print(f\"[EVENT] {event_name} occurred at {timestamp}\")\n\ndef log_system_event(event_type):\n    # Dependency 1: Get the current timestamp in UTC\n    current_time = _get_utc_now()\n    \n    # Dependency 2: Log the event type and timestamp\n    _log_event_details(event_type, current_time)\n    \n    return current_time", "summary": "Logs a system **`event_type`** by first retrieving the current time in UTC and then printing the event type along with the timestamp. It relies on **`_get_utc_now`** (defined in **`log_system_event.py`**) (which uses the **`datetime`** library) and **`_log_event_details`** (defined in **`log_system_event.py`**)."}
{"code": "def _encode_to_bytes(data_string):\n    return data_string.encode('utf-8')\n\ndef _compress_with_gzip(data_bytes):\n    import gzip\n    return gzip.compress(data_bytes)\n\ndef gzip_compress_string(text):\n    # Dependency 1: Encode the string to bytes\n    raw_bytes = _encode_to_bytes(text)\n    \n    # Dependency 2: Compress the bytes using gzip\n    compressed_bytes = _compress_with_gzip(raw_bytes)\n    \n    return compressed_bytes", "summary": "Compresses a raw **`text`** string using the Gzip algorithm. The function first uses **`_encode_to_bytes`** (defined in **`gzip_compress_string.py`**) to convert the string to byte format, and then calls **`_compress_with_gzip`** (defined in **`gzip_compress_string.py`**) (relying on the external **`gzip`** library) for compression."}
{"code": "def _get_first_non_null(item1, item2, item3):\n    return item1 if item1 is not None else item2 if item2 is not None else item3\n\ndef _format_result(value, prefix='ID-'):\n    return f'{prefix}{value}'\n\ndef coalesce_and_format(val_a, val_b, val_c):\n    # Dependency 1: Select the first non-null value from the three inputs\n    coalesced_value = _get_first_non_null(val_a, val_b, val_c)\n    \n    # Dependency 2: Format the resulting value with a prefix\n    formatted_output = _format_result(coalesced_value)\n    \n    return formatted_output", "summary": "Selects the first non-null value from three sequential inputs (**`val_a`**, **`val_b`**, **`val_c`**) and then formats the selected value with a standard prefix. The function relies on **`_get_first_non_null`** (defined in **`coalesce_and_format.py`**) for selection and **`_format_result`** (defined in **`coalesce_and_format.py`**) for formatting."}
{"code": "def _filter_by_criteria(data_list, key, criterion):\n    return [item for item in data_list if item.get(key) == criterion]\n\ndef _sort_by_name_ascending(filtered_list):\n    return sorted(filtered_list, key=lambda x: x['name'])\n\ndef fetch_and_sort_by_key(data, filter_key, filter_val):\n    # Dependency 1: Filter the data list based on a specific key/value pair\n    filtered_data = _filter_by_criteria(data, filter_key, filter_val)\n    \n    # Dependency 2: Sort the resulting list by the 'name' field\n    sorted_data = _sort_by_name_ascending(filtered_data)\n    \n    return sorted_data", "summary": "Filters a list of dictionaries based on a specific key/value criterion and then sorts the resulting subset by the 'name' field in ascending order. This two-step process uses **`_filter_by_criteria`** (defined in **`fetch_and_sort_by_key.py`**) followed by **`_sort_by_name_ascending`** (defined in **`fetch_and_sort_by_key.py`**)."}
{"code": "def _check_string_contains_digit(text):\n    return any(char.isdigit() for char in text)\n\ndef _check_string_length_min(text, min_length=8):\n    return len(text) >= min_length\n\ndef validate_password_complexity(password):\n    # Dependency 1: Check if the password meets the minimum length requirement\n    has_min_length = _check_string_length_min(password)\n    \n    # Dependency 2: Check if the password contains at least one digit\n    has_digit = _check_string_contains_digit(password)\n    \n    return has_min_length and has_digit", "summary": "Validates a password for basic complexity by ensuring it meets a minimum length requirement and contains at least one numeric digit. It uses **`_check_string_length_min`** (defined in **`validate_password_complexity.py`**) and **`_check_string_contains_digit`** (defined in **`validate_password_complexity.py`**) for the two criteria."}
{"code": "def _get_system_memory_usage():\n    import psutil\n    return psutil.virtual_memory().percent\n\ndef _alert_if_high(usage_percent, threshold=90):\n    return usage_percent > threshold\n\ndef monitor_memory_usage():\n    # Dependency 1: Get the current percentage of memory usage\n    mem_usage = _get_system_memory_usage()\n    \n    # Dependency 2: Check if the usage is above the critical alert threshold\n    is_critical = _alert_if_high(mem_usage)\n    \n    return {'usage_percent': mem_usage, 'critical_alert': is_critical}", "summary": "Monitors system memory usage by retrieving the current utilization percentage and determining if that percentage exceeds a critical alert threshold. It relies on **`_get_system_memory_usage`** (defined in **`monitor_memory_usage.py`**) (which uses the external **`psutil`** library) and **`_alert_if_high`** (defined in **`monitor_memory_usage.py`**)."}
{"code": "def _find_max_occurrence_char(text):\n    import collections\n    counts = collections.Counter(text)\n    if not counts: return None\n    return counts.most_common(1)[0][0]\n\ndef _check_if_vowel(char):\n    return char.lower() in 'aeiou'\n\ndef analyze_most_frequent_char(input_string):\n    # Dependency 1: Find the character that appears most frequently\n    most_common_char = _find_max_occurrence_char(input_string)\n    \n    # Dependency 2: Check if that character is a vowel\n    is_vowel = _check_if_vowel(most_common_char)\n    \n    return {'char': most_common_char, 'is_vowel': is_vowel}", "summary": "Analyzes an **`input_string`** to identify the most frequent character and then determines if that character is a vowel. It uses **`_find_max_occurrence_char`** (defined in **`analyze_most_frequent_char.py`**) (relying on the **`collections`** library) and **`_check_if_vowel`** (defined in **`analyze_most_frequent_char.py`**)."}
{"code": "def _calculate_total_seconds(hours, minutes, seconds):\n    return hours * 3600 + minutes * 60 + seconds\n\ndef _format_to_hms(total_seconds):\n    h = total_seconds // 3600\n    m = (total_seconds % 3600) // 60\n    s = total_seconds % 60\n    return f\"{h:02d}:{m:02d}:{s:02d}\"\n\ndef convert_time_to_hms_format(total_h, total_m, total_s):\n    # Dependency 1: Calculate the total time in seconds\n    seconds_total = _calculate_total_seconds(total_h, total_m, total_s)\n    \n    # Dependency 2: Format the total seconds back into HH:MM:SS string\n    hms_string = _format_to_hms(seconds_total)\n    \n    return hms_string", "summary": "Converts input time components (hours, minutes, seconds) into a total number of seconds, and then reformats the total duration back into an HH:MM:SS string. The conversion uses **`_calculate_total_seconds`** (defined in **`convert_time_to_hms_format.py`**) followed by **`_format_to_hms`** (defined in **`convert_time_to_hms_format.py`**)."}
{"code": "def _get_api_key(service_name):\n    # Mock secret manager retrieval\n    return f\"SECRET_{service_name.upper()}_KEY\"\n\ndef _configure_client(key, endpoint):\n    # Mock client setup\n    return f\"Client configured for {endpoint} with key starting {key[:5]}...\"\n\ndef initialize_service_client(service_endpoint, service_identifier):\n    # Dependency 1: Retrieve the required API key\n    api_key = _get_api_key(service_identifier)\n    \n    # Dependency 2: Configure the service client using the key and endpoint\n    client_status = _configure_client(api_key, service_endpoint)\n    \n    return client_status", "summary": "Initializes a service client by first retrieving a secret API key for the **`service_identifier`** and then using that key along with the **`service_endpoint`** to configure the client. It uses **`_get_api_key`** (defined in **`initialize_service_client.py`**) and **`_configure_client`** (defined in **`initialize_service_client.py`**)."}
{"code": "def _validate_ip_address(ip_string):\n    import ipaddress\n    try:\n        ipaddress.ip_address(ip_string)\n        return True\n    except ValueError: \n        return False\n\ndef _is_public_ip(ip_string):\n    import ipaddress\n    ip = ipaddress.ip_address(ip_string)\n    return not (ip.is_private or ip.is_loopback)\n\ndef analyze_ip_address_type(ip_address):\n    # Dependency 1: Validate the format of the IP address\n    if not _validate_ip_address(ip_address):\n        return {'is_valid': False, 'is_public': False}\n        \n    # Dependency 2: Check if the valid IP is public or private\n    is_public = _is_public_ip(ip_address)\n    \n    return {'is_valid': True, 'is_public': is_public}", "summary": "Analyzes an **`ip_address`** string by validating its format and determining if it is a public or private address. It relies on **`_validate_ip_address`** (defined in **`analyze_ip_address_type.py`**) (which uses the **`ipaddress`** library) for format checking, and then uses **`_is_public_ip`** (defined in **`analyze_ip_address_type.py`**) (also using **`ipaddress`**) for the public/private classification."}
{"code": "def _get_list_intersection(list1, list2):\n    return list(set(list1).intersection(set(list2)))\n\ndef _get_symmetric_difference(list1, list2):\n    return list(set(list1).symmetric_difference(set(list2)))\n\ndef analyze_list_overlap(list_a, list_b):\n    # Dependency 1: Find the common elements (intersection)\n    intersection = _get_list_intersection(list_a, list_b)\n    \n    # Dependency 2: Find the elements unique to each list (symmetric difference)\n    unique_elements = _get_symmetric_difference(list_a, list_b)\n    \n    return {'intersection': intersection, 'unique_only': unique_elements}", "summary": "Analyzes the overlap between **`list_a`** and **`list_b`** by calculating both the intersection (common elements) and the symmetric difference (elements unique to only one list). The function calls **`_get_list_intersection`** (defined in **`analyze_list_overlap.py`**) and **`_get_symmetric_difference`** (defined in **`analyze_list_overlap.py`**)."}
{"code": "def _run_optimization_algorithm(data, max_iterations):\n    # Mock complex algorithm execution\n    final_value = sum(data) / max_iterations\n    return final_value\n\ndef _report_optimization_status(value):\n    return f\"Optimization completed. Final value: {value:.2f}\"\n\ndef execute_optimization_run(input_data, iterations):\n    # Dependency 1: Run the optimization algorithm\n    result_value = _run_optimization_algorithm(input_data, iterations)\n    \n    # Dependency 2: Generate a formatted status report\n    status_report = _report_optimization_status(result_value)\n    \n    return status_report", "summary": "Executes a mock optimization algorithm on **`input_data`** over a set number of **`iterations`** and generates a formatted status report of the final result. The execution relies on **`_run_optimization_algorithm`** (defined in **`execute_optimization_run.py`**) followed by **`_report_optimization_status`** (defined in **`execute_optimization_run.py`**)."}
{"code": "def _convert_to_epoch_ms(datetime_obj):\n    return int(datetime_obj.timestamp() * 1000)\n\ndef _check_timestamp_in_range(timestamp_ms, start_ms, end_ms):\n    return start_ms <= timestamp_ms <= end_ms\n\ndef validate_timestamp_window(dt_object, start_dt, end_dt):\n    import datetime\n    # Dependency 1 & 2 & 3: Convert all datetime objects to epoch milliseconds\n    target_ms = _convert_to_epoch_ms(dt_object)\n    start_ms = _convert_to_epoch_ms(start_dt)\n    end_ms = _convert_to_epoch_ms(end_dt)\n    \n    # Dependency 4: Check if the target timestamp is between start and end\n    is_within_window = _check_timestamp_in_range(target_ms, start_ms, end_ms)\n    \n    return is_within_window", "summary": "Validates whether a **`dt_object`** timestamp falls within a window defined by **`start_dt`** and **`end_dt`**. The function uses **`_convert_to_epoch_ms`** (defined in **`validate_timestamp_window.py`**) (which relies on the **`datetime`** library) to standardize all timestamps, and then calls **`_check_timestamp_in_range`** (defined in **`validate_timestamp_window.py`**)."}
{"code": "def _get_api_endpoint(env='prod'):\n    if env == 'dev': return 'https://dev.api.com'\n    return 'https://prod.api.com'\n\ndef _send_get_request(url, params):\n    # Mock HTTP request\n    return f\"GET request sent to {url} with params {params}\"\n\ndef execute_external_query(query_params, environment):\n    # Dependency 1: Determine the correct API endpoint based on the environment\n    endpoint = _get_api_endpoint(environment)\n    \n    # Dependency 2: Send the GET request to the determined endpoint\n    response = _send_get_request(endpoint, query_params)\n    \n    return response", "summary": "Executes a mock external API query by first determining the correct API endpoint based on the specified **`environment`** and then sending a GET request with the provided **`query_params`**. The process relies on **`_get_api_endpoint`** (defined in **`execute_external_query.py`**) and **`_send_get_request`** (defined in **`execute_external_query.py`**)."}
{"code": "def _calculate_harmonic_series_sum(n):\n    if n == 0: return 0.0\n    return sum(1/i for i in range(1, n + 1))\n\ndef _compare_with_log(h_sum, n):\n    import math\n    # Compares Harmonic sum with ln(n) + Euler-Mascheroni constant (gamma)\n    gamma = 0.5772156649\n    expected = math.log(n) + gamma\n    return abs(h_sum - expected) < 1.0\n\ndef analyze_harmonic_series_approximation(n_terms):\n    # Dependency 1: Calculate the actual sum of the Harmonic series\n    harmonic_sum = _calculate_harmonic_series_sum(n_terms)\n    \n    # Dependency 2: Check if the sum is close to the expected log approximation\n    is_close = _compare_with_log(harmonic_sum, n_terms)\n    \n    return {'sum': harmonic_sum, 'approx_match': is_close}", "summary": "Calculates the sum of the Harmonic series up to **`n_terms`** and then compares that sum to its known logarithmic approximation using the Euler-Mascheroni constant. It uses **`_calculate_harmonic_series_sum`** (defined in **`analyze_harmonic_series_approximation.py`**) and **`_compare_with_log`** (defined in **`analyze_harmonic_series_approximation.py`**) (which relies on the **`math`** library) for the comparison."}
{"code": "def _format_to_currency(amount, symbol='USD'):\n    return f'{symbol} {amount:,.2f}'\n\ndef _check_if_negative(formatted_amount):\n    return '-' in formatted_amount\n\ndef display_financial_value(raw_amount):\n    # Dependency 1: Format the raw amount into a currency string\n    currency_string = _format_to_currency(raw_amount)\n    \n    # Dependency 2: Check if the resulting string contains a negative sign\n    is_debt = _check_if_negative(currency_string)\n    \n    return {'display': currency_string, 'is_negative': is_debt}", "summary": "Formats a raw numerical **`amount`** into a standard currency string (USD) and checks if the value is negative (representing a debt). The process uses **`_format_to_currency`** (defined in **`display_financial_value.py`**) and **`_check_if_negative`** (defined in **`display_financial_value.py`**)."}
{"code": "def _filter_dict_by_value(data_dict, min_val):\n    return {k: v for k, v in data_dict.items() if v >= min_val}\n\ndef _calculate_average_value(filtered_dict):\n    if not filtered_dict: return 0\n    return sum(filtered_dict.values()) / len(filtered_dict)\n\ndef analyze_high_value_items(input_data, min_threshold):\n    # Dependency 1: Filter the dictionary to include only values above the threshold\n    high_value_items = _filter_dict_by_value(input_data, min_threshold)\n    \n    # Dependency 2: Calculate the average of the remaining (high) values\n    average_high_value = _calculate_average_value(high_value_items)\n    \n    return {'filtered_count': len(high_value_items), 'average_value': average_high_value}", "summary": "Filters a dictionary to retain only items with values above a **`min_threshold`** and then calculates the average of these remaining high values. It uses **`_filter_dict_by_value`** (defined in **`analyze_high_value_items.py`**) for the selection, followed by **`_calculate_average_value`** (defined in **`analyze_high_value_items.py`**) for the mean calculation."}
{"code": "def _get_api_base_url():\n    return 'https://api.example.com'\n\ndef _build_full_url(base, path):\n    import urllib.parse\n    return urllib.parse.urljoin(base, path)\n\ndef construct_service_url(resource_path):\n    # Dependency 1: Get the base URL for the API\n    base_url = _get_api_base_url()\n    \n    # Dependency 2: Combine the base URL and the resource path safely\n    full_url = _build_full_url(base_url, resource_path)\n    \n    return full_url", "summary": "Constructs a full, qualified URL for an API resource by combining a base URL with a **`resource_path`**. It uses **`_get_api_base_url`** (defined in **`construct_service_url.py`**) to retrieve the base, and then uses **`_build_full_url`** (defined in **`construct_service_url.py`**) (relying on **`urllib.parse`**) to safely join the paths."}
{"code": "def _read_file_line_by_line(file_path):\n    with open(file_path, 'r') as f:\n        return f.readlines()\n\ndef _count_lines_containing(lines, substring):\n    return sum(1 for line in lines if substring in line)\n\ndef analyze_file_content_for_substring(filepath, search_term):\n    # Dependency 1: Read the file content line by line\n    all_lines = _read_file_line_by_line(filepath)\n    \n    # Dependency 2: Count how many lines contain the search term\n    match_count = _count_lines_containing(all_lines, search_term)\n    \n    return match_count", "summary": "Analyzes a file's content by reading it line by line and counting how many lines contain a specific **`search_term`**. The function uses **`_read_file_line_by_line`** (defined in **`analyze_file_content_for_substring.py`**) for file I/O and **`_count_lines_containing`** (defined in **`analyze_file_content_for_substring.py`**) for the final count."}
{"code": "def _check_type_is_iterable(obj):\n    try:\n        iter(obj)\n        return True\n    except TypeError: return False\n\ndef _get_first_n_elements(iterable_obj, n=5):\n    import itertools\n    return list(itertools.islice(iterable_obj, n))\n\ndef inspect_iterable_object(data_object):\n    # Dependency 1: Check if the object is iterable\n    is_iterable = _check_type_is_iterable(data_object)\n    \n    # Dependency 2: Extract the first 5 elements if iterable\n    first_elements = _get_first_n_elements(data_object) if is_iterable else []\n    \n    return {'is_iterable': is_iterable, 'sample': first_elements}", "summary": "Inspects a generic **`data_object`** to determine if it is iterable and, if so, extracts the first 5 elements. It relies on **`_check_type_is_iterable`** (defined in **`inspect_iterable_object.py`**) for the type check, and **`_get_first_n_elements`** (defined in **`inspect_iterable_object.py`**) (which uses the **`itertools`** library) for sampling."}
{"code": "def _get_max_drawdown(series):\n    # Mock function to find the maximum drop from a peak\n    import numpy\n    return numpy.max(numpy.maximum.accumulate(series) - series)\n\ndef _classify_volatility(max_drawdown, threshold=0.3):\n    return 'High Volatility' if max_drawdown > threshold else 'Low Volatility'\n\ndef analyze_market_volatility(price_series):\n    # Dependency 1: Calculate the maximum drawdown of the price series\n    drawdown = _get_max_drawdown(price_series)\n    \n    # Dependency 2: Classify volatility based on the drawdown\n    volatility_class = _classify_volatility(drawdown)\n    \n    return {'max_drawdown': drawdown, 'volatility_class': volatility_class}", "summary": "Analyzes the volatility of a **`price_series`** by calculating the maximum drawdown (peak-to-trough decline) and then classifying the volatility based on that result. It uses **`_get_max_drawdown`** (defined in **`analyze_market_volatility.py`**) (relying on the **`numpy`** library) and **`_classify_volatility`** (defined in **`analyze_market_volatility.py`**)."}
{"code": "def _check_if_date_is_holiday(date_obj):\n    # Mock holiday calendar lookup\n    return date_obj.month == 1 and date_obj.day == 1 # Mocking New Year's Day\n\ndef _format_date_with_holiday_flag(date_obj, is_holiday):\n    flag = ' (HOLIDAY)' if is_holiday else ''\n    return date_obj.strftime('%Y-%m-%d') + flag\n\ndef format_date_with_holiday_check(target_date):\n    # Dependency 1: Check if the date falls on a holiday\n    is_holiday = _check_if_date_is_holiday(target_date)\n    \n    # Dependency 2: Format the date string, optionally including the holiday flag\n    formatted_date = _format_date_with_holiday_flag(target_date, is_holiday)\n    \n    return formatted_date", "summary": "Formats a **`target_date`** object into a string, first checking if the date falls on a mock holiday and appending a flag if it does. The function uses **`_check_if_date_is_holiday`** (defined in **`format_date_with_holiday_check.py`**) for the lookup and **`_format_date_with_holiday_flag`** (defined in **`format_date_with_holiday_check.py`**) for the final output string."}
{"code": "def _filter_empty_strings(text_list):\n    return [s for s in text_list if s.strip()]\n\ndef _join_with_newline(string_list):\n    return '\\n'.join(string_list)\n\ndef consolidate_text_lines(raw_lines):\n    # Dependency 1: Filter out any empty or whitespace-only strings\n    non_empty_lines = _filter_empty_strings(raw_lines)\n    \n    # Dependency 2: Join the remaining lines into a single block of text separated by newlines\n    consolidated_text = _join_with_newline(non_empty_lines)\n    \n    return consolidated_text", "summary": "Consolidates a list of **`raw_lines`** into a single block of text by first filtering out all empty or whitespace-only strings and then joining the remaining lines with newline characters. The process uses **`_filter_empty_strings`** (defined in **`consolidate_text_lines.py`**) followed by **`_join_with_newline`** (defined in **`consolidate_text_lines.py`**)."}
{"code": "def _validate_numeric_type(value):\n    return isinstance(value, (int, float))\n\ndef _square_root(number):\n    import math\n    if number < 0: raise ValueError(\"Cannot take square root of negative number.\")\n    return math.sqrt(number)\n\ndef compute_safe_square_root(input_value):\n    # Dependency 1: Validate that the input is a numeric type\n    if not _validate_numeric_type(input_value):\n        raise TypeError(\"Input must be numeric.\")\n        \n    # Dependency 2: Compute the square root safely\n    result = _square_root(input_value)\n    \n    return result", "summary": "Computes the square root of an **`input_value`** after first validating that the input is a numeric type. It uses **`_validate_numeric_type`** (defined in **`compute_safe_square_root.py`**) for type checking and **`_square_root`** (defined in **`compute_safe_square_root.py`**) (which relies on the **`math`** library) for the calculation, which includes a negative number check."}
{"code": "def _get_cache_key(user_id, resource_id):\n    return f\"user:{user_id}_resource:{resource_id}\"\n\ndef _invalidate_cache_entry(cache_key):\n    # Mock cache invalidation call\n    print(f\"Invalidated cache key: {cache_key}\")\n    return True\n\ndef clear_resource_cache(user, resource):\n    # Dependency 1: Generate the standardized cache key\n    key = _get_cache_key(user, resource)\n    \n    # Dependency 2: Execute the cache invalidation\n    success = _invalidate_cache_entry(key)\n    \n    return success", "summary": "Clears a cache entry for a specific user and resource by first generating the standardized cache key and then executing a mock invalidation call. It relies on **`_get_cache_key`** (defined in **`clear_resource_cache.py`**) for key assembly and **`_invalidate_cache_entry`** (defined in **`clear_resource_cache.py`**) for the mock action."}
{"code": "def _round_to_n_decimal_places(number, n=2):\n    return round(number, n)\n\ndef _convert_to_string_with_padding(number_str, total_width=10):\n    return number_str.ljust(total_width)\n\ndef format_float_for_display(float_input):\n    # Dependency 1: Round the float to two decimal places\n    rounded_num = _round_to_n_decimal_places(float_input)\n    \n    # Dependency 2: Convert to a string and pad it to a fixed width\n    padded_string = _convert_to_string_with_padding(str(rounded_num))\n    \n    return padded_string", "summary": "Formats a **`float_input`** for display by first rounding it to two decimal places and then converting the number to a string and left-justifying it to a fixed width. The formatting uses **`_round_to_n_decimal_places`** (defined in **`format_float_for_display.py`**) followed by **`_convert_to_string_with_padding`** (defined in **`format_float_for_display.py`**)."}
{"code": "def _get_config_section(config_dict, section_key):\n    return config_dict.get(section_key, {})\n\ndef _override_config_values(section, overrides):\n    return {**section, **overrides}\n\ndef merge_configuration_sections(config, section_name, new_settings):\n    # Dependency 1: Extract the specific configuration section\n    base_section = _get_config_section(config, section_name)\n    \n    # Dependency 2: Merge the new settings over the base section\n    merged_section = _override_config_values(base_section, new_settings)\n    \n    return merged_section", "summary": "Merges new settings into a specific section of a configuration dictionary by first extracting the base configuration section and then using **`_override_config_values`** (defined in **`merge_configuration_sections.py`**) to merge the **`new_settings`** over the base."}
{"code": "def _check_http_method_validity(method):\n    valid_methods = ['GET', 'POST', 'PUT', 'DELETE', 'PATCH']\n    return method.upper() in valid_methods\n\ndef _log_invalid_attempt(method):\n    print(f\"[SECURITY] Invalid HTTP method attempted: {method}\")\n\ndef process_api_request(http_method, payload):\n    # Dependency 1: Check if the request method is a valid HTTP verb\n    if not _check_http_method_validity(http_method):\n        # Dependency 2: Log the invalid attempt\n        _log_invalid_attempt(http_method)\n        return False\n    \n    # Mock successful processing\n    return True", "summary": "Processes an API request by first checking if the **`http_method`** is a valid HTTP verb, and logging the attempt if it is invalid. The process uses **`_check_http_method_validity`** (defined in **`process_api_request.py`**) for validation and **`_log_invalid_attempt`** (defined in **`process_api_request.py`**) for logging."}
{"code": "def _calculate_interquartile_range(data):\n    import numpy\n    q3 = numpy.percentile(data, 75)\n    q1 = numpy.percentile(data, 25)\n    return q3 - q1\n\ndef _check_for_tight_distribution(iqr, threshold=5):\n    return iqr < threshold\n\ndef analyze_data_spread(data_list):\n    # Dependency 1: Calculate the Interquartile Range (IQR)\n    iqr_value = _calculate_interquartile_range(data_list)\n    \n    # Dependency 2: Determine if the distribution is tightly clustered (low IQR)\n    is_tight = _check_for_tight_distribution(iqr_value)\n    \n    return {'iqr': iqr_value, 'is_tight_distribution': is_tight}", "summary": "Analyzes the spread of a **`data_list`** by calculating the Interquartile Range (IQR) and then determining if the distribution is tightly clustered based on the IQR's value. It uses **`_calculate_interquartile_range`** (defined in **`analyze_data_spread.py`**) (relying on the external **`numpy`** library) and **`_check_for_tight_distribution`** (defined in **`analyze_data_spread.py`**)."}
{"code": "def _get_local_timezone():\n    import pytz\n    return pytz.timezone('America/New_York')\n\ndef _convert_to_local_time(utc_dt, timezone):\n    return utc_dt.astimezone(timezone).strftime('%Y-%m-%d %H:%M:%S')\n\ndef localize_utc_datetime(utc_datetime_object):\n    import datetime\n    # Dependency 1: Get the target local timezone object\n    local_tz = _get_local_timezone()\n    \n    # Dependency 2: Convert the UTC datetime object to the local timezone\n    local_time_str = _convert_to_local_time(utc_datetime_object, local_tz)\n    \n    return local_time_str", "summary": "Converts a UTC datetime object to a localized time string (using 'America/New York' as a mock local zone). It uses **`_get_local_timezone`** (defined in **`localize_utc_datetime.py`**) (relying on the **`pytz`** library) to fetch the target timezone, and then uses **`_convert_to_local_time`** (defined in **`localize_utc_datetime.py`**) (relying on the **`datetime`** library) for the conversion and formatting."}
{"code": "def _filter_by_regex_match(text_list, pattern):\n    import re\n    return [text for text in text_list if re.search(pattern, text)]\n\ndef _count_total_length(filtered_list):\n    return sum(len(s) for s in filtered_list)\n\ndef measure_matching_text_length(text_data, regex_pattern):\n    # Dependency 1: Filter the list to include only strings that match the regex pattern\n    matching_texts = _filter_by_regex_match(text_data, regex_pattern)\n    \n    # Dependency 2: Calculate the total character length of all matching strings\n    total_length = _count_total_length(matching_texts)\n    \n    return total_length", "summary": "Filters a list of strings (**`text_data`**) to include only those that match a given **`regex_pattern`** and then calculates the total combined character length of all matching strings. It relies on **`_filter_by_regex_match`** (defined in **`measure_matching_text_length.py`**) (which uses the **`re`** library) and **`_count_total_length`** (defined in **`measure_matching_text_length.py`**)."}
{"code": "def _check_if_file_is_empty(file_path):\n    import os\n    return os.path.getsize(file_path) == 0\n\ndef _log_file_status(file_path, is_empty):\n    status = 'empty' if is_empty else 'contains data'\n    print(f\"File {file_path} status: {status}\")\n\ndef verify_file_for_content(target_file):\n    # Dependency 1: Check the file size to see if it's empty\n    is_file_empty = _check_if_file_is_empty(target_file)\n    \n    # Dependency 2: Log the emptiness status\n    _log_file_status(target_file, is_file_empty)\n    \n    return is_file_empty", "summary": "Verifies if a **`target_file`** is empty by checking its size and logs the status to the console. It uses **`_check_if_file_is_empty`** (defined in **`verify_file_for_content.py`**) (relying on the **`os`** library) to check the size, and then **`_log_file_status`** (defined in **`verify_file_for_content.py`**) to report the finding."}
{"code": "def _compute_cosine_angle(dot_product, magnitude_a, magnitude_b):\n    return dot_product / (magnitude_a * magnitude_b)\n\ndef _check_if_vectors_are_orthogonal(cosine_angle, threshold=0.01):\n    return abs(cosine_angle) < threshold\n\ndef analyze_vector_orthogonality(dot_p, mag_a, mag_b):\n    # Dependency 1: Calculate the cosine of the angle between the vectors\n    cosine = _compute_cosine_angle(dot_p, mag_a, mag_b)\n    \n    # Dependency 2: Check if the cosine is close to zero (indicating orthogonality)\n    is_orthogonal = _check_if_vectors_are_orthogonal(cosine)\n    \n    return {'cosine_similarity': cosine, 'is_orthogonal': is_orthogonal}", "summary": "Analyzes two vectors for orthogonality by first calculating the cosine of the angle between them using their **`dot_p`** and **`mag_a`**/**`mag_b`** (magnitudes), and then checking if the resulting cosine is near zero. The process relies on **`_compute_cosine_angle`** (defined in **`analyze_vector_orthogonality.py`**) and **`_check_if_vectors_are_orthogonal`** (defined in **`analyze_vector_orthogonality.py`**)."}
{"code": "def _load_settings_from_ini(file_path, section):\n    import configparser\n    config = configparser.ConfigParser()\n    config.read(file_path)\n    return dict(config.items(section))\n\ndef _validate_required_keys(settings, required_keys):\n    return all(key in settings for key in required_keys)\n\ndef validate_ini_settings(config_file, section_name, mandatory_keys):\n    # Dependency 1: Load settings from the specified INI section\n    settings = _load_settings_from_ini(config_file, section_name)\n    \n    # Dependency 2: Validate that all mandatory keys are present\n    is_complete = _validate_required_keys(settings, mandatory_keys)\n    \n    return {'settings': settings, 'is_complete': is_complete}", "summary": "Validates settings within an INI file by first loading a specific **`section_name`** and then checking that all **`mandatory_keys`** are present. It relies on **`_load_settings_from_ini`** (defined in **`validate_ini_settings.py`**) (which uses the **`configparser`** library) and **`_validate_required_keys`** (defined in **`validate_ini_settings.py`**)."}
{"code": "def _get_user_agent_from_request(request_object):\n    # Mock request object interaction\n    return request_object.get('User-Agent', 'Unknown')\n\ndef _classify_device_type(user_agent_string):\n    if 'mobile' in user_agent_string.lower(): return 'Mobile'\n    if 'tablet' in user_agent_string.lower(): return 'Tablet'\n    return 'Desktop'\n\ndef analyze_request_device(http_request_mock):\n    # Dependency 1: Extract the User-Agent string from the request\n    user_agent = _get_user_agent_from_request(http_request_mock)\n    \n    # Dependency 2: Classify the device type based on the User-Agent\n    device_type = _classify_device_type(user_agent)\n    \n    return {'user_agent': user_agent, 'device_type': device_type}", "summary": "Analyzes a mock HTTP **`http_request_mock`** to determine the user's device type by first extracting the User-Agent string and then classifying the device based on keywords within that string. The process uses **`_get_user_agent_from_request`** (defined in **`analyze_request_device.py`**) and **`_classify_device_type`** (defined in **`analyze_request_device.py`**)."}
{"code": "def _create_html_link(url, link_text):\n    return f\"<a href='{url}'>{link_text}</a>\"\n\ndef _wrap_in_list_item(html_link):\n    return f\"<li>{html_link}</li>\"\n\ndef generate_link_list_item(target_url, display_text):\n    # Dependency 1: Create the HTML anchor link element\n    anchor_link = _create_html_link(target_url, display_text)\n    \n    # Dependency 2: Wrap the anchor link inside an HTML list item\n    list_item = _wrap_in_list_item(anchor_link)\n    \n    return list_item", "summary": "Generates a complete HTML list item (`<li>`) that contains an anchor link (`<a>`), using the provided **`target_url`** and **`display_text`**. The function calls **`_create_html_link`** (defined in **`generate_link_list_item.py`**) to form the link, and then **`_wrap_in_list_item`** (defined in **`generate_link_list_item.py`**) to enclose it."}
{"code": "def _is_valid_email_structure(email):\n    import re\n    # Simple check for user@domain.tld\n    return bool(re.match(r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\", email))\n\ndef _get_email_mailbox(email):\n    return email.split('@')[0]\n\ndef process_email_input(email_address):\n    # Dependency 1: Validate the basic email structure\n    if not _is_valid_email_structure(email_address):\n        raise ValueError(\"Invalid email structure.\")\n        \n    # Dependency 2: Extract the mailbox (username) part\n    mailbox = _get_email_mailbox(email_address)\n    \n    return mailbox", "summary": "Processes an **`email_address`** string by first validating its basic structure and then extracting the mailbox (username) part. It relies on **`_is_valid_email_structure`** (defined in **`process_email_input.py`**) (which uses the **`re`** library) for format validation and **`_get_email_mailbox`** (defined in **`process_email_input.py`**) for extraction."}
{"code": "def _check_time_delta_seconds(dt1, dt2, max_seconds):\n    import datetime\n    return abs((dt1 - dt2).total_seconds()) <= max_seconds\n\ndef _log_audit_event(dt1, dt2, status):\n    print(f\"Audit Check: Timestamps {dt1} and {dt2} status: {status}\")\n\ndef audit_timestamp_proximity(timestamp1, timestamp2, allowed_drift_seconds):\n    import datetime\n    # Dependency 1: Check the time difference in seconds against the maximum allowed drift\n    is_close = _check_time_delta_seconds(timestamp1, timestamp2, allowed_drift_seconds)\n    \n    # Dependency 2: Log the result of the audit check\n    _log_audit_event(timestamp1, timestamp2, 'PASS' if is_close else 'FAIL')\n    \n    return is_close", "summary": "Audits the proximity of two datetime objects by checking if the absolute difference between them exceeds an **`allowed_drift_seconds`**, and logs the result. It uses **`_check_time_delta_seconds`** (defined in **`audit_timestamp_proximity.py`**) (relying on the **`datetime`** library) for the comparison and **`_log_audit_event`** (defined in **`audit_timestamp_proximity.py`**) for reporting."}
{"code": "def _verify_checksum_luhn(card_number_str):\n    # Mock simplified Luhn algorithm check\n    digits = [int(d) for d in card_number_str if d.isdigit()]\n    if not digits: return False\n    return sum(digits) % 10 == 0\n\ndef _log_validation_failure(number):\n    print(f\"[VALIDATION] Checksum failure for number ending in {number[-4:]}\")\n\ndef validate_and_log_card(input_number):\n    # Dependency 1: Verify the card number using the Luhn checksum\n    is_valid = _verify_checksum_luhn(input_number)\n    \n    # Dependency 2: Log if the validation failed\n    if not is_valid:\n        _log_validation_failure(input_number)\n        \n    return is_valid", "summary": "Validates a card number using a simplified mock Luhn checksum algorithm via **`_verify_checksum_luhn`** (defined in **`validate_and_log_card.py`**) and, if the check fails, logs a validation failure message using **`_log_validation_failure`** (defined in **`validate_and_log_card.py`**)."}
{"code": "def _generate_uuid4_string():\n    import uuid\n    return str(uuid.uuid4())\n\ndef _add_prefix_to_id(uuid_str, prefix='SESSION-'):\n    return f'{prefix}{uuid_str}'\n\ndef create_prefixed_session_id():\n    # Dependency 1: Generate a universally unique identifier (UUID v4)\n    new_uuid = _generate_uuid4_string()\n    \n    # Dependency 2: Prepend a standard prefix to the UUID string\n    session_id = _add_prefix_to_id(new_uuid)\n    \n    return session_id", "summary": "Creates a prefixed session ID by first generating a UUID v4 string using **`_generate_uuid4_string`** (defined in **`create_prefixed_session_id.py`**) (which relies on the **`uuid`** library) and then prepending the 'SESSION-' prefix using **`_add_prefix_to_id`** (defined in **`create_prefixed_session_id.py`**)."}
{"code": "def _read_data_from_csv(file_path):\n    import pandas as pd\n    # Mock CSV read returning a list of dictionaries\n    return pd.DataFrame([{'name': 'A', 'value': 10}, {'name': 'B', 'value': 20}]).to_dict('records')\n\ndef _calculate_mean_value(records, key='value'):\n    values = [r[key] for r in records]\n    return sum(values) / len(values) if values else 0\n\ndef process_csv_and_get_mean(csv_file):\n    # Dependency 1: Read and parse the data from the CSV file\n    data_records = _read_data_from_csv(csv_file)\n    \n    # Dependency 2: Calculate the mean of the 'value' column\n    mean = _calculate_mean_value(data_records)\n    \n    return mean", "summary": "Processes a CSV file by first reading and parsing the data into records using **`_read_data_from_csv`** (defined in **`process_csv_and_get_mean.py`**) (which mocks using the **`pandas`** library), and then calculating the mean of the 'value' column using **`_calculate_mean_value`** (defined in **`process_csv_and_get_mean.py`**)."}
{"code": "def _check_if_palindrome(text):\n    normalized = ''.join(c.lower() for c in text if c.isalnum())\n    return normalized == normalized[::-1]\n\ndef _count_words(text):\n    return len(text.split())\n\ndef analyze_text_for_palindrome(input_text):\n    # Dependency 1: Check if the input text is a palindrome\n    is_palindrome = _check_if_palindrome(input_text)\n    \n    # Dependency 2: Count the number of words in the text\n    word_count = _count_words(input_text)\n    \n    return {'is_palindrome': is_palindrome, 'word_count': word_count}", "summary": "Analyzes an **`input_text`** by checking if it is a palindrome (ignoring non-alphanumeric characters and case) using **`_check_if_palindrome`** (defined in **`analyze_text_for_palindrome.py`**), and also counting the total number of words using **`_count_words`** (defined in **`analyze_text_for_palindrome.py`**)."}
{"code": "def _fetch_server_status(server_name):\n    # Mock external health check\n    return {'cpu': 0.15, 'memory': 0.65, 'disk': 0.85}\n\ndef _evaluate_disk_criticality(status_dict, threshold=0.8):\n    return status_dict.get('disk', 0) > threshold\n\ndef check_server_health(target_server):\n    # Dependency 1: Fetch the mock status metrics for the server\n    server_status = _fetch_server_status(target_server)\n    \n    # Dependency 2: Check specifically if the disk usage is critical\n    is_disk_critical = _evaluate_disk_criticality(server_status)\n    \n    return {'status_data': server_status, 'disk_critical': is_disk_critical}", "summary": "Checks the health of a **`target_server`** by fetching mock status metrics (CPU, memory, disk) using **`_fetch_server_status`** (defined in **`check_server_health.py`**) and then evaluating specifically if the disk usage exceeds a critical threshold using **`_evaluate_disk_criticality`** (defined in **`check_server_health.py`**)."}
{"code": "def _parse_query_params(url_with_params):\n    import urllib.parse\n    query = urllib.parse.urlparse(url_with_params).query\n    return urllib.parse.parse_qs(query)\n\ndef _get_first_value(parsed_params, key):\n    value_list = parsed_params.get(key, [])\n    return value_list[0] if value_list else None\n\ndef extract_first_query_value(full_url, target_key):\n    # Dependency 1: Parse the query parameters from the full URL\n    parsed_params = _parse_query_params(full_url)\n    \n    # Dependency 2: Extract the first value associated with the target key\n    first_value = _get_first_value(parsed_params, target_key)\n    \n    return first_value", "summary": "Extracts the first value associated with a **`target_key`** from the query parameters of a **`full_url`**. It uses **`_parse_query_params`** (defined in **`extract_first_query_value.py`**) (relying on the **`urllib.parse`** library) to parse the URL and **`_get_first_value`** (defined in **`extract_first_query_value.py`**) for value retrieval."}
{"code": "def _compute_factorial(n):\n    import math\n    return math.factorial(n)\n\ndef _check_if_greater_than(n, threshold):\n    return n > threshold\n\ndef analyze_factorial_size(input_n, limit):\n    # Dependency 1: Compute the factorial of the input number\n    result = _compute_factorial(input_n)\n    \n    # Dependency 2: Check if the result exceeds a defined limit\n    is_large = _check_if_greater_than(result, limit)\n    \n    return {'factorial': result, 'exceeds_limit': is_large}", "summary": "Analyzes the size of a factorial by first computing the factorial of **`input_n`** using **`_compute_factorial`** (defined in **`analyze_factorial_size.py`**) (which relies on the **`math`** library) and then checking if the result exceeds a specified **`limit`** using **`_check_if_greater_than`** (defined in **`analyze_factorial_size.py`**)."}
{"code": "def _sort_dict_by_value(data_dict, reverse=False):\n    return dict(sorted(data_dict.items(), key=lambda item: item[1], reverse=reverse))\n\ndef _get_top_key(sorted_dict):\n    return next(iter(sorted_dict)) if sorted_dict else None\n\ndef get_key_with_highest_value(input_dict):\n    # Dependency 1: Sort the dictionary items by value in descending order\n    sorted_by_value = _sort_dict_by_value(input_dict, reverse=True)\n    \n    # Dependency 2: Get the key of the first item (the one with the highest value)\n    top_key = _get_top_key(sorted_by_value)\n    \n    return top_key", "summary": "Identifies the key associated with the highest value in an **`input_dict`** by first sorting the dictionary items by value in descending order using **`_sort_dict_by_value`** (defined in **`get_key_with_highest_value.py`**) and then extracting the first key using **`_get_top_key`** (defined in **`get_key_with_highest_value.py`**)."}
{"code": "def _generate_fibonacci_sequence(n):\n    a, b = 0, 1\n    result = []\n    for _ in range(n):\n        result.append(a)\n        a, b = b, a + b\n    return result\n\ndef _filter_even_numbers(sequence):\n    return [x for x in sequence if x % 2 == 0]\n\ndef generate_and_filter_fib(length):\n    # Dependency 1: Generate the first N numbers of the Fibonacci sequence\n    fib_sequence = _generate_fibonacci_sequence(length)\n    \n    # Dependency 2: Filter the sequence to keep only the even numbers\n    even_numbers = _filter_even_numbers(fib_sequence)\n    \n    return even_numbers", "summary": "Generates the Fibonacci sequence up to a specified **`length`** using **`_generate_fibonacci_sequence`** (defined in **`generate_and_filter_fib.py`**) and then filters the resulting sequence to retain only the even numbers using **`_filter_even_numbers`** (defined in **`generate_and_filter_fib.py`**)."}
{"code": "def _validate_date_format(date_str, format_code='%Y-%m-%d'):\n    import datetime\n    try:\n        datetime.datetime.strptime(date_str, format_code)\n        return True\n    except ValueError: return False\n\ndef _get_weekday_name(date_str):\n    import datetime\n    dt_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n    return dt_obj.strftime('%A')\n\ndef analyze_date_and_weekday(date_input):\n    # Dependency 1: Validate the date string format (YYYY-MM-DD)\n    if not _validate_date_format(date_input):\n        raise ValueError(\"Date format invalid.\")\n        \n    # Dependency 2: Get the name of the weekday for the valid date\n    weekday_name = _get_weekday_name(date_input)\n    \n    return weekday_name", "summary": "Analyzes a date string by first validating its YYYY-MM-DD format using **`_validate_date_format`** (defined in **`analyze_date_and_weekday.py`**) (which relies on the **`datetime`** library) and then extracting the full name of the weekday using **`_get_weekday_name`** (defined in **`analyze_date_and_weekday.py`**) (also using **`datetime`**)."}
{"code": "def _get_api_rate_limit_header(response_headers):\n    # Mock header extraction\n    return int(response_headers.get('X-RateLimit-Remaining', 100))\n\ndef _check_if_limit_is_low(remaining_calls, threshold=10):\n    return remaining_calls < threshold\n\ndef monitor_api_quota(response_metadata):\n    # Dependency 1: Extract the remaining calls from the mock response headers\n    remaining_quota = _get_api_rate_limit_header(response_metadata)\n    \n    # Dependency 2: Check if the remaining quota is critically low\n    is_low = _check_if_limit_is_low(remaining_quota)\n    \n    return {'remaining': remaining_quota, 'is_quota_low': is_low}", "summary": "Monitors an API quota by extracting the remaining call count from mock response headers using **`_get_api_rate_limit_header`** (defined in **`monitor_api_quota.py`**) and then checking if this remaining limit is critically low using **`_check_if_limit_is_low`** (defined in **`monitor_api_quota.py`**)."}
{"code": "def _split_into_sentences(text):\n    import re\n    return re.split(r'[.?!]', text.strip())\n\ndef _get_average_word_count(sentences):\n    total_words = sum(len(s.split()) for s in sentences if s.strip())\n    valid_sentences = len([s for s in sentences if s.strip()])\n    return total_words / valid_sentences if valid_sentences else 0\n\ndef analyze_text_readability(document):\n    # Dependency 1: Split the document into sentences\n    sentences = _split_into_sentences(document)\n    \n    # Dependency 2: Calculate the average number of words per sentence\n    avg_words = _get_average_word_count(sentences)\n    \n    return avg_words", "summary": "Analyzes a text **`document`** for readability by first splitting it into sentences using **`_split_into_sentences`** (defined in **`analyze_text_readability.py`**) (which relies on the **`re`** library) and then calculating the average number of words per sentence using **`_get_average_word_count`** (defined in **`analyze_text_readability.py`**)."}
{"code": "def _get_cpu_affinity_mask():\n    import os\n    return os.sched_getaffinity(0)\n\ndef _count_available_cores(affinity_mask):\n    return len(affinity_mask)\n\ndef check_process_cpu_cores():\n    # Dependency 1: Get the current CPU affinity mask for the process\n    affinity = _get_cpu_affinity_mask()\n    \n    # Dependency 2: Count the number of available cores in the mask\n    core_count = _count_available_cores(affinity)\n    \n    return core_count", "summary": "Checks the number of CPU cores available to the current process by retrieving the CPU affinity mask using **`_get_cpu_affinity_mask`** (defined in **`check_process_cpu_cores.py`**) (which relies on the **`os`** library) and then counting the cores in the mask using **`_count_available_cores`** (defined in **`check_process_cpu_cores.py`**)."}
{"code": "def _encrypt_with_caesar(plaintext, shift=3):\n    ciphertext = ''\n    for char in plaintext:\n        if 'a' <= char <= 'z':\n            ciphertext += chr((ord(char) - ord('a') + shift) % 26 + ord('a'))\n        elif 'A' <= char <= 'Z':\n            ciphertext += chr((ord(char) - ord('A') + shift) % 26 + ord('A'))\n        else:\n            ciphertext += char\n    return ciphertext\n\ndef _hash_ciphertext(ciphertext):\n    import hashlib\n    return hashlib.md5(ciphertext.encode()).hexdigest()\n\ndef encrypt_and_hash_text(raw_text):\n    # Dependency 1: Encrypt the raw text using a Caesar cipher\n    encrypted = _encrypt_with_caesar(raw_text)\n    \n    # Dependency 2: Hash the resulting ciphertext using MD5\n    final_hash = _hash_ciphertext(encrypted)\n    \n    return final_hash", "summary": "Encrypts a **`raw_text`** using a Caesar cipher (shift=3) via **`_encrypt_with_caesar`** (defined in **`encrypt_and_hash_text.py`**) and then computes the MD5 hash of the resulting ciphertext using **`_hash_ciphertext`** (defined in **`encrypt_and_hash_text.py`**) (which relies on the **`hashlib`** library)."}
{"code": "def _compute_l2_norm(vector_list):\n    # Euclidean norm (L2)\n    return sum(x**2 for x in vector_list)**0.5\n\ndef _normalize_vector(vector_list, norm):\n    if norm == 0: return [0] * len(vector_list)\n    return [x / norm for x in vector_list]\n\ndef calculate_unit_vector(input_vector):\n    # Dependency 1: Calculate the L2 norm (magnitude) of the vector\n    norm = _compute_l2_norm(input_vector)\n    \n    # Dependency 2: Normalize the original vector by its magnitude\n    unit_vector = _normalize_vector(input_vector, norm)\n    \n    return unit_vector", "summary": "Calculates the unit vector for a given **`input_vector`** by first computing its L2 norm (magnitude) using **`_compute_l2_norm`** (defined in **`calculate_unit_vector.py`**) and then normalizing the vector by dividing each component by the calculated norm using **`_normalize_vector`** (defined in **`calculate_unit_vector.py`**)."}
{"code": "def _generate_api_key_string(length=32):\n    import secrets\n    import string\n    characters = string.ascii_letters + string.digits\n    return ''.join(secrets.choice(characters) for _ in range(length))\n\ndef _store_key_in_vault(key_id, api_key):\n    # Mock secure storage\n    print(f\"Storing key {key_id} in vault.\")\n\ndef create_and_store_api_key(identifier):\n    # Dependency 1: Generate a long, random API key string\n    new_key = _generate_api_key_string()\n    \n    # Dependency 2: Store the key in a mock secure vault\n    _store_key_in_vault(identifier, new_key)\n    \n    return new_key", "summary": "Generates a 32-character random API key string using **`_generate_api_key_string`** (defined in **`create_and_store_api_key.py`**) (relying on the **`secrets`** and **`string`** libraries) and then stores this key in a mock secure vault using **`_store_key_in_vault`** (defined in **`create_and_store_api_key.py`**)."}
{"code": "def _check_if_leap_year(year):\n    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\ndef _get_days_in_year(is_leap):\n    return 366 if is_leap else 365\n\ndef calculate_days_in_year(target_year):\n    # Dependency 1: Check if the target year is a leap year\n    is_leap = _check_if_leap_year(target_year)\n    \n    # Dependency 2: Determine the number of days based on the leap status\n    days = _get_days_in_year(is_leap)\n    \n    return days", "summary": "Calculates the number of days in a **`target_year`** by first checking if the year is a leap year using **`_check_if_leap_year`** (defined in **`calculate_days_in_year.py`**) (following the Gregorian calendar rules) and then using **`_get_days_in_year`** (defined in **`calculate_days_in_year.py`**) to return 366 or 365 days."}
{"code": "def _format_dict_to_key_value_pairs(data_dict):\n    return [f'{k}={v}' for k, v in data_dict.items()]\n\ndef _join_with_ampersand(pair_list):\n    return '&'.join(pair_list)\n\ndef serialize_to_url_query(input_dict):\n    # Dependency 1: Convert the dictionary items into 'key=value' strings\n    key_value_pairs = _format_dict_to_key_value_pairs(input_dict)\n    \n    # Dependency 2: Join the pairs using the ampersand delimiter\n    query_string = _join_with_ampersand(key_value_pairs)\n    \n    return query_string", "summary": "Serializes a dictionary into a URL query string format (e.g., `key1=value1&key2=value2`). It uses **`_format_dict_to_key_value_pairs`** (defined in **`serialize_to_url_query.py`**) to create the pairs and **`_join_with_ampersand`** (defined in **`serialize_to_url_query.py`**) to combine them with `&`."}
{"code": "def _get_current_username():\n    import getpass\n    return getpass.getuser()\n\ndef _check_if_privileged_user(username, privileged_list=['root', 'admin']):\n    return username in privileged_list\n\ndef check_user_privilege_status():\n    # Dependency 1: Get the username of the current running process\n    user = _get_current_username()\n    \n    # Dependency 2: Check if the username is in the list of privileged accounts\n    is_privileged = _check_if_privileged_user(user)\n    \n    return {'username': user, 'is_privileged': is_privileged}", "summary": "Checks the privilege status of the current user by retrieving the current username using **`_get_current_username`** (defined in **`check_user_privilege_status.py`**) (which relies on the **`getpass`** library) and then checking if the username is in a predefined list of privileged accounts via **`_check_if_privileged_user`** (defined in **`check_user_privilege_status.py`**)."}
{"code": "def _calculate_sample_variance(data_list):\n    import statistics\n    return statistics.variance(data_list)\n\ndef _calculate_std_dev_from_variance(variance):\n    import math\n    return math.sqrt(variance)\n\ndef compute_standard_deviation(input_data):\n    # Dependency 1: Calculate the sample variance of the data\n    variance = _calculate_sample_variance(input_data)\n    \n    # Dependency 2: Calculate the standard deviation by taking the square root of the variance\n    std_dev = _calculate_std_dev_from_variance(variance)\n    \n    return std_dev", "summary": "Computes the standard deviation of a dataset by first calculating the sample variance using **`_calculate_sample_variance`** (defined in **`compute_standard_deviation.py`**) (which relies on the **`statistics`** library) and then taking the square root of the variance using **`_calculate_std_dev_from_variance`** (defined in **`compute_standard_deviation.py`**) (which relies on the **`math`** library)."}
{"code": "def _check_if_url_is_internal(url, internal_domain='internal.corp.com'):\n    return internal_domain in url\n\ndef _route_traffic_securely(url):\n    # Mock secure proxy routing\n    print(f\"Routing {url} via secure proxy...\")\n\ndef route_traffic_by_domain(target_url):\n    # Dependency 1: Determine if the URL is internal\n    if _check_if_url_is_internal(target_url):\n        # Dependency 2: Route internal traffic securely (mock)\n        _route_traffic_securely(target_url)\n        return 'SECURE_ROUTE'\n    \n    return 'DIRECT_ROUTE'", "summary": "Routes traffic for a **`target_url`** by first determining if the URL belongs to a predefined internal domain using **`_check_if_url_is_internal`** (defined in **`route_traffic_by_domain.py`**), and if it is, performs a mock secure routing action using **`_route_traffic_securely`** (defined in **`route_traffic_by_domain.py`**)."}
{"code": "def _read_data_from_jsonl(file_path):\n    # Mock reading JSONL file\n    data = [{'id': 1, 'v': 'a'}, {'id': 2, 'v': 'b'}]\n    return data\n\ndef _transform_to_tuple_list(data_list, key_col):\n    return [(item[key_col], item['v']) for item in data_list]\n\ndef load_and_transform_data(data_file, primary_key):\n    # Dependency 1: Load the list of records from the mock JSONL file\n    raw_records = _read_data_from_jsonl(data_file)\n    \n    # Dependency 2: Transform the records into a list of (key, value) tuples\n    tuple_list = _transform_to_tuple_list(raw_records, primary_key)\n    \n    return tuple_list", "summary": "Loads records from a mock JSONL file using **`_read_data_from_jsonl`** (defined in **`load_and_transform_data.py`**) and then transforms the list of records into a list of (primary\\_key, 'v') tuples using **`_transform_to_tuple_list`** (defined in **`load_and_transform_data.py`**)."}
{"code": "def _calculate_levenshtein_distance(s1, s2):\n    # Mock function for edit distance (simulated)\n    import Levenshtein\n    return Levenshtein.distance(s1, s2)\n\ndef _is_similarity_high(distance, max_len, threshold_ratio=0.2):\n    return distance / max_len < threshold_ratio\n\ndef compare_string_similarity(str_a, str_b):\n    import Levenshtein\n    # Dependency 1: Calculate the Levenshtein (edit) distance\n    distance = _calculate_levenshtein_distance(str_a, str_b)\n    \n    # Dependency 2: Check if the similarity is high (distance is low relative to length)\n    max_len = max(len(str_a), len(str_b))\n    is_similar = _is_similarity_high(distance, max_len)\n    \n    return {'distance': distance, 'is_highly_similar': is_similar}", "summary": "Compares the similarity between two strings by first calculating the Levenshtein (edit) distance using **`_calculate_levenshtein_distance`** (defined in **`compare_string_similarity.py`**) (which relies on the **`Levenshtein`** library) and then checking if the distance is low relative to the string length using **`_is_similarity_high`** (defined in **`compare_string_similarity.py`**)."}
{"code": "def _check_if_list_contains_duplicates(input_list):\n    return len(input_list) != len(set(input_list))\n\ndef _log_list_uniqueness(is_duplicate):\n    status = 'DUPLICATES FOUND' if is_duplicate else 'All unique'\n    print(f\"List uniqueness check: {status}\")\n\ndef audit_list_for_uniqueness(data_to_audit):\n    # Dependency 1: Check if the list contains any duplicate elements\n    has_duplicates = _check_if_list_contains_duplicates(data_to_audit)\n    \n    # Dependency 2: Log the result of the uniqueness check\n    _log_list_uniqueness(has_duplicates)\n    \n    return has_duplicates", "summary": "Audits a list for uniqueness by checking if the list contains any duplicate elements using **`_check_if_list_contains_duplicates`** (defined in **`audit_list_for_uniqueness.py`**) (by comparing list length to set length) and then logging the result using **`_log_list_uniqueness`** (defined in **`audit_list_for_uniqueness.py`**)."}
{"code": "def _send_metrics_to_monitoring(metrics_dict):\n    # Mock sending data to Prometheus/StatsD\n    for key, value in metrics_dict.items():\n        print(f\"[METRIC] {key}: {value}\")\n\ndef _generate_timestamp_metric(metric_name):\n    import time\n    return {metric_name: time.time()}\n\ndef emit_timestamp_metric(metric_id):\n    # Dependency 1: Generate a metric dictionary containing the current timestamp\n    metric_payload = _generate_timestamp_metric(metric_id)\n    \n    # Dependency 2: Send the metric to the mock monitoring system\n    _send_metrics_to_monitoring(metric_payload)\n    \n    return metric_payload", "summary": "Emits a timestamp metric by first generating a metric dictionary containing the current epoch time using **`_generate_timestamp_metric`** (defined in **`emit_timestamp_metric.py`**) (which relies on the **`time`** library) and then sending this payload to a mock monitoring system using **`_send_metrics_to_monitoring`** (defined in **`emit_timestamp_metric.py`**)."}
{"code": "def _compute_sha256_hash(data):\n    import hashlib\n    return hashlib.sha256(data.encode('utf-8')).hexdigest()\n\ndef _verify_data_integrity(data, expected_hash):\n    calculated_hash = _compute_sha256_hash(data)\n    return calculated_hash == expected_hash\n\ndef check_data_integrity(input_data, integrity_hash):\n    # Dependency 1: Compute the SHA256 hash of the input data\n    # Note: _verify_data_integrity implicitly calls _compute_sha256_hash\n    \n    # Dependency 2: Compare the calculated hash with the expected hash\n    is_intact = _verify_data_integrity(input_data, integrity_hash)\n    \n    return is_intact", "summary": "Checks the data integrity of **`input_data`** by comparing its calculated SHA256 hash against an **`integrity_hash`**. The function directly calls **`_verify_data_integrity`** (defined in **`check_data_integrity.py`**), which internally uses **`_compute_sha256_hash`** (defined in **`check_data_integrity.py`**) (relying on the **`hashlib`** library) to perform the computation."}
{"code": "def _generate_random_integer(min_val, max_val):\n    import random\n    return random.randint(min_val, max_val)\n\ndef _check_if_number_is_prime(n):\n    if n <= 1: return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0: return False\n    return True\n\ndef generate_and_check_prime(lower_bound, upper_bound):\n    # Dependency 1: Generate a random integer within the bounds\n    random_num = _generate_random_integer(lower_bound, upper_bound)\n    \n    # Dependency 2: Check if the generated number is a prime number\n    is_prime = _check_if_number_is_prime(random_num)\n    \n    return {'number': random_num, 'is_prime': is_prime}", "summary": "Generates a random integer within specified bounds using **`_generate_random_integer`** (defined in **`generate_and_check_prime.py`**) (which relies on the **`random`** library) and then checks if the generated number is a prime number using **`_check_if_number_is_prime`** (defined in **`generate_and_check_prime.py`**)."}
{"code": "def _get_request_method(request_obj):\n    # Mock request object\n    return request_obj.get('method', 'GET')\n\ndef _is_safe_method(method):\n    return method in ['GET', 'HEAD', 'OPTIONS']\n\ndef check_http_request_safety(mock_request):\n    # Dependency 1: Get the HTTP method from the mock request object\n    method = _get_request_method(mock_request)\n    \n    # Dependency 2: Determine if the method is considered 'safe' (read-only)\n    is_safe = _is_safe_method(method)\n    \n    return {'method': method, 'is_safe': is_safe}", "summary": "Checks the safety of a mock HTTP request by extracting the request method using **`_get_request_method`** (defined in **`check_http_request_safety.py`**) and then determining if the method is read-only ('GET', 'HEAD', 'OPTIONS') using **`_is_safe_method`** (defined in **`check_http_request_safety.py`**)."}
{"code": "def _get_file_extension(filename):\n    import os\n    return os.path.splitext(filename)[1].lower()\n\ndef _check_if_allowed_extension(ext, allowed_list=['.jpg', '.png', '.gif']):\n    return ext in allowed_list\n\ndef validate_file_extension(target_file, allowed_extensions):\n    # Dependency 1: Extract the file extension\n    extension = _get_file_extension(target_file)\n    \n    # Dependency 2: Check if the extension is in the list of allowed extensions\n    is_allowed = _check_if_allowed_extension(extension, allowed_extensions)\n    \n    return {'extension': extension, 'is_allowed': is_allowed}", "summary": "Validates a **`target_file`** by first extracting its extension using **`_get_file_extension`** (defined in **`validate_file_extension.py`**) (which relies on the **`os`** library) and then checking if the extension is present in the **`allowed_extensions`** list using **`_check_if_allowed_extension`** (defined in **`validate_file_extension.py`**)."}
{"code": "def _fetch_user_details(user_id):\n    # Mock database retrieval\n    return {'id': user_id, 'status': 'active', 'last_login': '2025-11-20'}\n\ndef _check_account_status(user_details, required_status='active'):\n    return user_details.get('status') == required_status\n\ndef verify_user_active_status(uid):\n    # Dependency 1: Fetch the user details from the mock database\n    details = _fetch_user_details(uid)\n    \n    # Dependency 2: Check if the user's status is 'active'\n    is_active = _check_account_status(details)\n    \n    return is_active", "summary": "Verifies if a user with a given **`uid`** is active by first fetching their details from a mock database using **`_fetch_user_details`** (defined in **`verify_user_active_status.py`**) and then checking the 'status' field against the required 'active' status using **`_check_account_status`** (defined in **`verify_user_active_status.py`**)."}
{"code": "def _compute_haversine_distance(lat1, lon1, lat2, lon2):\n    # Mock Haversine calculation (only checks difference)\n    import math\n    lat_diff = math.radians(lat2 - lat1)\n    lon_diff = math.radians(lon2 - lon1)\n    return (lat_diff**2 + lon_diff**2)**0.5 # Simplified distance\n\ndef _check_if_near(distance, max_distance=0.1):\n    return distance <= max_distance\n\ndef analyze_location_proximity(coord_a, coord_b):\n    # Dependency 1: Compute the mock geographical distance (Haversine-like)\n    distance = _compute_haversine_distance(coord_a[0], coord_a[1], coord_b[0], coord_b[1])\n    \n    # Dependency 2: Check if the computed distance is within the 'near' threshold\n    is_near = _check_if_near(distance)\n    \n    return {'distance': distance, 'is_near': is_near}", "summary": "Analyzes the geographical proximity of two coordinate pairs by calculating a mock Haversine distance using **`_compute_haversine_distance`** (defined in **`analyze_location_proximity.py`**) (which relies on the **`math`** library) and then checking if that distance is below a threshold using **`_check_if_near`** (defined in **`analyze_location_proximity.py`**)."}
{"code": "def _remove_stopwords(word_list, stopwords=set(['a', 'the', 'is', 'in'])):\n    return [word for word in word_list if word.lower() not in stopwords]\n\ndef _calculate_tfidf_score(filtered_words):\n    # Mock TF-IDF score calculation based on word count\n    return len(filtered_words)\n\ndef filter_and_score_words(tokenized_text):\n    # Dependency 1: Remove common stopwords from the list of tokens\n    cleaned_tokens = _remove_stopwords(tokenized_text)\n    \n    # Dependency 2: Calculate a mock TF-IDF score based on the remaining words\n    score = _calculate_tfidf_score(cleaned_tokens)\n    \n    return {'cleaned_tokens': cleaned_tokens, 'mock_score': score}", "summary": "Processes a list of **`tokenized_text`** by first removing common stopwords using **`_remove_stopwords`** (defined in **`filter_and_score_words.py`**) and then calculating a mock TF-IDF score based on the count of the remaining words using **`_calculate_tfidf_score`** (defined in **`filter_and_score_words.py`**)."}
{"code": "def _encode_url_component(value):\n    import urllib.parse\n    return urllib.parse.quote_plus(str(value))\n\ndef _create_url_segment(key, encoded_value):\n    return f'{key}/{encoded_value}'\n\ndef build_url_segment(segment_key, raw_value):\n    # Dependency 1: URL-encode the raw input value\n    encoded_value = _encode_url_component(raw_value)\n    \n    # Dependency 2: Combine the key and the encoded value into a path segment\n    url_segment = _create_url_segment(segment_key, encoded_value)\n    \n    return url_segment", "summary": "Builds a URL path segment by first URL-encoding the **`raw_value`** using **`_encode_url_component`** (defined in **`build_url_segment.py`**) (which relies on the **`urllib.parse`** library) and then combining it with the **`segment_key`** using **`_create_url_segment`** (defined in **`build_url_segment.py`**)."}
{"code": "def _check_if_dictionary(obj):\n    return isinstance(obj, dict)\n\ndef _get_all_dict_keys(data_dict):\n    return list(data_dict.keys())\n\ndef inspect_object_keys(data_input):\n    # Dependency 1: Check if the input object is a dictionary\n    if not _check_if_dictionary(data_input):\n        return {'is_dict': False, 'keys': []}\n        \n    # Dependency 2: Get a list of all keys in the dictionary\n    keys = _get_all_dict_keys(data_input)\n    \n    return {'is_dict': True, 'keys': keys}", "summary": "Inspects an **`data_input`** object to determine if it is a dictionary using **`_check_if_dictionary`** (defined in **`inspect_object_keys.py`**) and, if so, retrieves a list of all its keys using **`_get_all_dict_keys`** (defined in **`inspect_object_keys.py`**)."}
{"code": "def _get_process_memory_info():\n    import psutil\n    process = psutil.Process()\n    return process.memory_info().rss / (1024 * 1024) # RSS in MB\n\ndef _check_if_high_memory(memory_mb, limit_mb=500):\n    return memory_mb > limit_mb\n\ndef monitor_process_memory():\n    # Dependency 1: Get the current Resident Set Size (RSS) memory usage in MB\n    rss_mb = _get_process_memory_info()\n    \n    # Dependency 2: Check if the memory usage exceeds a high-water mark limit\n    is_high = _check_if_high_memory(rss_mb)\n    \n    return {'rss_mb': rss_mb, 'is_high_memory': is_high}", "summary": "Monitors the current process's memory usage by retrieving the Resident Set Size (RSS) in megabytes using **`_get_process_memory_info`** (defined in **`monitor_process_memory.py`**) (which relies on the external **`psutil`** library) and then checking if the usage exceeds a defined limit using **`_check_if_high_memory`** (defined in **`monitor_process_memory.py`**)."}
{"code": "def _remove_non_numeric_chars(text):\n    import re\n    return re.sub(r'[^0-9]', '', text)\n\ndef _sum_digits(numeric_string):\n    return sum(int(d) for d in numeric_string)\n\ndef calculate_digit_sum(input_string):\n    # Dependency 1: Remove all non-numeric characters from the string\n    numeric_only = _remove_non_numeric_chars(input_string)\n    \n    # Dependency 2: Calculate the sum of all remaining digits\n    digit_sum = _sum_digits(numeric_only)\n    \n    return digit_sum", "summary": "Calculates the sum of all digits within an **`input_string`** by first removing all non-numeric characters using **`_remove_non_numeric_chars`** (defined in **`calculate_digit_sum.py`**) (which relies on the **`re`** library) and then summing the remaining digits using **`_sum_digits`** (defined in **`calculate_digit_sum.py`**)."}
{"code": "def _execute_mock_graphql_query(query, variables):\n    # Mock GraphQL execution\n    return {'data': {'user': {'id': variables.get('id', 0), 'name': 'GraphQL User'}} }\n\ndef _extract_field_value(response, path_key='data.user.name'):\n    path = path_key.split('.')\n    value = response\n    for key in path:\n        value = value.get(key, None)\n        if value is None: break\n    return value\n\ndef query_and_extract_user_name(graphql_query, user_id):\n    # Dependency 1: Execute the mock GraphQL query\n    response = _execute_mock_graphql_query(graphql_query, {'id': user_id})\n    \n    # Dependency 2: Extract the user's name using a path key\n    user_name = _extract_field_value(response)\n    \n    return user_name", "summary": "Executes a mock GraphQL query using **`_execute_mock_graphql_query`** (defined in **`query_and_extract_user_name.py`**) with a provided **`user_id`** as a variable, and then extracts the resulting user's name from the structured JSON response using **`_extract_field_value`** (defined in **`query_and_extract_user_name.py`**)."}
{"code": "def _get_z_score(value, mean, std_dev):\n    if std_dev == 0: return 0\n    return (value - mean) / std_dev\n\ndef _classify_as_outlier(z_score, threshold=3.0):\n    return abs(z_score) > threshold\n\ndef analyze_data_for_outlier(data_point, dataset_mean, dataset_std):\n    # Dependency 1: Calculate the Z-score for the data point\n    z_score = _get_z_score(data_point, dataset_mean, dataset_std)\n    \n    # Dependency 2: Classify the point as an outlier if the Z-score exceeds the threshold\n    is_outlier = _classify_as_outlier(z_score)\n    \n    return {'z_score': z_score, 'is_outlier': is_outlier}", "summary": "Analyzes a **`data_point`** to determine if it is an outlier by first calculating its Z-score (deviation from the mean in standard deviations) using **`_get_z_score`** (defined in **`analyze_data_for_outlier.py`**) and then classifying it against a threshold using **`_classify_as_outlier`** (defined in **`analyze_data_for_outlier.py`**)."}
{"code": "def _get_data_from_redis(key):\n    # Mock Redis cache retrieval\n    return {'key': key, 'data': 'data_from_redis'}\n\ndef _log_cache_hit(key):\n    print(f\"[CACHE] HIT: Retrieved key {key}\")\n\ndef retrieve_and_log_cache(cache_key):\n    # Dependency 1: Retrieve the data from the mock Redis cache\n    data = _get_data_from_redis(cache_key)\n    \n    # Dependency 2: Log the successful cache hit\n    _log_cache_hit(cache_key)\n    \n    return data", "summary": "Retrieves data from a mock Redis cache using **`_get_data_from_redis`** (defined in **`retrieve_and_log_cache.py`**) based on a **`cache_key`** and then logs the successful cache hit using **`_log_cache_hit`** (defined in **`retrieve_and_log_cache.py`**)."}
{"code": "def _render_template_jinja2(template_name, context):\n    # Mock Jinja2 rendering\n    return f\"RENDERED: {template_name} with user {context.get('user')}\"\n\ndef _send_rendered_html_email(html_body, recipient):\n    # Mock email sending service\n    print(f\"Email sent to {recipient} with body length {len(html_body)}\")\n\ndef send_templated_email(recipient_email, template_id, template_context):\n    # Dependency 1: Render the template using the context data\n    rendered_html = _render_template_jinja2(template_id, template_context)\n    \n    # Dependency 2: Send the resulting HTML email to the recipient\n    _send_rendered_html_email(rendered_html, recipient_email)\n    \n    return rendered_html", "summary": "Sends a templated email by first rendering the specified **`template_id`** with **`template_context`** using the mock function **`_render_template_jinja2`** (defined in **`send_templated_email.py`**) and then sending the resulting HTML body to the **`recipient_email`** via **`_send_rendered_html_email`** (defined in **`send_templated_email.py`**)."}
{"code": "def _check_if_tuple(obj):\n    return isinstance(obj, tuple)\n\ndef _get_tuple_size(data_tuple):\n    return len(data_tuple)\n\ndef inspect_tuple_size(input_object):\n    # Dependency 1: Check if the input object is a tuple\n    if not _check_if_tuple(input_object):\n        return {'is_tuple': False, 'size': 0}\n        \n    # Dependency 2: Get the number of elements in the tuple\n    tuple_size = _get_tuple_size(input_object)\n    \n    return {'is_tuple': True, 'size': tuple_size}", "summary": "Inspects an **`input_object`** to determine if it is a tuple using **`_check_if_tuple`** (defined in **`inspect_tuple_size.py`**) and, if so, returns the number of elements in the tuple using **`_get_tuple_size`** (defined in **`inspect_tuple_size.py`**)."}
{"code": "def _extract_text_between_delimiters(text, start_delim='[', end_delim=']'):\n    import re\n    match = re.search(re.escape(start_delim) + '(.*?)' + re.escape(end_delim), text)\n    return match.group(1) if match else None\n\ndef _clean_whitespace(text):\n    return text.strip() if text else None\n\ndef extract_and_clean_bracketed_text(source_text):\n    # Dependency 1: Extract the substring between the default delimiters (brackets)\n    extracted = _extract_text_between_delimiters(source_text)\n    \n    # Dependency 2: Clean leading/trailing whitespace from the extracted text\n    cleaned = _clean_whitespace(extracted)\n    \n    return cleaned", "summary": "Extracts the text found between square brackets (`[]`) in **`source_text`** using **`_extract_text_between_delimiters`** (defined in **`extract_and_clean_bracketed_text.py`**) (which relies on the **`re`** library) and then cleans any leading/trailing whitespace from the extracted string using **`_clean_whitespace`** (defined in **`extract_and_clean_bracketed_text.py`**)."}
{"code": "def _calculate_chebyshev_distance(v1, v2):\n    # Maximum coordinate difference (L-infinity norm)\n    return max(abs(a - b) for a, b in zip(v1, v2))\n\ndef _is_distance_within_budget(distance, budget):\n    return distance <= budget\n\ndef check_distance_budget(vector_a, vector_b, distance_limit):\n    # Dependency 1: Calculate the Chebyshev distance between the two vectors\n    chebyshev_dist = _calculate_chebyshev_distance(vector_a, vector_b)\n    \n    # Dependency 2: Check if the calculated distance is within the budget\n    is_within_budget = _is_distance_within_budget(chebyshev_dist, distance_limit)\n    \n    return {'distance': chebyshev_dist, 'within_budget': is_within_budget}", "summary": "Checks if the distance between two vectors (**`vector_a`** and **`vector_b`**) is within a **`distance_limit`** by first calculating the Chebyshev distance (L-infinity norm) using **`_calculate_chebyshev_distance`** (defined in **`check_distance_budget.py`**) and then checking the limit using **`_is_distance_within_budget`** (defined in **`check_distance_budget.py`**)."}
{"code": "def _get_process_id():\n    import os\n    return os.getpid()\n\ndef _format_log_message(pid, message):\n    return f\"[PID {pid}] {message}\"\n\ndef log_with_process_id(log_message):\n    # Dependency 1: Retrieve the Process ID (PID) of the current process\n    pid = _get_process_id()\n    \n    # Dependency 2: Format the log message to include the PID\n    formatted_log = _format_log_message(pid, log_message)\n    \n    print(formatted_log)\n    return formatted_log", "summary": "Formats a **`log_message`** by first retrieving the current process ID (PID) using **`_get_process_id`** (defined in **`log_with_process_id.py`**) (which relies on the **`os`** library) and then inserting the PID into the message using **`_format_log_message`** (defined in **`log_with_process_id.py`**)."}
{"code": "def _get_list_unique_values(input_list):\n    return sorted(list(set(input_list)))\n\ndef _calculate_mode_simple(unique_values):\n    return unique_values[0] if unique_values else None\n\ndef get_sorted_unique_values(data_list):\n    # Dependency 1: Extract all unique values from the list and sort them\n    unique_sorted = _get_list_unique_values(data_list)\n    \n    # Dependency 2: Get the first element (mock simple mode)\n    simple_mode = _calculate_mode_simple(unique_sorted)\n    \n    return {'unique_values': unique_sorted, 'simple_mode': simple_mode}", "summary": "Extracts the unique values from a **`data_list`**, returns them as a sorted list using **`_get_list_unique_values`** (defined in **`get_sorted_unique_values.py`**), and also returns the first unique value as a mock 'simple mode' using **`_calculate_mode_simple`** (defined in **`get_sorted_unique_values.py`**)."}
{"code": "def _escape_html_entities(raw_text):\n    import html\n    return html.escape(raw_text)\n\ndef _wrap_in_pre_tag(escaped_text):\n    return f\"<pre>{escaped_text}</pre>\"\n\ndef display_text_safely_in_html(unsafe_text):\n    # Dependency 1: Escape special characters in the text to prevent XSS/injection\n    safe_text = _escape_html_entities(unsafe_text)\n    \n    # Dependency 2: Wrap the safe text in an HTML <pre> tag\n    html_output = _wrap_in_pre_tag(safe_text)\n    \n    return html_output", "summary": "Formats raw text for safe display in HTML by first escaping special characters using **`_escape_html_entities`** (defined in **`display_text_safely_in_html.py`**) (which relies on the **`html`** library) and then wrapping the result in an HTML `<pre>` tag using **`_wrap_in_pre_tag`** (defined in **`display_text_safely_in_html.py`**)."}
{"code": "def _get_timestamp_seconds():\n    import time\n    return int(time.time())\n\ndef _check_if_expired(timestamp_sec, expiration_sec, current_sec):\n    return current_sec > (timestamp_sec + expiration_sec)\n\ndef check_token_expiration(creation_timestamp, lifetime_seconds):\n    # Dependency 1: Get the current epoch time in seconds\n    current_time = _get_timestamp_seconds()\n    \n    # Dependency 2: Check if the token has expired based on its creation time and lifetime\n    is_expired = _check_if_expired(creation_timestamp, lifetime_seconds, current_time)\n    \n    return is_expired", "summary": "Checks if a token has expired by first retrieving the current epoch time in seconds using **`_get_timestamp_seconds`** (defined in **`check_token_expiration.py`**) (which relies on the **`time`** library) and then comparing the token's **`creation_timestamp`** plus its **`lifetime_seconds`** against the current time using **`_check_if_expired`** (defined in **`check_token_expiration.py`**)."}
{"code": "def _compute_poisson_probability(k, lambda_val):\n    import math\n    # Mock calculation of P(X=k) using Poisson formula\n    return (lambda_val**k * math.exp(-lambda_val)) / math.factorial(k)\n\ndef _check_if_low_probability(prob, threshold=0.01):\n    return prob < threshold\n\ndef analyze_poisson_event_likelihood(k_events, lambda_rate):\n    # Dependency 1: Compute the Poisson probability for k events given lambda\n    probability = _compute_poisson_probability(k_events, lambda_rate)\n    \n    # Dependency 2: Check if the computed probability is critically low\n    is_low = _check_if_low_probability(probability)\n    \n    return {'probability': probability, 'is_low_likelihood': is_low}", "summary": "Analyzes the likelihood of an event by calculating the Poisson probability of **`k_events`** occurring given a **`lambda_rate`** using **`_compute_poisson_probability`** (defined in **`analyze_poisson_event_likelihood.py`**) (which relies on the **`math`** library) and then checking if the probability is critically low using **`_check_if_low_probability`** (defined in **`analyze_poisson_event_likelihood.py`**)."}
{"code": "def _clean_string_for_slug(text):\n    import re\n    text = text.lower()\n    text = re.sub(r'[^a-z0-9\\s-]', '', text)\n    text = re.sub(r'\\s+', '-', text)\n    return text.strip('-')\n\ndef _add_unique_suffix(slug, suffix_id):\n    return f'{slug}-{suffix_id}'\n\ndef generate_unique_slug(title, identifier):\n    # Dependency 1: Clean and format the title into a URL slug\n    base_slug = _clean_string_for_slug(title)\n    \n    # Dependency 2: Append a unique identifier to the base slug\n    unique_slug = _add_unique_suffix(base_slug, identifier)\n    \n    return unique_slug", "summary": "Generates a unique URL slug by first cleaning and formatting a **`title`** string using **`_clean_string_for_slug`** (defined in **`generate_unique_slug.py`**) (which relies on the **`re`** library) and then appending a unique **`identifier`** using **`_add_unique_suffix`** (defined in **`generate_unique_slug.py`**)."}
{"code": "def _get_total_system_memory():\n    import psutil\n    return psutil.virtual_memory().total // (1024 * 1024) # Total in MB\n\ndef _convert_to_gigabytes(memory_mb):\n    return memory_mb / 1024\n\ndef get_system_memory_in_gb():\n    # Dependency 1: Get the total system memory in megabytes\n    total_mb = _get_total_system_memory()\n    \n    # Dependency 2: Convert the total memory from megabytes to gigabytes\n    total_gb = _convert_to_gigabytes(total_mb)\n    \n    return total_gb", "summary": "Retrieves the total system memory in gigabytes by first getting the total memory in megabytes using **`_get_total_system_memory`** (defined in **`get_system_memory_in_gb.py`**) (which relies on the external **`psutil`** library) and then converting the MB value to GB using **`_convert_to_gigabytes`** (defined in **`get_system_memory_in_gb.py`**)."}
{"code": "def _send_s3_upload(file_path, bucket_name):\n    # Mock S3 SDK call\n    return f\"Successfully uploaded {file_path} to S3 bucket {bucket_name}\"\n\ndef _log_upload_time(upload_message):\n    import time\n    print(f\"[{time.ctime()}] {upload_message}\")\n\ndef upload_file_and_log(local_path, s3_target):\n    # Dependency 1: Execute the mock S3 upload\n    upload_status = _send_s3_upload(local_path, s3_target)\n    \n    # Dependency 2: Log the upload status with a timestamp\n    _log_upload_time(upload_status)\n    \n    return upload_status", "summary": "Uploads a local file specified by **`local_path`** to a mock S3 bucket using **`_send_s3_upload`** (defined in **`upload_file_and_log.py`**) and then logs the successful upload message along with a timestamp using **`_log_upload_time`** (defined in **`upload_file_and_log.py`**) (which relies on the **`time`** library)."}
{"code": "def _parse_semver(version_string):\n    import re\n    match = re.match(r'v?(\\d+)\\.(\\d+)\\.(\\d+)', version_string)\n    if match: return [int(g) for g in match.groups()]\n    return [0, 0, 0]\n\ndef _is_patch_version(v1, v2):\n    return v1[0] == v2[0] and v1[1] == v2[1] and v1[2] != v2[2]\n\ndef check_semver_patch_release(version_a, version_b):\n    # Dependency 1: Parse both version strings into integer lists (major, minor, patch)\n    parsed_a = _parse_semver(version_a)\n    parsed_b = _parse_semver(version_b)\n    \n    # Dependency 2: Check if the change is only in the patch number\n    is_patch = _is_patch_version(parsed_a, parsed_b)\n    \n    return is_patch", "summary": "Checks if the difference between **`version_a`** and **`version_b`** is only a patch version change by first parsing both semantic version strings into major, minor, and patch integer lists using **`_parse_semver`** (defined in **`check_semver_patch_release.py`**) (which relies on the **`re`** library) and then comparing the components using **`_is_patch_version`** (defined in **`check_semver_patch_release.py`**)."}
{"code": "def _read_data_from_ini(file_path):\n    import configparser\n    config = configparser.ConfigParser()\n    config.read(file_path)\n    return {s: dict(config.items(s)) for s in config.sections()}\n\ndef _find_section_with_key(full_config, key):\n    for section, settings in full_config.items():\n        if key in settings: return section\n    return None\n\ndef find_config_section_by_key(ini_file, target_key):\n    # Dependency 1: Read the full configuration from the INI file\n    config_data = _read_data_from_ini(ini_file)\n    \n    # Dependency 2: Find which section contains the target key\n    found_section = _find_section_with_key(config_data, target_key)\n    \n    return found_section", "summary": "Finds which section of an INI configuration file contains a **`target_key`** by first reading and parsing the entire configuration using **`_read_data_from_ini`** (defined in **`find_config_section_by_key.py`**) (which relies on the **`configparser`** library) and then iterating through the sections using **`_find_section_with_key`** (defined in **`find_config_section_by_key.py`**)."}
{"code": "def _compute_weighted_average(values, weights):\n    if not weights: return 0\n    return sum(v * w for v, w in zip(values, weights)) / sum(weights)\n\ndef _get_value_median(values):\n    import statistics\n    return statistics.median(values)\n\ndef analyze_data_centrality_weighted(data_points, factor_weights):\n    # Dependency 1: Compute the weighted average of the data points\n    w_avg = _compute_weighted_average(data_points, factor_weights)\n    \n    # Dependency 2: Calculate the simple median of the data points\n    median = _get_value_median(data_points)\n    \n    return {'weighted_average': w_avg, 'median': median}", "summary": "Analyzes the centrality of a dataset by calculating both the weighted average using **`_compute_weighted_average`** (defined in **`analyze_data_centrality_weighted.py`**) and the simple median using **`_get_value_median`** (defined in **`analyze_data_centrality_weighted.py`**) (which relies on the **`statistics`** library)."}
{"code": "def _validate_mac_address(mac_string):\n    import re\n    # Simple check for XX:XX:XX:XX:XX:XX format\n    return bool(re.match(r'^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$', mac_string))\n\ndef _get_vendor_prefix(mac_string):\n    return mac_string.split(':')[0:3]\n\ndef analyze_mac_address(mac_input):\n    # Dependency 1: Validate the MAC address format\n    if not _validate_mac_address(mac_input):\n        raise ValueError(\"Invalid MAC address format.\")\n        \n    # Dependency 2: Extract the vendor prefix (first three segments)\n    vendor_prefix = _get_vendor_prefix(mac_input)\n    \n    return vendor_prefix", "summary": "Analyzes a MAC address by first validating its format using **`_validate_mac_address`** (defined in **`analyze_mac_address.py`**) (which relies on the **`re`** library) and then extracting the vendor prefix (the first three segments) using **`_get_vendor_prefix`** (defined in **`analyze_mac_address.py`**)."}
{"code": "def _generate_csv_string(header, data_rows):\n    import csv\n    from io import StringIO\n    output = StringIO()\n    writer = csv.writer(output)\n    writer.writerow(header)\n    writer.writerows(data_rows)\n    return output.getvalue()\n\ndef _log_data_size(csv_data):\n    print(f\"Generated CSV string is {len(csv_data)} bytes long.\")\n\ndef serialize_to_csv_and_log_size(column_names, records):\n    # Dependency 1: Generate the CSV data string\n    csv_output = _generate_csv_string(column_names, records)\n    \n    # Dependency 2: Log the size of the generated CSV string\n    _log_data_size(csv_output)\n    \n    return csv_output", "summary": "Serializes header and record data into a CSV string using **`_generate_csv_string`** (defined in **`serialize_to_csv_and_log_size.py`**) (which relies on the **`csv`** and **`io`** libraries) and then logs the byte size of the resulting string using **`_log_data_size`** (defined in **`serialize_to_csv_and_log_size.py`**)."}
{"code": "def _create_date_object(year, month, day):\n    import datetime\n    return datetime.date(year, month, day)\n\ndef _calculate_days_until(start_date, end_date):\n    return (end_date - start_date).days\n\ndef compute_days_difference(year1, month1, day1, year2, month2, day2):\n    import datetime\n    # Dependency 1: Create the starting date object\n    date_a = _create_date_object(year1, month1, day1)\n    \n    # Dependency 2: Create the ending date object\n    date_b = _create_date_object(year2, month2, day2)\n    \n    # Dependency 3: Calculate the difference in days\n    days_diff = _calculate_days_until(date_a, date_b)\n    \n    return days_diff", "summary": "Computes the difference in days between two dates specified by their year, month, and day components. It uses **`_create_date_object`** (defined in **`compute_days_difference.py`**) (relying on the **`datetime`** library) to construct the date objects and **`_calculate_days_until`** (defined in **`compute_days_difference.py`**) for the subtraction."}
{"code": "def _find_duplicate_keys(dict_list):\n    import collections\n    all_keys = [k for d in dict_list for k in d.keys()]\n    key_counts = collections.Counter(all_keys)\n    return [k for k, count in key_counts.items() if count > 1]\n\ndef _log_duplicates(duplicate_keys):\n    if duplicate_keys: print(f\"Warning: Found duplicated keys in dictionaries: {duplicate_keys}\")\n\ndef audit_list_of_dicts_for_key_overlap(list_of_dicts):\n    # Dependency 1: Find all keys that appear in more than one dictionary\n    duplicates = _find_duplicate_keys(list_of_dicts)\n    \n    # Dependency 2: Log a warning if duplicates are found\n    _log_duplicates(duplicates)\n    \n    return duplicates", "summary": "Audits a list of dictionaries to find keys that are present in more than one dictionary, using **`_find_duplicate_keys`** (defined in **`audit_list_of_dicts_for_key_overlap.py`**) (which relies on the **`collections`** library) and logs a warning if duplicates are found using **`_log_duplicates`** (defined in **`audit_list_of_dicts_for_key_overlap.py`**)."}
{"code": "def _calculate_moving_average(data, window_size):\n    # Mock simple moving average\n    import numpy\n    weights = numpy.ones(window_size) / window_size\n    return numpy.convolve(data, weights, 'valid')\n\ndef _get_trend_direction(avg_series):\n    if not avg_series: return 'N/A'\n    if avg_series[-1] > avg_series[0]: return 'Upward'\n    if avg_series[-1] < avg_series[0]: return 'Downward'\n    return 'Stable'\n\ndef analyze_data_trend(series, window):\n    # Dependency 1: Calculate the moving average of the time series\n    moving_avg = _calculate_moving_average(series, window)\n    \n    # Dependency 2: Determine the overall trend direction (Upward, Downward, Stable)\n    trend = _get_trend_direction(moving_avg)\n    \n    return {'moving_average': moving_avg.tolist(), 'trend': trend}", "summary": "Analyzes a data series for trend direction by first calculating a simple moving average over a specified **`window`** using **`_calculate_moving_average`** (defined in **`analyze_data_trend.py`**) (which relies on the external **`numpy`** library) and then determining the trend (Upward/Downward/Stable) from the start and end of the average series using **`_get_trend_direction`** (defined in **`analyze_data_trend.py`**)."}
{"code": "def _get_api_key_from_env(key_name):\n    import os\n    return os.environ.get(key_name)\n\ndef _check_if_present(key_value):\n    return key_value is not None\n\ndef verify_api_key_env_var(env_var_name):\n    # Dependency 1: Attempt to retrieve the API key value from environment variables\n    api_key = _get_api_key_from_env(env_var_name)\n    \n    # Dependency 2: Check if the key value was successfully retrieved (i.e., is present)\n    is_present = _check_if_present(api_key)\n    \n    return is_present", "summary": "Verifies the presence of a required API key by attempting to retrieve its value from environment variables using **`_get_api_key_from_env`** (defined in **`verify_api_key_env_var.py`**) (which relies on the **`os`** library) and then checking if the retrieved value is not null using **`_check_if_present`** (defined in **`verify_api_key_env_var.py`**)."}
{"code": "def _convert_to_json_string(data_dict):\n    import json\n    return json.dumps(data_dict, indent=4)\n\ndef _write_to_temp_file(content):\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', delete=False) as tmp_file:\n        tmp_file.write(content)\n        return tmp_file.name\n\ndef serialize_and_write_to_temp_json(input_data):\n    # Dependency 1: Serialize the input dictionary to a formatted JSON string\n    json_string = _convert_to_json_string(input_data)\n    \n    # Dependency 2: Write the JSON string content to a named temporary file\n    temp_file_path = _write_to_temp_file(json_string)\n    \n    return temp_file_path", "summary": "Serializes an **`input_data`** dictionary into a formatted JSON string using **`_convert_to_json_string`** (defined in **`serialize_and_write_to_temp_json.py`**) (which relies on the **`json`** library) and then writes this JSON string to a named temporary file using **`_write_to_temp_file`** (defined in **`serialize_and_write_to_temp_json.py`**) (which relies on the **`tempfile`** library)."}
{"code": "def _filter_list_by_length(string_list, min_len):\n    return [s for s in string_list if len(s) >= min_len]\n\ndef _capitalize_list_elements(string_list):\n    return [s.capitalize() for s in string_list]\n\ndef process_text_list(texts, minimum_length):\n    # Dependency 1: Filter the list to only include strings longer than the minimum length\n    long_strings = _filter_list_by_length(texts, minimum_length)\n    \n    # Dependency 2: Capitalize the first letter of each remaining string\n    capitalized_list = _capitalize_list_elements(long_strings)\n    \n    return capitalized_list", "summary": "Processes a list of strings by first filtering the list to keep only strings longer than a **`minimum_length`** using **`_filter_list_by_length`** (defined in **`process_text_list.py`**) and then capitalizing the first letter of each remaining string using **`_capitalize_list_elements`** (defined in **`process_text_list.py`**)."}
{"code": "def _check_if_file_exists(file_path):\n    import os\n    return os.path.exists(file_path) and os.path.isfile(file_path)\n\ndef _get_file_size_bytes(file_path):\n    import os\n    return os.path.getsize(file_path)\n\ndef inspect_file_existence_and_size(target_path):\n    # Dependency 1: Check if the file actually exists on the filesystem\n    exists = _check_if_file_exists(target_path)\n    \n    # Dependency 2: Get the file size (only if it exists)\n    size = _get_file_size_bytes(target_path) if exists else 0\n    \n    return {'exists': exists, 'size_bytes': size}", "summary": "Inspects a file at **`target_path`** by first checking if the file exists using **`_check_if_file_exists`** (defined in **`inspect_file_existence_and_size.py`**) (which relies on the **`os`** library) and then retrieving its size in bytes using **`_get_file_size_bytes`** (defined in **`inspect_file_existence_and_size.py`**) (also relying on **`os`**), if it exists."}
{"code": "def _compute_geometric_progression(a, r, n):\n    return [a * (r ** i) for i in range(n)]\n\ndef _get_sum_of_last_n(series, n_last):\n    return sum(series[-n_last:])\n\ndef analyze_geometric_series(start_term, ratio, count, last_n):\n    # Dependency 1: Compute the terms of the geometric progression\n    series = _compute_geometric_progression(start_term, ratio, count)\n    \n    # Dependency 2: Calculate the sum of the last N terms in the series\n    last_sum = _get_sum_of_last_n(series, last_n)\n    \n    return {'series': series, 'sum_last_n': last_sum}", "summary": "Analyzes a geometric series by first computing the terms of the progression using **`_compute_geometric_progression`** (defined in **`analyze_geometric_series.py`**) given the starting term, ratio, and total count, and then calculating the sum of the last $N$ terms using **`_get_sum_of_last_n`** (defined in **`analyze_geometric_series.py`**)."}
{"code": "def _validate_data_schema(data, required_fields):\n    return all(field in data for field in required_fields)\n\ndef _transform_data_to_list(data_dict, fields):\n    return [data_dict.get(f) for f in fields]\n\ndef process_and_transform_data(record, fields_to_transform):\n    # Dependency 1: Validate that the record contains all required fields\n    if not _validate_data_schema(record, fields_to_transform):\n        raise ValueError(\"Missing required fields.\")\n        \n    # Dependency 2: Transform the dictionary data into a simple list based on field order\n    transformed_list = _transform_data_to_list(record, fields_to_transform)\n    \n    return transformed_list", "summary": "Processes a dictionary **`record`** by first validating that it contains all **`fields_to_transform`** using **`_validate_data_schema`** (defined in **`process_and_transform_data.py`**) and then transforming the data into an ordered list of values based on the order of the fields using **`_transform_data_to_list`** (defined in **`process_and_transform_data.py`**)."}
{"code": "def _get_utc_datetime_object():\n    import datetime\n    return datetime.datetime.utcnow()\n\ndef _format_datetime_rfc3339(dt_obj):\n    return dt_obj.isoformat(\"T\") + \"Z\"\n\ndef get_current_time_rfc3339():\n    import datetime\n    # Dependency 1: Get the current UTC datetime object\n    utc_dt = _get_utc_datetime_object()\n    \n    # Dependency 2: Format the datetime object into the RFC3339 string standard\n    rfc3339_string = _format_datetime_rfc3339(utc_dt)\n    \n    return rfc3339_string", "summary": "Retrieves the current time in UTC and formats it into the RFC3339 string standard. It uses **`_get_utc_datetime_object`** (defined in **`get_current_time_rfc3339.py`**) (relying on the **`datetime`** library) to get the time and **`_format_datetime_rfc3339`** (defined in **`get_current_time_rfc3339.py`**) for the specific formatting."}
{"code": "def _check_if_list_is_sorted(input_list):\n    return all(input_list[i] <= input_list[i+1] for i in range(len(input_list) - 1))\n\ndef _find_max_element(input_list):\n    return max(input_list) if input_list else None\n\ndef analyze_list_order(data_list):\n    # Dependency 1: Check if the list is sorted in ascending order\n    is_sorted = _check_if_list_is_sorted(data_list)\n    \n    # Dependency 2: Find the maximum element in the list\n    max_value = _find_max_element(data_list)\n    \n    return {'is_sorted': is_sorted, 'max_value': max_value}", "summary": "Analyzes a **`data_list`** by checking if it is sorted in ascending order using **`_check_if_list_is_sorted`** (defined in **`analyze_list_order.py`**) and then finding the maximum element in the list using **`_find_max_element`** (defined in **`analyze_list_order.py`**)."}
{"code": "def _get_data_type_name(value):\n    return type(value).__name__\n\ndef _construct_metadata_tag(key, type_name):\n    return f\"METADATA: key='{key}', type='{type_name}'\"\n\ndef create_metadata_for_variable(variable_name, variable_value):\n    # Dependency 1: Get the string name of the variable's data type\n    type_name = _get_data_type_name(variable_value)\n    \n    # Dependency 2: Construct a formatted metadata tag using the name and type\n    metadata_tag = _construct_metadata_tag(variable_name, type_name)\n    \n    return metadata_tag", "summary": "Creates a formatted metadata tag for a variable by first retrieving the string name of the variable's data type using **`_get_data_type_name`** (defined in **`create_metadata_for_variable.py`**) and then constructing a tag string that includes the **`variable_name`** and the type using **`_construct_metadata_tag`** (defined in **`create_metadata_for_variable.py`**)."}
{"code": "def _calculate_harmonic_mean(numbers):\n    import statistics\n    return statistics.harmonic_mean(numbers) if all(n > 0 for n in numbers) else 0\n\ndef _compare_mean_ratio(h_mean, a_mean):\n    if a_mean == 0: return 0\n    return h_mean / a_mean\n\ndef analyze_mean_ratio(data):\n    import statistics\n    # Dependency 1: Calculate the harmonic mean\n    h_mean = _calculate_harmonic_mean(data)\n    \n    # Calculate Arithmetic Mean (internal to this function for the ratio)\n    a_mean = statistics.mean(data) if data else 0\n    \n    # Dependency 2: Compare the ratio of Harmonic Mean to Arithmetic Mean\n    ratio = _compare_mean_ratio(h_mean, a_mean)\n    \n    return {'harmonic_mean': h_mean, 'arithmetic_mean': a_mean, 'mean_ratio': ratio}", "summary": "Analyzes the ratio of the Harmonic Mean to the Arithmetic Mean for a dataset. It calculates the Harmonic Mean using **`_calculate_harmonic_mean`** (defined in **`analyze_mean_ratio.py`**) (which relies on the **`statistics`** library) and then calculates the ratio against the Arithmetic Mean using **`_compare_mean_ratio`** (defined in **`analyze_mean_ratio.py`**)."}
{"code": "def _fetch_file_checksum(file_path, algorithm='md5'):\n    import hashlib\n    hash_func = hashlib.md5()\n    with open(file_path, 'rb') as f:\n        while chunk := f.read(4096):\n            hash_func.update(chunk)\n    return hash_func.hexdigest()\n\ndef _check_checksum_match(local_checksum, remote_checksum):\n    return local_checksum == remote_checksum\n\ndef verify_file_download(local_file_path, expected_md5):\n    # Dependency 1: Calculate the MD5 checksum of the local file\n    calculated_checksum = _fetch_file_checksum(local_file_path)\n    \n    # Dependency 2: Compare the calculated checksum with the expected remote checksum\n    is_verified = _check_checksum_match(calculated_checksum, expected_md5)\n    \n    return is_verified", "summary": "Verifies the integrity of a downloaded file by calculating its MD5 checksum using **`_fetch_file_checksum`** (defined in **`verify_file_download.py`**) (which relies on the **`hashlib`** library) and then comparing the result against an **`expected_md5`** hash using **`_check_checksum_match`** (defined in **`verify_file_download.py`**)."}
{"code": "def _read_credentials_from_file(file_path):\n    # Mock secure file read\n    return {'user': 'service_account', 'pass': 'top_secret'}\n\ndef _authorize_with_api(user, password):\n    # Mock API authorization call\n    if user == 'service_account': return 'TOKEN_123'\n    return None\n\ndef get_api_auth_token(credential_file):\n    # Dependency 1: Read the service credentials from a mock file\n    credentials = _read_credentials_from_file(credential_file)\n    \n    # Dependency 2: Use the credentials to authorize and get an API token\n    auth_token = _authorize_with_api(credentials['user'], credentials['pass'])\n    \n    return auth_token", "summary": "Retrieves an API authorization token by first reading mock service credentials (username and password) from a **`credential_file`** using **`_read_credentials_from_file`** (defined in **`get_api_auth_token.py`**) and then using those credentials to perform a mock authorization via **`_authorize_with_api`** (defined in **`get_api_auth_token.py`**)."}
{"code": "def _convert_to_set(data_list):\n    return set(data_list)\n\ndef _check_if_superset(set_a, set_b):\n    return set_a.issuperset(set_b)\n\ndef analyze_set_relationship(list_a, list_b):\n    # Dependency 1 & 2: Convert both lists to sets\n    set_a = _convert_to_set(list_a)\n    set_b = _convert_to_set(list_b)\n    \n    # Dependency 3: Check if set A is a superset of set B\n    is_superset = _check_if_superset(set_a, set_b)\n    \n    return is_superset", "summary": "Analyzes the relationship between two lists by converting them both to sets using **`_convert_to_set`** (defined in **`analyze_set_relationship.py`**) and then checking if the first set (`set_a`) is a superset of the second set (`set_b`) using **`_check_if_superset`** (defined in **`analyze_set_relationship.py`**)."}
{"code": "def _get_system_uptime_seconds():\n    import time\n    # Mock implementation\n    return time.monotonic() \n\ndef _format_seconds_to_dhms(seconds):\n    days = seconds // (3600 * 24)\n    hours = (seconds % (3600 * 24)) // 3600\n    minutes = (seconds % 3600) // 60\n    return f\"{int(days)}d {int(hours)}h {int(minutes)}m\"\n\ndef get_formatted_system_uptime():\n    import time\n    # Dependency 1: Get the system's uptime in seconds\n    uptime_seconds = _get_system_uptime_seconds()\n    \n    # Dependency 2: Format the seconds into Days:Hours:Minutes string\n    formatted_uptime = _format_seconds_to_dhms(uptime_seconds)\n    \n    return formatted_uptime", "summary": "Retrieves the system uptime in seconds using **`_get_system_uptime_seconds`** (defined in **`get_formatted_system_uptime.py`**) (which relies on the **`time`** library) and then formats this duration into a human-readable Days:Hours:Minutes string using **`_format_seconds_to_dhms`** (defined in **`get_formatted_system_uptime.py`**)."}
{"code": "def _compute_l1_norm(vector):\n    # Manhattan distance (L1)\n    return sum(abs(x) for x in vector)\n\ndef _check_if_sparse(vector, sparsity_threshold=0.5):\n    non_zero = sum(1 for x in vector if x != 0)\n    return (len(vector) - non_zero) / len(vector) > sparsity_threshold\n\ndef analyze_vector_sparsity(input_vector):\n    # Dependency 1: Compute the L1 norm of the vector\n    l1_norm = _compute_l1_norm(input_vector)\n    \n    # Dependency 2: Check if the vector is considered sparse (has many zeros)\n    is_sparse = _check_if_sparse(input_vector)\n    \n    return {'l1_norm': l1_norm, 'is_sparse': is_sparse}", "summary": "Analyzes an **`input_vector`** by calculating its L1 norm (Manhattan distance) using **`_compute_l1_norm`** (defined in **`analyze_vector_sparsity.py`**) and then checking if the vector is considered sparse based on a zero-count ratio using **`_check_if_sparse`** (defined in **`analyze_vector_sparsity.py`**)."}
{"code": "def _get_api_response_status(response_obj):\n    # Mock HTTP response\n    return response_obj.get('status_code', 200)\n\ndef _is_status_success(status_code):\n    return 200 <= status_code < 300\n\ndef check_http_response_success(mock_response):\n    # Dependency 1: Extract the HTTP status code from the mock response\n    status_code = _get_api_response_status(mock_response)\n    \n    # Dependency 2: Check if the status code indicates success (2xx range)\n    is_success = _is_status_success(status_code)\n    \n    return {'status_code': status_code, 'is_success': is_success}", "summary": "Checks the success of a mock HTTP response by extracting the status code using **`_get_api_response_status`** (defined in **`check_http_response_success.py`**) and then determining if the code falls within the 200-299 success range using **`_is_status_success`** (defined in **`check_http_response_success.py`**)."}
{"code": "def _find_longest_string(text_list):\n    if not text_list: return None\n    return max(text_list, key=len)\n\ndef _check_if_contains_vowels(text):\n    return any(c.lower() in 'aeiou' for c in text)\n\ndef analyze_longest_string_vowels(string_data):\n    # Dependency 1: Find the longest string in the list\n    longest_str = _find_longest_string(string_data)\n    \n    # Dependency 2: Check if the longest string contains any vowels\n    has_vowels = _check_if_contains_vowels(longest_str or '')\n    \n    return {'longest_string': longest_str, 'has_vowels': has_vowels}", "summary": "Analyzes a list of strings by first finding the longest string in the list using **`_find_longest_string`** (defined in **`analyze_longest_string_vowels.py`**) and then checking if that longest string contains any vowels using **`_check_if_contains_vowels`** (defined in **`analyze_longest_string_vowels.py`**)."}
{"code": "def _get_current_system_locale():\n    import locale\n    return locale.getdefaultlocale()[0]\n\ndef _check_if_european_locale(locale_code):\n    return locale_code and locale_code.split('_')[1] in ['FR', 'DE', 'IT', 'ES', 'UK']\n\ndef identify_system_locale_region():\n    import locale\n    # Dependency 1: Get the default system locale code\n    locale_code = _get_current_system_locale()\n    \n    # Dependency 2: Check if the locale code indicates a major European region\n    is_european = _check_if_european_locale(locale_code)\n    \n    return {'locale_code': locale_code, 'is_european': is_european}", "summary": "Identifies the system's default locale code using **`_get_current_system_locale`** (defined in **`identify_system_locale_region.py`**) (which relies on the **`locale`** library) and then checks if the locale code corresponds to a major European region using **`_check_if_european_locale`** (defined in **`identify_system_locale_region.py`**)."}
{"code": "def _get_list_chunks(data_list, chunk_size):\n    return [data_list[i:i + chunk_size] for i in range(0, len(data_list), chunk_size)]\n\ndef _reverse_each_chunk(chunk_list):\n    return [chunk[::-1] for chunk in chunk_list]\n\ndef chunk_and_reverse_list(full_list, size):\n    # Dependency 1: Split the full list into non-overlapping chunks\n    chunks = _get_list_chunks(full_list, size)\n    \n    # Dependency 2: Reverse the elements within each individual chunk\n    reversed_chunks = _reverse_each_chunk(chunks)\n    \n    return reversed_chunks", "summary": "Chunks a **`full_list`** into smaller lists of a specified **`size`** using **`_get_list_chunks`** (defined in **`chunk_and_reverse_list.py`**) and then reverses the order of elements within each of those individual chunks using **`_reverse_each_chunk`** (defined in **`chunk_and_reverse_list.py`**)."}
{"code": "def _check_database_connection(db_credentials):\n    # Mock connection check\n    return db_credentials.get('host') == 'prod.db.corp'\n\ndef _log_connection_status(is_connected):\n    status = 'SUCCESS' if is_connected else 'FAILURE'\n    print(f\"[DB_CHECK] Connection status: {status}\")\n\ndef verify_database_health(connection_params):\n    # Dependency 1: Check the mock database connection status\n    is_healthy = _check_database_connection(connection_params)\n    \n    # Dependency 2: Log the result of the connection check\n    _log_connection_status(is_healthy)\n    \n    return is_healthy", "summary": "Verifies the database health by checking the mock connection status using **`_check_database_connection`** (defined in **`verify_database_health.py`**) based on provided credentials, and then logs the connection status (SUCCESS/FAILURE) using **`_log_connection_status`** (defined in **`verify_database_health.py`**)."}
{"code": "def _get_system_platform_name():\n    import platform\n    return platform.system()\n\ndef _is_windows_platform(system_name):\n    return system_name == 'Windows'\n\ndef check_current_operating_system():\n    # Dependency 1: Get the current operating system name\n    os_name = _get_system_platform_name()\n    \n    # Dependency 2: Check specifically if the platform is Windows\n    is_windows = _is_windows_platform(os_name)\n    \n    return {'os_name': os_name, 'is_windows': is_windows}", "summary": "Checks the current operating system by retrieving the system platform name using **`_get_system_platform_name`** (defined in **`check_current_operating_system.py`**) (which relies on the **`platform`** library) and then determines if it is 'Windows' using **`_is_windows_platform`** (defined in **`check_current_operating_system.py`**)."}
{"code": "def _send_slack_notification(channel, message):\n    # Mock Slack API integration\n    print(f\"[SLACK] Notified {channel}: {message}\")\n\ndef _format_alert_message(metric, value, threshold):\n    return f\"ALERT: {metric} reached {value}. Threshold is {threshold}.\"\n\ndef send_metric_alert(alert_metric, current_value, alert_threshold, slack_channel):\n    # Dependency 1: Format the detailed alert message\n    message = _format_alert_message(alert_metric, current_value, alert_threshold)\n    \n    # Dependency 2: Send the formatted message to the target Slack channel\n    _send_slack_notification(slack_channel, message)\n    \n    return message", "summary": "Sends a metric alert by first formatting a detailed message (including metric, value, and threshold) using **`_format_alert_message`** (defined in **`send_metric_alert.py`**) and then sending that formatted message to a mock Slack channel using **`_send_slack_notification`** (defined in **`send_metric_alert.py`**)."}
{"code": "def _get_list_intersection(list_a, list_b):\n    return list(set(list_a) & set(list_b))\n\ndef _count_common_elements(intersection_list):\n    return len(intersection_list)\n\ndef analyze_list_overlap(data_1, data_2):\n    # Dependency 1: Find the intersection (common elements) between the two lists\n    common_elements = _get_list_intersection(data_1, data_2)\n    \n    # Dependency 2: Count how many common elements were found\n    overlap_count = _count_common_elements(common_elements)\n    \n    return {'common_elements': common_elements, 'overlap_count': overlap_count}", "summary": "Analyzes the overlap between **`data_1`** and **`data_2`** by first finding the intersection of their elements using **`_get_list_intersection`** (defined in **`analyze_list_overlap.py`**) and then counting the number of common elements found using **`_count_common_elements`** (defined in **`analyze_list_overlap.py`**)."}
{"code": "def _read_data_from_pickle(file_path):\n    import pickle\n    with open(file_path, 'rb') as f:\n        return pickle.load(f)\n\ndef _get_dict_value_by_path(data_dict, path):\n    keys = path.split('.')\n    value = data_dict\n    for key in keys:\n        value = value.get(key, None)\n        if value is None: break\n    return value\n\ndef load_pickle_and_extract_value(pickle_file, data_path):\n    # Dependency 1: Load the data structure from the pickle file\n    data_loaded = _read_data_from_pickle(pickle_file)\n    \n    # Dependency 2: Extract a specific value using a dot-separated path\n    extracted_value = _get_dict_value_by_path(data_loaded, data_path)\n    \n    return extracted_value", "summary": "Loads a data structure from a **`pickle_file`** using **`_read_data_from_pickle`** (defined in **`load_pickle_and_extract_value.py`**) (which relies on the **`pickle`** library) and then extracts a specific value from the loaded dictionary using a dot-separated path via **`_get_dict_value_by_path`** (defined in **`load_pickle_and_extract_value.py`**)."}
{"code": "def _convert_to_base64(raw_bytes):\n    import base64\n    return base64.b64encode(raw_bytes).decode('utf-8')\n\ndef _prefix_with_mime_type(b64_string, mime='image/jpeg'):\n    return f'data:{mime};base64,{b64_string}'\n\ndef create_data_uri(raw_data, data_mime_type):\n    # Dependency 1: Convert the raw byte data to a base64 encoded string\n    b64_content = _convert_to_base64(raw_data)\n    \n    # Dependency 2: Prepend the MIME type to form a data URI\n    data_uri = _prefix_with_mime_type(b64_content, data_mime_type)\n    \n    return data_uri", "summary": "Creates a data URI by first converting raw bytes (**`raw_data`**) into a base64 encoded string using **`_convert_to_base64`** (defined in **`create_data_uri.py`**) (which relies on the **`base64`** library) and then prepending the specified MIME type to form the complete URI using **`_prefix_with_mime_type`** (defined in **`create_data_uri.py`**)."}
{"code": "def _get_number_sign(n):\n    if n > 0: return 1\n    if n < 0: return -1\n    return 0\n\ndef _check_if_absolute_value_is_large(n, threshold=1000):\n    return abs(n) >= threshold\n\ndef analyze_number_properties(input_num):\n    # Dependency 1: Determine the sign of the number (+1, -1, or 0)\n    sign = _get_number_sign(input_num)\n    \n    # Dependency 2: Check if the absolute value of the number is large\n    is_large = _check_if_absolute_value_is_large(input_num)\n    \n    return {'sign': sign, 'is_large_abs': is_large}", "summary": "Analyzes an **`input_num`** by first determining its sign (+1, -1, or 0) using **`_get_number_sign`** (defined in **`analyze_number_properties.py`**) and then checking if its absolute value is large (greater than or equal to 1000) using **`_check_if_absolute_value_is_large`** (defined in **`analyze_number_properties.py`**)."}
{"code": "def _check_if_socket_is_open(host, port):\n    import socket\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.settimeout(1)\n        s.connect((host, port))\n        return True\n    except (socket.error, socket.timeout): return False\n    finally: s.close()\n\ndef _log_port_status(host, port, is_open):\n    status = 'OPEN' if is_open else 'CLOSED'\n    print(f\"[PORT_SCAN] {host}:{port} is {status}\")\n\ndef scan_tcp_port(target_host, target_port):\n    # Dependency 1: Check if the TCP port is open on the target host\n    port_open = _check_if_socket_is_open(target_host, target_port)\n    \n    # Dependency 2: Log the result of the port check\n    _log_port_status(target_host, target_port, port_open)\n    \n    return port_open", "summary": "Scans a TCP port by checking if the port is open on the **`target_host`** using **`_check_if_socket_is_open`** (defined in **`scan_tcp_port.py`**) (which relies on the **`socket`** library) and then logs the outcome (OPEN/CLOSED) using **`_log_port_status`** (defined in **`scan_tcp_port.py`**)."}
{"code": "def _compute_jaccard_index(set1, set2):\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    return intersection / union if union > 0 else 0\n\ndef _classify_similarity(jaccard_score, threshold=0.5):\n    return 'HIGH' if jaccard_score >= threshold else 'LOW'\n\ndef analyze_set_similarity(list_a, list_b):\n    # Convert lists to sets\n    set_a, set_b = set(list_a), set(list_b)\n    \n    # Dependency 1: Compute the Jaccard index (set similarity score)\n    j_score = _compute_jaccard_index(set_a, set_b)\n    \n    # Dependency 2: Classify the similarity as HIGH or LOW based on a threshold\n    similarity = _classify_similarity(j_score)\n    \n    return {'score': j_score, 'similarity': similarity}", "summary": "Analyzes the similarity between two lists (**`list_a`**, **`list_b`**) by first calculating the Jaccard index (ratio of intersection to union) of their corresponding sets using **`_compute_jaccard_index`** (defined in **`analyze_set_similarity.py`**) and then classifying the similarity score as HIGH or LOW using **`_classify_similarity`** (defined in **`analyze_set_similarity.py`**)."}
{"code": "def _get_python_version_info():\n    import sys\n    return sys.version_info\n\ndef _check_min_major_version(version_info, required_major=3):\n    return version_info.major >= required_major\n\ndef verify_python_major_version():\n    # Dependency 1: Get the running Python version information\n    version = _get_python_version_info()\n    \n    # Dependency 2: Check if the major version meets the minimum requirement (e.g., 3)\n    is_compatible = _check_min_major_version(version)\n    \n    return {'version': f'{version.major}.{version.minor}', 'compatible': is_compatible}", "summary": "Verifies the current Python environment by retrieving the version information tuple using **`_get_python_version_info`** (defined in **`verify_python_major_version.py`**) (which relies on the **`sys`** library) and then checking if the major version meets a minimum requirement (default 3) using **`_check_min_major_version`** (defined in **`verify_python_major_version.py`**)."}
{"code": "def _get_file_modification_time(file_path):\n    import os\n    import datetime\n    timestamp = os.path.getmtime(file_path)\n    return datetime.datetime.fromtimestamp(timestamp)\n\ndef _check_if_recently_modified(mod_time, days_limit=7):\n    import datetime\n    return (datetime.datetime.now() - mod_time).days < days_limit\n\ndef audit_file_recent_modification(target_file):\n    # Dependency 1: Get the last modification datetime object for the file\n    modification_dt = _get_file_modification_time(target_file)\n    \n    # Dependency 2: Check if the file was modified within the last 7 days\n    is_recent = _check_if_recently_modified(modification_dt)\n    \n    return {'mod_time': modification_dt, 'is_recent': is_recent}", "summary": "Audits a file by first retrieving its last modification time as a datetime object using **`_get_file_modification_time`** (defined in **`audit_file_recent_modification.py`**) (which relies on the **`os`** and **`datetime`** libraries) and then checking if this time is within the last 7 days using **`_check_if_recently_modified`** (defined in **`audit_file_recent_modification.py`**)."}
{"code": "def _calculate_cosine_similarity(v1, v2):\n    import numpy\n    v1 = numpy.array(v1)\n    v2 = numpy.array(v2)\n    dot_product = numpy.dot(v1, v2)\n    norm_v1 = numpy.linalg.norm(v1)\n    norm_v2 = numpy.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0: return 0.0\n    return dot_product / (norm_v1 * norm_v2)\n\ndef _is_vector_relationship_parallel(similarity_score, threshold=0.95):\n    return abs(similarity_score) >= threshold\n\ndef analyze_vector_orientation(vector_a, vector_b):\n    # Dependency 1: Calculate the cosine similarity between the two vectors\n    cosine_sim = _calculate_cosine_similarity(vector_a, vector_b)\n    \n    # Dependency 2: Check if the vectors are nearly parallel (similarity near 1 or -1)\n    is_parallel = _is_vector_relationship_parallel(cosine_sim)\n    \n    return {'similarity': cosine_sim, 'is_parallel': is_parallel}", "summary": "Analyzes the orientation of two vectors by calculating the cosine similarity between them using **`_calculate_cosine_similarity`** (defined in **`analyze_vector_orientation.py`**) (which relies on the external **`numpy`** library for dot product and norms) and then checking if the similarity score indicates a nearly parallel relationship using **`_is_vector_relationship_parallel`** (defined in **`analyze_vector_orientation.py`**)."}
{"code": "def _read_data_from_yml(file_path):\n    import yaml\n    with open(file_path, 'r') as f:\n        return yaml.safe_load(f)\n\ndef _count_top_level_keys(data_dict):\n    return len(data_dict.keys())\n\ndef process_yaml_and_count_keys(yaml_file):\n    # Dependency 1: Read and parse the YAML file content\n    config_data = _read_data_from_yml(yaml_file)\n    \n    # Dependency 2: Count the number of keys at the top level of the configuration\n    key_count = _count_top_level_keys(config_data)\n    \n    return key_count", "summary": "Processes a YAML file by first reading and safely loading its content into a dictionary using **`_read_data_from_yml`** (defined in **`process_yaml_and_count_keys.py`**) (which relies on the external **`yaml`** library) and then counting the number of top-level keys in the loaded data using **`_count_top_level_keys`** (defined in **`process_yaml_and_count_keys.py`**)."}
{"code": "def _check_if_data_is_empty(data_container):\n    if isinstance(data_container, (list, dict, set, str)): return not bool(data_container)\n    return data_container is None\n\ndef _get_default_if_empty(data_container, default_value='EMPTY'):\n    return default_value if _check_if_data_is_empty(data_container) else data_container\n\ndef handle_empty_data(input_data, fallback):\n    # Dependency 1: Check if the input data structure is empty/None\n    # Note: _get_default_if_empty implicitly calls _check_if_data_is_empty\n    \n    # Dependency 2: Return a fallback value if the input is empty, otherwise return the input\n    result = _get_default_if_empty(input_data, fallback)\n    \n    return result", "summary": "Handles potentially empty data by checking if the **`input_data`** is empty or `None` using **`_check_if_data_is_empty`** (defined in **`handle_empty_data.py`**) and then returning a specified **`fallback`** value if it is empty, or the original data otherwise, using **`_get_default_if_empty`** (defined in **`handle_empty_data.py`**)."}
{"code": "def _get_process_creation_time():\n    import psutil\n    process = psutil.Process()\n    return process.create_time() # Epoch time\n\ndef _convert_epoch_to_readable_string(epoch_time):\n    import datetime\n    return datetime.datetime.fromtimestamp(epoch_time).strftime('%Y-%m-%d %H:%M:%S')\n\ndef get_process_start_time_formatted():\n    # Dependency 1: Get the process creation time as an epoch timestamp\n    create_ts = _get_process_creation_time()\n    \n    # Dependency 2: Convert the epoch timestamp to a readable datetime string\n    readable_time = _convert_epoch_to_readable_string(create_ts)\n    \n    return readable_time", "summary": "Retrieves the start time of the current process by getting the creation epoch time using **`_get_process_creation_time`** (defined in **`get_process_start_time_formatted.py`**) (which relies on the external **`psutil`** library) and then converting that epoch time into a readable datetime string using **`_convert_epoch_to_readable_string`** (defined in **`get_process_start_time_formatted.py`**) (which relies on the **`datetime`** library)."}
{"code": "def _check_if_port_is_standard_http(port_number):\n    return port_number in [80, 443]\n\ndef _format_url_with_port(host, port, is_secure=False):\n    if is_secure: prefix = 'https://'\n    else: prefix = 'http://'\n    \n    # Omit port for standard ports\n    if _check_if_port_is_standard_http(port): return f'{prefix}{host}'\n    \n    return f'{prefix}{host}:{port}'\n\ndef build_http_url(target_host, target_port, use_ssl):\n    # Dependency 1: Check if the port is a standard HTTP/HTTPS port\n    # Note: _format_url_with_port implicitly calls _check_if_port_is_standard_http\n    \n    # Dependency 2: Construct the full URL string, omitting standard ports\n    full_url = _format_url_with_port(target_host, target_port, use_ssl)\n    \n    return full_url", "summary": "Builds a full HTTP/HTTPS URL by checking if the **`target_port`** is standard (80 or 443) using **`_check_if_port_is_standard_http`** (defined in **`build_http_url.py`**) and then constructing the final URL using **`_format_url_with_port`** (defined in **`build_http_url.py`**), which omits the port number if it is standard."}
{"code": "def _compute_geometric_mean(numbers):\n    import statistics\n    import math\n    # Check for non-positive numbers as required for Geometric Mean\n    if not all(n > 0 for n in numbers): return 0\n    return statistics.geometric_mean(numbers)\n\ndef _log_mean_value(mean_type, value):\n    print(f\"[MEAN_CALC] {mean_type} Mean: {value:.4f}\")\n\ndef calculate_and_log_geometric_mean(data_points):\n    # Dependency 1: Compute the geometric mean of the positive data points\n    g_mean = _compute_geometric_mean(data_points)\n    \n    # Dependency 2: Log the calculated geometric mean\n    _log_mean_value('Geometric', g_mean)\n    \n    return g_mean", "summary": "Calculates the geometric mean of a list of **`data_points`** using **`_compute_geometric_mean`** (defined in **`calculate_and_log_geometric_mean.py`**) (which relies on the **`statistics`** and **`math`** libraries) and then logs the calculated value using **`_log_mean_value`** (defined in **`calculate_and_log_geometric_mean.py`**)."}
{"code": "def _read_data_from_db_query(db_connection, query):\n    # Mock execution of SQL query\n    return [{'col1': 1, 'col2': 'A'}, {'col1': 2, 'col2': 'B'}]\n\ndef _convert_records_to_set_of_ids(records, key='col1'):\n    return {r[key] for r in records}\n\ndef fetch_and_extract_ids(db_conn, sql_query):\n    # Dependency 1: Execute the mock database query and retrieve records\n    raw_records = _read_data_from_db_query(db_conn, sql_query)\n    \n    # Dependency 2: Extract the values from a specific key ('col1') into a set\n    id_set = _convert_records_to_set_of_ids(raw_records)\n    \n    return id_set", "summary": "Fetches data from a mock database by executing an **`sql_query`** using **`_read_data_from_db_query`** (defined in **`fetch_and_extract_ids.py`**) and then converts the resulting list of records into a set containing the values of the 'col1' key using **`_convert_records_to_set_of_ids`** (defined in **`fetch_and_extract_ids.py`**)."}
{"code": "def _check_if_ipv4_address(ip_string):\n    import ipaddress\n    try:\n        ipaddress.IPv4Address(ip_string)\n        return True\n    except ipaddress.AddressValueError: return False\n\ndef _get_ip_class(ip_string):\n    # Mock class checker (based on first octet)\n    if ip_string.startswith('192.168.'): return 'Private'\n    if ip_string.startswith('10.'): return 'Private'\n    return 'Public'\n\ndef analyze_ip_address_type(ip_candidate):\n    # Dependency 1: Validate if the input string is a valid IPv4 address\n    if not _check_if_ipv4_address(ip_candidate):\n        return {'is_valid': False, 'ip_class': 'N/A'}\n        \n    # Dependency 2: Determine if the valid IP is public or a specific private range\n    ip_class = _get_ip_class(ip_candidate)\n    \n    return {'is_valid': True, 'ip_class': ip_class}", "summary": "Analyzes an **`ip_candidate`** by first validating it as a formal IPv4 address using **`_check_if_ipv4_address`** (defined in **`analyze_ip_address_type.py`**) (which relies on the **`ipaddress`** library) and then classifying the valid IP as 'Private' or 'Public' based on its prefix using **`_get_ip_class`** (defined in **`analyze_ip_address_type.py`**)."}
{"code": "def _compute_rsa_key_fingerprint(public_key):\n    # Mock calculation (just returns the hash of the key string)\n    import hashlib\n    return hashlib.sha256(public_key.encode()).hexdigest()[:16]\n\ndef _check_fingerprint_revocation_list(fingerprint):\n    # Mock check against a list of revoked keys\n    return fingerprint == 'd41d8cd98f00b204'\n\ndef audit_public_key_revocation(key_data):\n    # Dependency 1: Compute a unique fingerprint for the public key\n    fingerprint = _compute_rsa_key_fingerprint(key_data)\n    \n    # Dependency 2: Check the computed fingerprint against a mock revocation list\n    is_revoked = _check_fingerprint_revocation_list(fingerprint)\n    \n    return {'fingerprint': fingerprint, 'is_revoked': is_revoked}", "summary": "Audits a **`key_data`** by first computing a short SHA256 fingerprint of the public key using **`_compute_rsa_key_fingerprint`** (defined in **`audit_public_key_revocation.py`**) (which relies on the **`hashlib`** library) and then checking this fingerprint against a mock revocation list using **`_check_fingerprint_revocation_list`** (defined in **`audit_public_key_revocation.py`**)."}
{"code": "def _compress_string_gzip(data_string):\n    import gzip\n    return gzip.compress(data_string.encode('utf-8'))\n\ndef _calculate_compression_ratio(original_length, compressed_bytes):\n    return len(compressed_bytes) / original_length if original_length > 0 else 0\n\ndef compress_and_analyze_text(input_text):\n    # Dependency 1: Compress the input string using GZIP\n    compressed_data = _compress_string_gzip(input_text)\n    \n    # Dependency 2: Calculate the compression ratio (compressed size / original size)\n    ratio = _calculate_compression_ratio(len(input_text), compressed_data)\n    \n    return {'compressed_size': len(compressed_data), 'ratio': ratio}", "summary": "Compresses an **`input_text`** string using GZIP compression via **`_compress_string_gzip`** (defined in **`compress_and_analyze_text.py`**) (which relies on the **`gzip`** library) and then calculates the compression ratio (compressed size / original size) using **`_calculate_compression_ratio`** (defined in **`compress_and_analyze_text.py`**)."}
{"code": "def _get_process_cpu_usage():\n    import psutil\n    return psutil.cpu_percent(interval=1)\n\ndef _throttle_if_high_cpu(cpu_percent, max_limit=80):\n    if cpu_percent > max_limit:\n        import time\n        time.sleep(1)\n        return True\n    return False\n\ndef monitor_and_throttle_cpu():\n    # Dependency 1: Get the current CPU usage percentage\n    cpu_use = _get_process_cpu_usage()\n    \n    # Dependency 2: Check if CPU is high and apply a mock throttle (sleep)\n    was_throttled = _throttle_if_high_cpu(cpu_use)\n    \n    return {'cpu_percent': cpu_use, 'throttled': was_throttled}", "summary": "Monitors the process CPU usage by getting the current percentage over a 1-second interval using **`_get_process_cpu_usage`** (defined in **`monitor_and_throttle_cpu.py`**) (which relies on the external **`psutil`** library) and then applying a mock 1-second throttle (sleep) if the usage exceeds 80% using **`_throttle_if_high_cpu`** (defined in **`monitor_and_throttle_cpu.py`**) (which relies on the **`time`** library)."}
{"code": "def _compute_matrix_transpose(matrix):\n    # Assumes rectangular matrix (list of lists)\n    return [list(row) for row in zip(*matrix)]\n\ndef _verify_if_symmetric(original_matrix, transposed_matrix):\n    return original_matrix == transposed_matrix\n\ndef check_matrix_symmetry(input_matrix):\n    # Dependency 1: Compute the transpose of the input matrix\n    transposed = _compute_matrix_transpose(input_matrix)\n    \n    # Dependency 2: Verify if the original matrix equals its transpose\n    is_symmetric = _verify_if_symmetric(input_matrix, transposed)\n    \n    return {'is_symmetric': is_symmetric}", "summary": "Checks if a matrix is symmetric by first computing its transpose using **`_compute_matrix_transpose`** (defined in **`check_matrix_symmetry.py`**) and then verifying if the original matrix is equal to the transposed matrix using **`_verify_if_symmetric`** (defined in **`check_matrix_symmetry.py`**)."}
{"code": "def _read_credentials_from_vault(key_id):\n    # Mock vault retrieval (securely)\n    return {'api_token': f'vault-token-{key_id}'}\n\ndef _create_auth_header(token):\n    return {'Authorization': f'Bearer {token}'}\n\ndef generate_api_auth_header(resource_key_id):\n    # Dependency 1: Securely retrieve the API token from the mock vault\n    credentials = _read_credentials_from_vault(resource_key_id)\n    \n    # Dependency 2: Format the token into a standard Authorization Bearer header\n    auth_header = _create_auth_header(credentials['api_token'])\n    \n    return auth_header", "summary": "Generates an API authorization header by first retrieving the API token from a mock secure vault using **`_read_credentials_from_vault`** (defined in **`generate_api_auth_header.py`**) and then formatting that token into a standard 'Authorization: Bearer' HTTP header using **`_create_auth_header`** (defined in **`generate_api_auth_header.py`**)."}
{"code": "def _get_system_user_list():\n    # Mock reading /etc/passwd\n    return ['root', 'daemon', 'user1', 'system_user']\n\ndef _filter_system_users(user_list, system_names=['root', 'daemon']):\n    return [u for u in user_list if u not in system_names]\n\ndef find_non_system_users():\n    # Dependency 1: Get the list of all mock system users\n    all_users = _get_system_user_list()\n    \n    # Dependency 2: Filter the list to exclude the default system accounts\n    app_users = _filter_system_users(all_users)\n    \n    return app_users", "summary": "Finds non-system users by first retrieving a mock list of all system users using **`_get_system_user_list`** (defined in **`find_non_system_users.py`**) and then filtering that list to exclude predefined system account names using **`_filter_system_users`** (defined in **`find_non_system_users.py`**)."}
{"code": "def _calculate_distance_from_origin(vector):\n    # Euclidean distance (L2 norm) from origin (0, 0, ...)\n    return sum(x**2 for x in vector)**0.5\n\ndef _get_normalized_vector_magnitude(magnitude, max_magnitude=100):\n    return min(1.0, magnitude / max_magnitude)\n\ndef analyze_vector_magnitude(data_vector):\n    # Dependency 1: Calculate the Euclidean distance (magnitude) of the vector from the origin\n    magnitude = _calculate_distance_from_origin(data_vector)\n    \n    # Dependency 2: Normalize the magnitude to a 0-1 range based on a max value\n    normalized_mag = _get_normalized_vector_magnitude(magnitude)\n    \n    return {'magnitude': magnitude, 'normalized': normalized_mag}", "summary": "Analyzes the magnitude of a vector by calculating its Euclidean distance (L2 norm) from the origin using **`_calculate_distance_from_origin`** (defined in **`analyze_vector_magnitude.py`**) and then normalizing this magnitude to a 0-1 range based on a maximum threshold using **`_get_normalized_vector_magnitude`** (defined in **`analyze_vector_magnitude.py`**)."}
{"code": "def _check_if_port_is_in_range(port, min_port=1024, max_port=65535):\n    return min_port <= port <= max_port\n\ndef _log_port_range_violation(port, is_valid):\n    if not is_valid: print(f\"[SECURITY] Port {port} violates expected non-privileged range.\")\n\ndef validate_application_port(target_port):\n    # Dependency 1: Check if the port number falls within the expected range (e.g., non-privileged)\n    is_valid = _check_if_port_is_in_range(target_port)\n    \n    # Dependency 2: Log a security warning if the port is outside the expected range\n    _log_port_range_violation(target_port, is_valid)\n    \n    return is_valid", "summary": "Validates an **`target_port`** number by checking if it falls within the expected non-privileged range (1024-65535) using **`_check_if_port_is_in_range`** (defined in **`validate_application_port.py`**) and then logging a security warning if the check fails using **`_log_port_range_violation`** (defined in **`validate_application_port.py`**)."}
{"code": "def _get_module_dependency_list(module_name):\n    # Mock function to inspect dependencies\n    if module_name == 'app_core': return ['db_connector', 'logger', 'http_client']\n    return []\n\ndef _format_list_to_string(item_list, delimiter=';'):\n    return delimiter.join(item_list)\n\ndef list_module_dependencies(module_id):\n    # Dependency 1: Get the list of dependencies for the module\n    dependencies = _get_module_dependency_list(module_id)\n    \n    # Dependency 2: Format the dependency list into a single delimited string\n    dependency_string = _format_list_to_string(dependencies)\n    \n    return dependency_string", "summary": "Lists a module's dependencies by first retrieving a mock list of dependency names using **`_get_module_dependency_list`** (defined in **`list_module_dependencies.py`**) and then formatting that list into a single string separated by a delimiter (default ';') using **`_format_list_to_string`** (defined in **`list_module_dependencies.py`**)."}
{"code": "def _normalize_string_case(text, case='lower'):\n    if case == 'lower': return text.lower()\n    if case == 'upper': return text.upper()\n    return text\n\ndef _remove_leading_trailing_spaces(text):\n    return text.strip()\n\ndef normalize_text_for_comparison(input_text):\n    # Dependency 1: Convert the input text to lowercase\n    lowercase_text = _normalize_string_case(input_text, case='lower')\n    \n    # Dependency 2: Remove any leading or trailing whitespace\n    cleaned_text = _remove_leading_trailing_spaces(lowercase_text)\n    \n    return cleaned_text", "summary": "Normalizes an **`input_text`** string for comparison purposes by first converting it to lowercase using **`_normalize_string_case`** (defined in **`normalize_text_for_comparison.py`**) and then removing any leading or trailing whitespace using **`_remove_leading_trailing_spaces`** (defined in **`normalize_text_for_comparison.py`**)."}
{"code": "def _load_data_from_sftp(remote_path):\n    # Mock SFTP connection and file retrieval\n    return f\"Content of {remote_path} from SFTP.\"\n\ndef _hash_data_content(content):\n    import hashlib\n    return hashlib.sha1(content.encode()).hexdigest()\n\ndef fetch_and_hash_remote_file(sftp_location):\n    # Dependency 1: Load the file content from the mock SFTP server\n    file_content = _load_data_from_sftp(sftp_location)\n    \n    # Dependency 2: Compute the SHA1 hash of the retrieved file content\n    content_hash = _hash_data_content(file_content)\n    \n    return content_hash", "summary": "Fetches a remote file by loading its content from a mock SFTP location using **`_load_data_from_sftp`** (defined in **`fetch_and_hash_remote_file.py`**) and then computes the SHA1 hash of the retrieved content using **`_hash_data_content`** (defined in **`fetch_and_hash_remote_file.py`**) (which relies on the **`hashlib`** library)."}
{"code": "def _compute_rsa_key_length(public_key):\n    # Mock calculation (just returns the length of the key string)\n    return len(public_key) * 8 # Simulate bits\n\ndef _check_min_key_size(key_size_bits, min_bits=2048):\n    return key_size_bits >= min_bits\n\ndef audit_encryption_key_strength(key_string):\n    # Dependency 1: Compute the mock length (in bits) of the key\n    key_length = _compute_rsa_key_length(key_string)\n    \n    # Dependency 2: Check if the key size meets the minimum required length\n    is_strong = _check_min_key_size(key_length)\n    \n    return {'key_length': key_length, 'is_strong': is_strong}", "summary": "Audits the strength of an encryption key by computing its mock length in bits using **`_compute_rsa_key_length`** (defined in **`audit_encryption_key_strength.py`**) and then checking if this length meets a minimum required size (default 2048 bits) using **`_check_min_key_size`** (defined in **`audit_encryption_key_strength.py`**)."}
{"code": "def _parse_cookie_header(header_string):\n    cookies = {}\n    for part in header_string.split(';'):\n        if '=' in part:\n            key, value = part.strip().split('=', 1)\n            cookies[key] = value\n    return cookies\n\ndef _get_session_id_from_cookies(cookie_dict, session_key='session_id'):\n    return cookie_dict.get(session_key)\n\ndef extract_session_id_from_cookie_header(raw_cookie_header):\n    # Dependency 1: Parse the raw cookie header string into a dictionary of key-value pairs\n    cookies = _parse_cookie_header(raw_cookie_header)\n    \n    # Dependency 2: Extract the specific 'session_id' value from the dictionary\n    session_id = _get_session_id_from_cookies(cookies)\n    \n    return session_id", "summary": "Extracts the session ID from a raw HTTP Cookie header string by first parsing the string into a dictionary of cookie key-value pairs using **`_parse_cookie_header`** (defined in **`extract_session_id_from_cookie_header.py`**) and then retrieving the specific 'session\\_id' value from that dictionary using **`_get_session_id_from_cookies`** (defined in **`extract_session_id_from_cookie_header.py`**)."}
{"code": "def _calculate_sample_kurtosis(data_list):\n    import scipy.stats\n    # Requires external library scipy\n    return scipy.stats.kurtosis(data_list, fisher=True)\n\ndef _is_heavy_tailed_distribution(kurtosis_value, threshold=0.0):\n    return kurtosis_value > threshold\n\ndef analyze_distribution_tail(data_sample):\n    import scipy.stats\n    # Dependency 1: Calculate the sample kurtosis (Fisher's definition)\n    kurtosis_val = _calculate_sample_kurtosis(data_sample)\n    \n    # Dependency 2: Check if the kurtosis indicates a heavy-tailed distribution (leptokurtic)\n    is_heavy_tailed = _is_heavy_tailed_distribution(kurtosis_val)\n    \n    return {'kurtosis': kurtosis_val, 'is_leptokurtic': is_heavy_tailed}", "summary": "Analyzes a data distribution's tail heaviness by calculating the sample kurtosis (Fisher's) using **`_calculate_sample_kurtosis`** (defined in **`analyze_distribution_tail.py`**) (which relies on the external **`scipy.stats`** library) and then checking if the kurtosis value indicates a heavy-tailed (leptokurtic) distribution using **`_is_heavy_tailed_distribution`** (defined in **`analyze_distribution_tail.py`**)."}
{"code": "def _get_database_backup_status():\n    # Mock external API call\n    return {'last_backup': '2025-11-20T10:00:00Z', 'successful': True}\n\ndef _check_if_backup_succeeded(status_dict):\n    return status_dict.get('successful', False)\n\ndef monitor_daily_backup():\n    # Dependency 1: Fetch the mock status of the latest database backup\n    backup_status = _get_database_backup_status()\n    \n    # Dependency 2: Check specifically if the backup was marked as successful\n    succeeded = _check_if_backup_succeeded(backup_status)\n    \n    return {'status': backup_status, 'succeeded': succeeded}", "summary": "Monitors the daily database backup by fetching the mock backup status details using **`_get_database_backup_status`** (defined in **`monitor_daily_backup.py`**) and then specifically checking if the 'successful' flag is set to true using **`_check_if_backup_succeeded`** (defined in **`monitor_daily_backup.py`**)."}
{"code": "def _check_string_for_consecutive_chars(text, n=3):\n    import re\n    return bool(re.search(r'(.)\\1{' + str(n-1) + '}', text))\n\ndef _log_security_warning(text, has_consecutive):\n    if has_consecutive: print(f\"[SECURITY] Possible weak password/key detected: {text}\")\n\ndef audit_string_consecutives(input_string):\n    # Dependency 1: Check the string for 3 or more consecutive identical characters\n    is_weak = _check_string_for_consecutive_chars(input_string, n=3)\n    \n    # Dependency 2: Log a security warning if consecutive characters are found\n    _log_security_warning(input_string, is_weak)\n    \n    return is_weak", "summary": "Audits an **`input_string`** for security by checking for the presence of three or more consecutive identical characters using **`_check_string_for_consecutive_chars`** (defined in **`audit_string_consecutives.py`**) (which relies on the **`re`** library) and then logging a security warning if a match is found using **`_log_security_warning`** (defined in **`audit_string_consecutives.py`**)."}
{"code": "def _get_api_response_body(response_obj):\n    # Mock HTTP response\n    return response_obj.get('body', '{}')\n\ndef _parse_json_string(json_string):\n    import json\n    return json.loads(json_string)\n\ndef parse_api_response_body(mock_http_response):\n    # Dependency 1: Extract the raw JSON body string from the mock response\n    raw_json = _get_api_response_body(mock_http_response)\n    \n    # Dependency 2: Parse the JSON string into a Python dictionary\n    parsed_data = _parse_json_string(raw_json)\n    \n    return parsed_data", "summary": "Parses the body of a mock HTTP response by first extracting the raw JSON string using **`_get_api_response_body`** (defined in **`parse_api_response_body.py`**) and then converting that JSON string into a Python dictionary using **`_parse_json_string`** (defined in **`parse_api_response_body.py`**) (which relies on the **`json`** library)."}
{"code": "def _fetch_file_content_lines(file_path):\n    # Mock file read\n    with open(file_path, 'r') as f:\n        return f.readlines()\n\ndef _filter_out_comments(lines, comment_char='#'):\n    return [line.strip() for line in lines if not line.strip().startswith(comment_char)]\n\ndef load_and_clean_config_file(config_path):\n    # Dependency 1: Fetch all lines from the configuration file\n    raw_lines = _fetch_file_content_lines(config_path)\n    \n    # Dependency 2: Filter out lines that start with the comment character\n    active_config = _filter_out_comments(raw_lines)\n    \n    return active_config", "summary": "Loads a configuration file by fetching all content lines using **`_fetch_file_content_lines`** (defined in **`load_and_clean_config_file.py`**) and then cleaning the data by filtering out lines that begin with the comment character (default '#') using **`_filter_out_comments`** (defined in **`load_and_clean_config_file.py`**)."}
{"code": "def _compute_rsa_signature(data, private_key):\n    # Mock signing process\n    return f\"SIGNATURE({private_key[-4:]}:{data[:10]}...)\"\n\ndef _send_data_to_queue(signed_data):\n    # Mock queue API call\n    print(f\"[QUEUE] Sending signed data: {signed_data}\")\n\ndef sign_and_queue_message(message_content, signing_key):\n    # Dependency 1: Compute the cryptographic signature for the message content\n    signed_message = _compute_rsa_signature(message_content, signing_key)\n    \n    # Dependency 2: Send the signed message to a mock message queue\n    _send_data_to_queue(signed_message)\n    \n    return signed_message", "summary": "Signs a **`message_content`** using a mock RSA signature process via **`_compute_rsa_signature`** (defined in **`sign_and_queue_message.py`**) and then sends the resulting signed message to a mock message queue using **`_send_data_to_queue`** (defined in **`sign_and_queue_message.py`**)."}
{"code": "def _convert_to_epoch_ms(dt_object):\n    return int(dt_object.timestamp() * 1000)\n\ndef _get_current_utc_time():\n    import datetime\n    return datetime.datetime.utcnow()\n\ndef get_current_time_epoch_ms():\n    # Dependency 1: Get the current time as a UTC datetime object\n    utc_dt = _get_current_utc_time()\n    \n    # Dependency 2: Convert the datetime object into milliseconds since the epoch\n    epoch_ms = _convert_to_epoch_ms(utc_dt)\n    \n    return epoch_ms", "summary": "Retrieves the current time by getting the UTC datetime object using **`_get_current_utc_time`** (defined in **`get_current_time_epoch_ms.py`**) (which relies on the **`datetime`** library) and then converting that object into milliseconds since the epoch using **`_convert_to_epoch_ms`** (defined in **`get_current_time_epoch_ms.py`**)."}
{"code": "def _create_list_from_range(start, end, step=1):\n    return list(range(start, end, step))\n\ndef _filter_out_multiples(number_list, divisor=5):\n    return [n for n in number_list if n % divisor != 0]\n\ndef generate_and_filter_numbers(start_val, end_val, skip_multiple_of):\n    # Dependency 1: Create a list of integers within the specified range\n    full_range = _create_list_from_range(start_val, end_val)\n    \n    # Dependency 2: Filter the list to exclude numbers that are a multiple of the divisor\n    filtered_list = _filter_out_multiples(full_range, skip_multiple_of)\n    \n    return filtered_list", "summary": "Generates a list of integers within a specified range using **`_create_list_from_range`** (defined in **`generate_and_filter_numbers.py`**) and then filters that list to exclude numbers that are multiples of a specific divisor using **`_filter_out_multiples`** (defined in **`generate_and_filter_numbers.py`**)."}
{"code": "def _get_system_environment_variables():\n    import os\n    return dict(os.environ)\n\ndef _find_secret_keys(env_vars, secret_prefixes=('API_KEY', 'SECRET', 'TOKEN')):\n    return {k: '***HIDDEN***' for k in env_vars if any(k.startswith(p) for p in secret_prefixes)}\n\ndef audit_environment_for_secrets():\n    # Dependency 1: Get all current system environment variables\n    env_vars = _get_system_environment_variables()\n    \n    # Dependency 2: Find keys that likely contain secrets based on common prefixes\n    found_secrets = _find_secret_keys(env_vars)\n    \n    return found_secrets", "summary": "Audits the system environment variables for potential secrets by retrieving all environment variables using **`_get_system_environment_variables`** (defined in **`audit_environment_for_secrets.py`**) (which relies on the **`os`** library) and then filtering the keys based on common secret prefixes using **`_find_secret_keys`** (defined in **`audit_environment_for_secrets.py`**)."}
{"code": "def _convert_to_snake_case(text):\n    import re\n    text = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', text)\n    text = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', text).lower()\n    return text\n\ndef _prefix_with_namespace(snake_case_text, namespace='app_'):\n    return f'{namespace}{snake_case_text}'\n\ndef create_namespaced_variable_name(camel_case_input):\n    # Dependency 1: Convert the camelCase input string to snake_case\n    snake_case = _convert_to_snake_case(camel_case_input)\n    \n    # Dependency 2: Prefix the snake_case string with a standard namespace\n    namespaced_var = _prefix_with_namespace(snake_case)\n    \n    return namespaced_var", "summary": "Creates a namespaced variable name by first converting a **`camel_case_input`** string into snake\\_case using **`_convert_to_snake_case`** (defined in **`create_namespaced_variable_name.py`**) (which relies on the **`re`** library) and then prepending a default namespace ('app\\_') using **`_prefix_with_namespace`** (defined in **`create_namespaced_variable_name.py`**)."}
{"code": "def _read_data_from_redis_stream(stream_key, last_id):\n    # Mock stream read returning new messages\n    return [{'id': '167888', 'data': 'message 1'}, {'id': '167889', 'data': 'message 2'}]\n\ndef _get_latest_id(messages):\n    return messages[-1].get('id') if messages else None\n\ndef process_redis_stream_messages(stream_name, starting_id):\n    # Dependency 1: Read new messages from the mock Redis stream\n    new_messages = _read_data_from_redis_stream(stream_name, starting_id)\n    \n    # Dependency 2: Extract the ID of the very last message received\n    last_read_id = _get_latest_id(new_messages)\n    \n    return {'messages': new_messages, 'new_last_id': last_read_id}", "summary": "Processes messages from a mock Redis stream by reading new entries starting from a **`starting_id`** using **`_read_data_from_redis_stream`** (defined in **`process_redis_stream_messages.py`**) and then extracting the ID of the very last message received using **`_get_latest_id`** (defined in **`process_redis_stream_messages.py`**)."}
{"code": "def _find_list_duplicates(input_list):\n    import collections\n    counts = collections.Counter(input_list)\n    return [item for item, count in counts.items() if count > 1]\n\ndef _remove_duplicates_from_list(input_list):\n    return list(set(input_list))\n\ndef isolate_and_remove_duplicates(data_list):\n    # Dependency 1: Find all elements that appear more than once\n    duplicate_elements = _find_list_duplicates(data_list)\n    \n    # Dependency 2: Create a new list with only unique elements\n    unique_list = _remove_duplicates_from_list(data_list)\n    \n    return {'duplicates': duplicate_elements, 'unique_list': unique_list}", "summary": "Isolates and removes duplicates from a list. It first identifies all duplicate elements using **`_find_list_duplicates`** (defined in **`isolate_and_remove_duplicates.py`**) (which relies on the **`collections`** library) and then creates a new list containing only unique elements using **`_remove_duplicates_from_list`** (defined in **`isolate_and_remove_duplicates.py`**)."}
{"code": "def _calculate_sample_skewness(data_list):\n    import scipy.stats\n    # Requires external library scipy\n    return scipy.stats.skew(data_list)\n\ndef _classify_skewness(skew_value, threshold=0.5):\n    if skew_value > threshold: return 'Positive'\n    if skew_value < -threshold: return 'Negative'\n    return 'Symmetric'\n\ndef analyze_data_skewness(data_sample):\n    import scipy.stats\n    # Dependency 1: Calculate the sample skewness of the distribution\n    skew_val = _calculate_sample_skewness(data_sample)\n    \n    # Dependency 2: Classify the distribution as Positive, Negative, or Symmetric\n    classification = _classify_skewness(skew_val)\n    \n    return {'skewness': skew_val, 'classification': classification}", "summary": "Analyzes a data distribution's skewness by calculating the sample skewness value using **`_calculate_sample_skewness`** (defined in **`analyze_data_skewness.py`**) (which relies on the external **`scipy.stats`** library) and then classifying the skewness as Positive, Negative, or Symmetric using **`_classify_skewness`** (defined in **`analyze_data_skewness.py`**)."}
{"code": "def _read_data_from_ini_section(file_path, section_name):\n    import configparser\n    config = configparser.ConfigParser()\n    config.read(file_path)\n    return dict(config.items(section_name)) if section_name in config else {}\n\ndef _check_if_all_keys_present(config_dict, required_keys):\n    return all(k in config_dict for k in required_keys)\n\ndef validate_config_section(ini_file, section, required_settings):\n    # Dependency 1: Read the settings only for the specified INI section\n    settings = _read_data_from_ini_section(ini_file, section)\n    \n    # Dependency 2: Check if all required keys are present in the section settings\n    is_complete = _check_if_all_keys_present(settings, required_settings)\n    \n    return is_complete", "summary": "Validates a configuration section by first reading the settings only for the specified **`section`** from an INI file using **`_read_data_from_ini_section`** (defined in **`validate_config_section.py`**) (which relies on the **`configparser`** library) and then checking if all **`required_settings`** are present using **`_check_if_all_keys_present`** (defined in **`validate_config_section.py`**)."}
{"code": "def _get_current_username_and_id():\n    import os\n    return os.getuid(), os.getlogin()\n\ndef _format_user_display_name(uid, username):\n    return f\"{username} (UID: {uid})\"\n\ndef get_formatted_user_info():\n    import os\n    # Dependency 1: Get the current user ID (UID) and username\n    uid, uname = _get_current_username_and_id()\n    \n    # Dependency 2: Format the ID and username into a single display string\n    display_info = _format_user_display_name(uid, uname)\n    \n    return display_info", "summary": "Retrieves the current user information by getting the user ID (UID) and username using **`_get_current_username_and_id`** (defined in **`get_formatted_user_info.py`**) (which relies on the **`os`** library) and then formatting this information into a display string using **`_format_user_display_name`** (defined in **`get_formatted_user_info.py`**)."}
{"code": "def _create_date_range_list(start_date, end_date):\n    import pandas as pd\n    # Mock use of pandas for date range generation\n    return pd.date_range(start=start_date, end=end_date, freq='D').strftime('%Y-%m-%d').tolist()\n\ndef _filter_weekends(date_list):\n    import datetime\n    # Check if a date string is a weekday (Monday=0, Sunday=6)\n    is_weekday = lambda d: datetime.datetime.strptime(d, '%Y-%m-%d').weekday() < 5\n    return [d for d in date_list if is_weekday(d)]\n\ndef generate_business_days_in_range(start, end):\n    # Dependency 1: Generate a full list of dates in the range (mocking pandas)\n    full_range = _create_date_range_list(start, end)\n    \n    # Dependency 2: Filter the list to remove weekend dates (Sat/Sun)\n    business_days = _filter_weekends(full_range)\n    \n    return business_days", "summary": "Generates a list of business days between a **`start`** and **`end`** date by first generating the full date range using **`_create_date_range_list`** (defined in **`generate_business_days_in_range.py`**) (mocking the **`pandas`** library) and then filtering out weekend dates using **`_filter_weekends`** (defined in **`generate_business_days_in_range.py`**) (which relies on the **`datetime`** library)."}
{"code": "def _compute_rsa_encryption(plaintext, public_key):\n    # Mock encryption (just reverses the text)\n    return plaintext[::-1]\n\ndef _base64_encode_bytes(encrypted_bytes):\n    import base64\n    return base64.b64encode(encrypted_bytes.encode()).decode('utf-8')\n\ndef encrypt_and_encode_data(raw_data, recipient_key):\n    # Dependency 1: Encrypt the raw data using the mock RSA public key\n    encrypted = _compute_rsa_encryption(raw_data, recipient_key)\n    \n    # Dependency 2: Base64 encode the resulting encrypted bytes\n    b64_output = _base64_encode_bytes(encrypted)\n    \n    return b64_output", "summary": "Encrypts **`raw_data`** using a mock RSA encryption process via **`_compute_rsa_encryption`** (defined in **`encrypt_and_encode_data.py`**) and then Base64 encodes the resulting ciphertext using **`_base64_encode_bytes`** (defined in **`encrypt_and_encode_data.py`**) (which relies on the **`base64`** library)."}
{"code": "def _validate_if_contains_non_ascii(text):\n    return any(ord(c) > 127 for c in text)\n\ndef _encode_to_ascii_safe(text, fallback='?'):\n    return text.encode('ascii', errors='replace').decode('ascii')\n\ndef check_and_sanitize_text(input_text):\n    # Dependency 1: Check if the text contains any non-ASCII characters\n    has_non_ascii = _validate_if_contains_non_ascii(input_text)\n    \n    # Dependency 2: Encode the text to ASCII, replacing non-ASCII characters with '?'\n    sanitized_text = _encode_to_ascii_safe(input_text)\n    \n    return {'contains_non_ascii': has_non_ascii, 'sanitized_output': sanitized_text}", "summary": "Checks an **`input_text`** for the presence of non-ASCII characters using **`_validate_if_contains_non_ascii`** (defined in **`check_and_sanitize_text.py`**) and then sanitizes the text by encoding it to ASCII and replacing any non-ASCII characters with '?' using **`_encode_to_ascii_safe`** (defined in **`check_and_sanitize_text.py`**)."}
{"code": "def _get_process_open_file_count():\n    import psutil\n    process = psutil.Process()\n    return len(process.open_files())\n\ndef _log_file_descriptor_usage(fd_count, max_fds=1024):\n    if fd_count > max_fds * 0.8: print(f\"[WARNING] FD usage high: {fd_count} open files.\")\n\ndef monitor_file_descriptor_usage():\n    # Dependency 1: Get the count of files currently open by the process\n    open_fd_count = _get_process_open_file_count()\n    \n    # Dependency 2: Log a warning if the file descriptor count is approaching a limit\n    _log_file_descriptor_usage(open_fd_count)\n    \n    return open_fd_count", "summary": "Monitors file descriptor usage by retrieving the count of files currently open by the process using **`_get_process_open_file_count`** (defined in **`monitor_file_descriptor_usage.py`**) (which relies on the external **`psutil`** library) and then logs a warning if this count exceeds a usage threshold using **`_log_file_descriptor_usage`** (defined in **`monitor_file_descriptor_usage.py`**)."}
{"code": "def _remove_html_tags(html_content):\n    import re\n    return re.sub(r'<[^>]+>', '', html_content)\n\ndef _truncate_to_summary(plain_text, max_length=150):\n    return plain_text[:max_length] + '...' if len(plain_text) > max_length else plain_text\n\ndef create_text_summary_from_html(raw_html):\n    # Dependency 1: Remove all HTML tags to get plain text\n    plain_text = _remove_html_tags(raw_html)\n    \n    # Dependency 2: Truncate the plain text to a maximum length for a summary\n    summary = _truncate_to_summary(plain_text)\n    \n    return summary", "summary": "Creates a text summary from raw HTML content by first removing all HTML tags using **`_remove_html_tags`** (defined in **`create_text_summary_from_html.py`**) (which relies on the **`re`** library) and then truncating the resulting plain text to a maximum length using **`_truncate_to_summary`** (defined in **`create_text_summary_from_html.py`**)."}
{"code": "def _get_list_of_prime_numbers(limit):\n    primes = []\n    for n in range(2, limit + 1):\n        if all(n % i != 0 for i in range(2, int(n**0.5) + 1)):\n            primes.append(n)\n    return primes\n\ndef _calculate_sum(number_list):\n    return sum(number_list)\n\ndef compute_sum_of_primes_up_to_limit(max_value):\n    # Dependency 1: Generate a list of all prime numbers up to the limit\n    prime_list = _get_list_of_prime_numbers(max_value)\n    \n    # Dependency 2: Calculate the sum of all the generated prime numbers\n    total_sum = _calculate_sum(prime_list)\n    \n    return total_sum", "summary": "Computes the sum of all prime numbers up to a **`max_value`** by first generating the list of prime numbers using **`_get_list_of_prime_numbers`** (defined in **`compute_sum_of_primes_up_to_limit.py`**) and then calculating the total sum of that list using **`_calculate_sum`** (defined in **`compute_sum_of_primes_up_to_limit.py`**)."}
{"code": "def _fetch_user_preferences(user_id):\n    # Mock database lookup\n    return {'theme': 'dark', 'language': 'en', 'notifications': True}\n\ndef _check_notification_status(preferences):\n    return preferences.get('notifications', False)\n\ndef verify_user_notifications_enabled(uid):\n    # Dependency 1: Fetch the user's mock preference settings\n    user_prefs = _fetch_user_preferences(uid)\n    \n    # Dependency 2: Check specifically if the 'notifications' flag is enabled\n    is_enabled = _check_notification_status(user_prefs)\n    \n    return is_enabled", "summary": "Verifies if a user has notifications enabled by first fetching the user's mock preference settings using **`_fetch_user_preferences`** (defined in **`verify_user_notifications_enabled.py`**) and then checking the 'notifications' flag within those preferences using **`_check_notification_status`** (defined in **`verify_user_notifications_enabled.py`**)."}
{"code": "def _compute_levenshtein_ratio(s1, s2):\n    import Levenshtein\n    # Similarity ratio: 1 - (distance / max_length)\n    return Levenshtein.ratio(s1, s2)\n\ndef _check_if_fuzzy_match(ratio, min_ratio=0.8):\n    return ratio >= min_ratio\n\ndef check_string_fuzzy_match(str_a, str_b):\n    import Levenshtein\n    # Dependency 1: Compute the Levenshtein similarity ratio\n    similarity_ratio = _compute_levenshtein_ratio(str_a, str_b)\n    \n    # Dependency 2: Check if the ratio indicates a fuzzy match (e.g., ratio >= 0.8)\n    is_match = _check_if_fuzzy_match(similarity_ratio)\n    \n    return {'ratio': similarity_ratio, 'is_fuzzy_match': is_match}", "summary": "Checks for a fuzzy match between **`str_a`** and **`str_b`** by first calculating the Levenshtein similarity ratio using **`_compute_levenshtein_ratio`** (defined in **`check_string_fuzzy_match.py`**) (which relies on the **`Levenshtein`** library) and then checking if the ratio meets a minimum threshold using **`_check_if_fuzzy_match`** (defined in **`check_string_fuzzy_match.py`**)."}
{"code": "def _format_dict_to_yaml_string(data_dict):\n    import yaml\n    return yaml.dump(data_dict, default_flow_style=False)\n\ndef _log_yaml_content(yaml_data):\n    print(f\"--- YAML Output ---\\n{yaml_data}\\n---\")\n\ndef serialize_to_yaml_and_log(input_data):\n    # Dependency 1: Serialize the input dictionary to a YAML string\n    yaml_string = _format_dict_to_yaml_string(input_data)\n    \n    # Dependency 2: Log the generated YAML content\n    _log_yaml_content(yaml_string)\n    \n    return yaml_string", "summary": "Serializes an **`input_data`** dictionary into a YAML string using **`_format_dict_to_yaml_string`** (defined in **`serialize_to_yaml_and_log.py`**) (which relies on the external **`yaml`** library) and then logs the resulting YAML content using **`_log_yaml_content`** (defined in **`serialize_to_yaml_and_log.py`**)."}
{"code": "def _extract_domain_from_url(url):\n    import urllib.parse\n    return urllib.parse.urlparse(url).netloc\n\ndef _check_if_domain_is_external(domain, internal_domains=['corp.com', 'app.com']):\n    return domain not in internal_domains\n\ndef analyze_url_domain_type(full_url):\n    # Dependency 1: Extract the network location (domain) from the URL\n    domain = _extract_domain_from_url(full_url)\n    \n    # Dependency 2: Check if the extracted domain is considered external\n    is_external = _check_if_domain_is_external(domain)\n    \n    return {'domain': domain, 'is_external': is_external}", "summary": "Analyzes a **`full_url`** by first extracting the network location (domain) using **`_extract_domain_from_url`** (defined in **`analyze_url_domain_type.py`**) (which relies on the **`urllib.parse`** library) and then checking if the extracted domain is *not* present in a list of predefined internal domains using **`_check_if_domain_is_external`** (defined in **`analyze_url_domain_type.py`**)."}
{"code": "def _compute_fibonacci_recursive(n):\n    if n <= 1: return n\n    return _compute_fibonacci_recursive(n-1) + _compute_fibonacci_recursive(n-2)\n\ndef _check_if_number_is_perfect_square(n):\n    import math\n    if n < 0: return False\n    sqrt_n = int(math.sqrt(n))\n    return sqrt_n * sqrt_n == n\n\ndef analyze_fibonacci_square_property(index):\n    # Dependency 1: Compute the Fibonacci number at the given index\n    fib_number = _compute_fibonacci_recursive(index)\n    \n    # Dependency 2: Check if the resulting Fibonacci number is a perfect square\n    is_square = _check_if_number_is_perfect_square(fib_number)\n    \n    return {'fib_number': fib_number, 'is_perfect_square': is_square}", "summary": "Analyzes the Fibonacci number at a given **`index`** by first computing the value recursively using **`_compute_fibonacci_recursive`** (defined in **`analyze_fibonacci_square_property.py`**) and then checking if the resulting number is a perfect square using **`_check_if_number_is_perfect_square`** (defined in **`analyze_fibonacci_square_property.py`**) (which relies on the **`math`** library)."}
{"code": "def _fetch_data_from_remote_api(endpoint):\n    # Mock successful API call\n    return {'status': 200, 'data': [{'id': 1}, {'id': 2}] }\n\ndef _verify_api_response_structure(response_dict, required_key='data'):\n    return isinstance(response_dict.get(required_key), list)\n\ndef execute_and_verify_api_call(api_endpoint):\n    # Dependency 1: Execute the mock API call and get the response\n    api_response = _fetch_data_from_remote_api(api_endpoint)\n    \n    # Dependency 2: Verify if the response has a 'data' key containing a list\n    is_valid_structure = _verify_api_response_structure(api_response)\n    \n    return is_valid_structure", "summary": "Executes a mock remote API call using **`_fetch_data_from_remote_api`** (defined in **`execute_and_verify_api_call.py`**) and then verifies if the resulting response dictionary contains a top-level key named 'data' that holds a list, using **`_verify_api_response_structure`** (defined in **`execute_and_verify_api_call.py`**)."}
{"code": "def _check_if_list_contains_only_integers(input_list):\n    return all(isinstance(x, int) for x in input_list)\n\ndef _log_type_status(list_name, is_integer_list):\n    status = 'PASS: All Integers' if is_integer_list else 'FAIL: Mixed Types'\n    print(f\"[TYPE_CHECK] List {list_name}: {status}\")\n\ndef audit_list_type_consistency(data_list, list_identifier):\n    # Dependency 1: Check if every element in the list is an integer\n    is_pure_int = _check_if_list_contains_only_integers(data_list)\n    \n    # Dependency 2: Log the result of the type check\n    _log_type_status(list_identifier, is_pure_int)\n    \n    return is_pure_int", "summary": "Audits a list for type consistency by checking if every element in the **`data_list`** is an integer using **`_check_if_list_contains_only_integers`** (defined in **`audit_list_type_consistency.py`**) and then logging the result (PASS/FAIL) with a list identifier using **`_log_type_status`** (defined in **`audit_list_type_consistency.py`**)."}
{"code": "def _get_system_load_averages():\n    import os\n    # Returns 1, 5, and 15 minute load averages\n    return os.getloadavg()\n\ndef _check_if_load_is_high(load_averages, core_count=4, threshold_factor=1.5):\n    # Check if 1-minute load exceeds 1.5x core count\n    return load_averages[0] > core_count * threshold_factor\n\ndef monitor_system_load(num_cores):\n    import os\n    # Dependency 1: Get the 1, 5, and 15 minute system load averages\n    load_1, load_5, load_15 = _get_system_load_averages()\n    \n    # Dependency 2: Check if the 1-minute load is critically high relative to core count\n    is_high = _check_if_load_is_high((load_1, load_5, load_15), num_cores)\n    \n    return {'load_1m': load_1, 'is_critically_high': is_high}", "summary": "Monitors system load by retrieving the 1, 5, and 15 minute load averages using **`_get_system_load_averages`** (defined in **`monitor_system_load.py`**) (which relies on the **`os`** library) and then checking if the 1-minute load is critically high relative to the number of CPU cores using **`_check_if_load_is_high`** (defined in **`monitor_system_load.py`**)."}
{"code": "def _find_highest_occurrence(data_list):\n    import collections\n    counts = collections.Counter(data_list)\n    if not counts: return None, 0\n    most_common = counts.most_common(1)[0]\n    return most_common[0], most_common[1]\n\ndef _check_if_majority_element(count, total_size, required_ratio=0.5):\n    return count / total_size > required_ratio\n\ndef analyze_list_for_majority_element(input_list):\n    # Dependency 1: Find the element with the highest occurrence and its count\n    element, count = _find_highest_occurrence(input_list)\n    \n    # Dependency 2: Check if the highest occurrence count constitutes a majority (over 50%)\n    is_majority = _check_if_majority_element(count, len(input_list))\n    \n    return {'most_frequent': element, 'is_majority': is_majority}", "summary": "Analyzes a list for a majority element by first finding the element with the highest occurrence and its count using **`_find_highest_occurrence`** (defined in **`analyze_list_for_majority_element.py`**) (which relies on the **`collections`** library) and then checking if that count exceeds 50% of the total list size using **`_check_if_majority_element`** (defined in **`analyze_list_for_majority_element.py`**)."}
{"code": "def _execute_ldap_bind(username, password):\n    # Mock LDAP binding\n    if username.startswith('CN='): return True\n    return False\n\ndef _log_auth_result(username, is_success):\n    result = 'AUTHORIZED' if is_success else 'DENIED'\n    print(f\"[AUTH_LDAP] User {username} status: {result}\")\n\ndef authenticate_ldap_user(user_dn, user_pw):\n    # Dependency 1: Attempt to bind/authenticate with the mock LDAP server\n    is_authenticated = _execute_ldap_bind(user_dn, user_pw)\n    \n    # Dependency 2: Log the authentication result for auditing\n    _log_auth_result(user_dn, is_authenticated)\n    \n    return is_authenticated", "summary": "Authenticates a user against a mock LDAP server by attempting a bind operation using **`_execute_ldap_bind`** (defined in **`authenticate_ldap_user.py`**) and then logging the authentication result (AUTHORIZED/DENIED) for auditing purposes using **`_log_auth_result`** (defined in **`authenticate_ldap_user.py`**)."}
{"code": "def _check_if_data_type_is_numeric(value):\n    return isinstance(value, (int, float))\n\ndef _convert_to_string_representation(value):\n    return str(value)\n\ndef inspect_and_stringify_value(data_value):\n    # Dependency 1: Check if the input value is a numeric type (int or float)\n    is_numeric = _check_if_data_type_is_numeric(data_value)\n    \n    # Dependency 2: Convert the value to its string representation\n    string_value = _convert_to_string_representation(data_value)\n    \n    return {'is_numeric': is_numeric, 'string_value': string_value}", "summary": "Inspects a **`data_value`** by first checking if it is a numeric type (integer or float) using **`_check_if_data_type_is_numeric`** (defined in **`inspect_and_stringify_value.py`**) and then converting the value into its string representation using **`_convert_to_string_representation`** (defined in **`inspect_and_stringify_value.py`**)."}
{"code": "def _get_ipv4_broadcast_address(ip_address, subnet_mask):\n    # Mock network calculation\n    return '192.168.1.255'\n\ndef _check_if_ip_is_host_address(ip_address, broadcast_address):\n    # Simplified check\n    return ip_address != broadcast_address\n\ndef verify_host_ip_address(input_ip, input_mask):\n    # Dependency 1: Calculate the network's broadcast address\n    broadcast_addr = _get_ipv4_broadcast_address(input_ip, input_mask)\n    \n    # Dependency 2: Check if the input IP is the broadcast address (and thus not a host address)\n    is_host = _check_if_ip_is_host_address(input_ip, broadcast_addr)\n    \n    return {'is_host_address': is_host, 'broadcast': broadcast_addr}", "summary": "Verifies if an **`input_ip`** address is a valid host address by first calculating the network's broadcast address using **`_get_ipv4_broadcast_address`** (defined in **`verify_host_ip_address.py`**) and then checking if the input IP is different from the calculated broadcast address using **`_check_if_ip_is_host_address`** (defined in **`verify_host_ip_address.py`**)."}
{"code": "def _get_list_chunks_with_remainder(data_list, chunk_size):\n    chunks = []\n    for i in range(0, len(data_list), chunk_size):\n        chunks.append(data_list[i:i + chunk_size])\n    return chunks\n\ndef _get_remainder_size(data_list, chunk_size):\n    return len(data_list) % chunk_size\n\ndef analyze_list_chunking(full_list, size):\n    # Dependency 1: Split the list into chunks (including the final remainder chunk)\n    chunks = _get_list_chunks_with_remainder(full_list, size)\n    \n    # Dependency 2: Calculate the size of the remainder (last chunk size if not full)\n    remainder_size = _get_remainder_size(full_list, size)\n    \n    return {'chunk_count': len(chunks), 'remainder_size': remainder_size}", "summary": "Analyzes the chunking of a list by first splitting the **`full_list`** into chunks of a specified **`size`** using **`_get_list_chunks_with_remainder`** (defined in **`analyze_list_chunking.py`**) and then calculating the size of the final remainder chunk using **`_get_remainder_size`** (defined in **`analyze_list_chunking.py`**)."}
{"code": "def _get_random_choice(item_list):\n    import random\n    return random.choice(item_list)\n\ndef _log_selection(item):\n    print(f\"[SELECTION] Randomly selected: {item}\")\n\ndef select_random_item_and_log(options):\n    # Dependency 1: Select a single item randomly from the list\n    selected_item = _get_random_choice(options)\n    \n    # Dependency 2: Log the selected item\n    _log_selection(selected_item)\n    \n    return selected_item", "summary": "Selects a single item randomly from the list of **`options`** using **`_get_random_choice`** (defined in **`select_random_item_and_log.py`**) (which relies on the **`random`** library) and then logs the selected item using **`_log_selection`** (defined in **`select_random_item_and_log.py`**)."}
{"code": "def _check_if_contains_all_vowels(text):\n    normalized_text = set(c.lower() for c in text if c.isalpha())\n    required_vowels = set('aeiou')\n    return required_vowels.issubset(normalized_text)\n\ndef _count_unique_consonants(text):\n    consonants = set('bcdfghjklmnpqrstvwxyz')\n    unique_chars = set(c.lower() for c in text if c.isalpha())\n    return len(consonants.intersection(unique_chars))\n\ndef analyze_text_vowels_consonants(input_text):\n    # Dependency 1: Check if the text contains at least one of every major vowel\n    has_all_vowels = _check_if_contains_all_vowels(input_text)\n    \n    # Dependency 2: Count the number of unique consonants present\n    unique_consonant_count = _count_unique_consonants(input_text)\n    \n    return {'has_all_vowels': has_all_vowels, 'unique_consonant_count': unique_consonant_count}", "summary": "Analyzes a text string by checking if it contains at least one of every major English vowel ('a', 'e', 'i', 'o', 'u') using **`_check_if_contains_all_vowels`** (defined in **`analyze_text_vowels_consonants.py`**) and then counting the number of unique consonant characters present using **`_count_unique_consonants`** (defined in **`analyze_text_vowels_consonants.py`**)."}
{"code": "def _get_random_choice(item_list):\n    import random\n    return random.choice(item_list)\n\ndef _log_selection(item):\n    print(f\"[SELECTION] Randomly selected: {item}\")\n\ndef select_random_item_and_log(options):\n    # Dependency 1: Select a single item randomly from the list\n    selected_item = _get_random_choice(options)\n    \n    # Dependency 2: Log the selected item\n    _log_selection(selected_item)\n    \n    return selected_item", "summary": "Selects a single item randomly from the list of **`options`** using **`_get_random_choice`** (defined in **`select_random_item_and_log.py`**) (which relies on the **`random`** library) and then logs the selected item using **`_log_selection`** (defined in **`select_random_item_and_log.py`**)."}
{"code": "def _check_if_contains_all_vowels(text):\n    normalized_text = set(c.lower() for c in text if c.isalpha())\n    required_vowels = set('aeiou')\n    return required_vowels.issubset(normalized_text)\n\ndef _count_unique_consonants(text):\n    consonants = set('bcdfghjklmnpqrstvwxyz')\n    unique_chars = set(c.lower() for c in text if c.isalpha())\n    return len(consonants.intersection(unique_chars))\n\ndef analyze_text_vowels_consonants(input_text):\n    # Dependency 1: Check if the text contains at least one of every major vowel\n    has_all_vowels = _check_if_contains_all_vowels(input_text)\n    \n    # Dependency 2: Count the number of unique consonants present\n    unique_consonant_count = _count_unique_consonants(input_text)\n    \n    return {'has_all_vowels': has_all_vowels, 'unique_consonant_count': unique_consonant_count}", "summary": "Analyzes a text string by checking if it contains at least one of every major English vowel ('a', 'e', 'i', 'o', 'u') using **`_check_if_contains_all_vowels`** (defined in **`analyze_text_vowels_consonants.py`**) and then counting the number of unique consonant characters present using **`_count_unique_consonants`** (defined in **`analyze_text_vowels_consonants.py`**)."}
{"code": "def _compute_sha3_512_hash(data):\n    import hashlib\n    return hashlib.sha3_512(data.encode('utf-8')).hexdigest()\n\ndef _truncate_hash_for_checksum(full_hash, length=10):\n    return full_hash[:length]\n\ndef generate_data_checksum(input_data):\n    # Dependency 1: Compute the full SHA3-512 hash of the input data\n    full_checksum = _compute_sha3_512_hash(input_data)\n    \n    # Dependency 2: Truncate the full hash to a shorter length for a quick checksum\n    short_checksum = _truncate_hash_for_checksum(full_checksum)\n    \n    return short_checksum", "summary": "Generates a data checksum by first computing the full SHA3-512 hash of the **`input_data`** using **`_compute_sha3_512_hash`** (defined in **`generate_data_checksum.py`**) (which relies on the **`hashlib`** library) and then truncating the result to the first 10 characters using **`_truncate_hash_for_checksum`** (defined in **`generate_data_checksum.py`**)."}
{"code": "def _verify_email_format(email_str):\n    import re\n    # Simple regex for email validation\n    regex = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}$'\n    return bool(re.fullmatch(regex, email_str))\n\ndef _normalize_email_to_lower(email_str):\n    return email_str.lower()\n\ndef validate_and_normalize_email(email_candidate):\n    # Dependency 1: Verify the email string conforms to a basic format pattern\n    is_valid = _verify_email_format(email_candidate)\n    \n    # Dependency 2: If valid, normalize the email address to lowercase\n    normalized_email = _normalize_email_to_lower(email_candidate) if is_valid else None\n    \n    return {'is_valid': is_valid, 'normalized': normalized_email}", "summary": "Validates and normalizes an **`email_candidate`** by first verifying its format against a simple regex pattern using **`_verify_email_format`** (defined in **`validate_and_normalize_email.py`**) (which relies on the **`re`** library) and then converting the valid email to lowercase using **`_normalize_email_to_lower`** (defined in **`validate_and_normalize_email.py`**)."}
{"code": "def _send_log_to_syslog(message, priority='info'):\n    import syslog\n    syslog.syslog(syslog.LOG_INFO, f\"[{priority.upper()}] {message}\")\n\ndef _format_audit_message(action, user, status):\n    return f\"AUDIT: Action '{action}' by user {user} - Status: {status}\"\n\ndef log_user_action_audit(action_name, acting_user, result_status):\n    import syslog\n    # Dependency 1: Format the detailed audit message\n    audit_msg = _format_audit_message(action_name, acting_user, result_status)\n    \n    # Dependency 2: Send the formatted message to the system syslog (mocked)\n    _send_log_to_syslog(audit_msg)\n    \n    return audit_msg", "summary": "Logs a user action audit by first formatting the detailed message (action, user, status) using **`_format_audit_message`** (defined in **`log_user_action_audit.py`**) and then sending the formatted message with an 'info' priority to the system syslog using **`_send_log_to_syslog`** (defined in **`log_user_action_audit.py`**) (which relies on the **`syslog`** library)."}
{"code": "def _compute_list_standard_deviation(number_list):\n    import statistics\n    if len(number_list) < 2: return 0.0\n    return statistics.stdev(number_list)\n\ndef _check_if_data_is_volatile(std_dev, mean, threshold=0.2):\n    # Check if coefficient of variation is high\n    if mean == 0: return std_dev > 0\n    return (std_dev / abs(mean)) > threshold\n\ndef analyze_data_volatility(data_points):\n    import statistics\n    # Dependency 1: Compute the sample standard deviation\n    std_dev = _compute_list_standard_deviation(data_points)\n    \n    # Dependency 2: Check if the data is considered volatile based on CV (STD/Mean)\n    mean = statistics.mean(data_points)\n    is_volatile = _check_if_data_is_volatile(std_dev, mean)\n    \n    return {'stdev': std_dev, 'is_volatile': is_volatile}", "summary": "Analyzes data volatility by first computing the sample standard deviation of the **`data_points`** using **`_compute_list_standard_deviation`** (defined in **`analyze_data_volatility.py`**) (which relies on the **`statistics`** library) and then checking if the coefficient of variation (STD/Mean) exceeds a threshold (default 0.2) using **`_check_if_data_is_volatile`** (defined in **`analyze_data_volatility.py`**)."}
{"code": "def _check_if_string_is_valid_url(text):\n    import re\n    # Simple check for scheme://domain\n    regex = r'^https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(/.*)?$'\n    return bool(re.match(regex, text))\n\ndef _log_validation_result(text, is_valid):\n    status = 'Valid URL' if is_valid else 'Invalid URL'\n    print(f\"[VALIDATION] '{text[:30]}...' is {status}\")\n\ndef validate_and_log_url(url_candidate):\n    # Dependency 1: Check if the input string looks like a valid URL\n    is_url = _check_if_string_is_valid_url(url_candidate)\n    \n    # Dependency 2: Log the result of the validation check\n    _log_validation_result(url_candidate, is_url)\n    \n    return is_url", "summary": "Validates a **`url_candidate`** by checking if it matches a basic URL structure (scheme://domain) using **`_check_if_string_is_valid_url`** (defined in **`validate_and_log_url.py`**) (which relies on the **`re`** library) and then logs the outcome of the validation using **`_log_validation_result`** (defined in **`validate_and_log_url.py`**)."}
{"code": "def _get_process_memory_info():\n    import psutil\n    process = psutil.Process()\n    # returns memory usage in bytes\n    return process.memory_info().rss\n\ndef _convert_bytes_to_megabytes(bytes_value):\n    return bytes_value / (1024 * 1024)\n\ndef get_process_memory_usage_mb():\n    # Dependency 1: Get the Resident Set Size (RSS) memory usage in bytes\n    rss_bytes = _get_process_memory_info()\n    \n    # Dependency 2: Convert the byte value into megabytes\n    rss_mb = _convert_bytes_to_megabytes(rss_bytes)\n    \n    return rss_mb", "summary": "Retrieves the current process memory usage by first getting the Resident Set Size (RSS) in bytes using **`_get_process_memory_info`** (defined in **`get_process_memory_usage_mb.py`**) (which relies on the external **`psutil`** library) and then converting that byte value into megabytes using **`_convert_bytes_to_megabytes`** (defined in **`get_process_memory_usage_mb.py`**)."}
{"code": "def _read_data_from_csv(file_path):\n    import csv\n    with open(file_path, 'r', newline='') as f:\n        reader = csv.DictReader(f)\n        return list(reader)\n\ndef _count_rows_with_specific_value(data_records, key, required_value):\n    return sum(1 for record in data_records if record.get(key) == required_value)\n\ndef count_csv_records_by_field(csv_file, column_key, target_value):\n    # Dependency 1: Read the CSV file into a list of dictionaries\n    records = _read_data_from_csv(csv_file)\n    \n    # Dependency 2: Count how many records have a specific value in a given column\n    match_count = _count_rows_with_specific_value(records, column_key, target_value)\n    \n    return match_count", "summary": "Counts CSV records matching a field value by first reading the CSV file into a list of dictionaries using **`_read_data_from_csv`** (defined in **`count_csv_records_by_field.py`**) (which relies on the **`csv`** library) and then counting the rows where the specified **`column_key`** matches the **`target_value`** using **`_count_rows_with_specific_value`** (defined in **`count_csv_records_by_field.py`**)."}
{"code": "def _compute_greatest_common_divisor(a, b):\n    import math\n    return math.gcd(a, b)\n\ndef _check_if_coprime(gcd_value):\n    # Two numbers are coprime if their GCD is 1\n    return gcd_value == 1\n\ndef check_if_numbers_are_coprime(num1, num2):\n    # Dependency 1: Compute the Greatest Common Divisor (GCD) of the two numbers\n    gcd = _compute_greatest_common_divisor(num1, num2)\n    \n    # Dependency 2: Check if the GCD is 1 (indicating they are coprime)\n    is_coprime = _check_if_coprime(gcd)\n    \n    return {'gcd': gcd, 'is_coprime': is_coprime}", "summary": "Checks if two numbers are coprime by first computing their Greatest Common Divisor (GCD) using **`_compute_greatest_common_divisor`** (defined in **`check_if_numbers_are_coprime.py`**) (which relies on the **`math`** library) and then verifying if the GCD equals 1 using **`_check_if_coprime`** (defined in **`check_if_numbers_are_coprime.py`**)."}
{"code": "def _extract_all_http_links(html_content):\n    import re\n    return re.findall(r'href=\"(https?://.*?)\"', html_content)\n\ndef _filter_out_duplicate_links(link_list):\n    return sorted(list(set(link_list)))\n\ndef extract_unique_links_from_html(web_content):\n    # Dependency 1: Find all HTTP/HTTPS links in the HTML content using regex\n    all_links = _extract_all_http_links(web_content)\n    \n    # Dependency 2: Remove duplicates and return a sorted list of unique links\n    unique_links = _filter_out_duplicate_links(all_links)\n    \n    return unique_links", "summary": "Extracts unique links from HTML by first finding all HTTP/HTTPS `href` attributes using regex via **`_extract_all_http_links`** (defined in **`extract_unique_links_from_html.py`**) (which relies on the **`re`** library) and then filtering the resulting list to remove duplicates and return a sorted list using **`_filter_out_duplicate_links`** (defined in **`extract_unique_links_from_html.py`**)."}
{"code": "def _get_system_network_interfaces():\n    import psutil\n    # Returns a dictionary of interfaces and their addresses\n    return psutil.net_if_addrs()\n\ndef _find_ipv4_address(interface_info):\n    for addr in interface_info:\n        if addr.family.name == 'AF_INET':\n            return addr.address\n    return None\n\ndef get_local_ipv4_addresses():\n    import psutil\n    # Dependency 1: Get information about all network interfaces\n    interfaces = _get_system_network_interfaces()\n    \n    # Dependency 2: Extract the primary IPv4 address for each interface\n    ipv4_map = {name: _find_ipv4_address(info) for name, info in interfaces.items()}\n    \n    return {k: v for k, v in ipv4_map.items() if v is not None}", "summary": "Retrieves local IPv4 addresses by first getting information on all network interfaces using **`_get_system_network_interfaces`** (defined in **`get_local_ipv4_addresses.py`**) (which relies on the external **`psutil`** library) and then iterating through the interfaces to extract the 'AF\\_INET' address for each one using **`_find_ipv4_address`** (defined in **`get_local_ipv4_addresses.py`**)."}
{"code": "def _verify_checksum_integrity(data, expected_checksum):\n    import hashlib\n    actual_checksum = hashlib.sha256(data).hexdigest()\n    return actual_checksum == expected_checksum\n\ndef _log_integrity_status(file_path, is_ok):\n    status = 'OK' if is_ok else 'FAILED'\n    print(f\"[INTEGRITY] File {file_path} checksum: {status}\")\n\ndef audit_file_integrity(file_content, file_name, known_checksum):\n    # Dependency 1: Verify the file content against the expected SHA256 checksum\n    is_intact = _verify_checksum_integrity(file_content, known_checksum)\n    \n    # Dependency 2: Log the result of the integrity check\n    _log_integrity_status(file_name, is_intact)\n    \n    return is_intact", "summary": "Audits file integrity by first computing the SHA256 hash of the **`file_content`** and comparing it to the **`known_checksum`** using **`_verify_checksum_integrity`** (defined in **`audit_file_integrity.py`**) (which relies on the **`hashlib`** library) and then logging the outcome of the integrity check using **`_log_integrity_status`** (defined in **`audit_file_integrity.py`**)."}
{"code": "def _format_time_difference_ms(time_diff_seconds):\n    return int(time_diff_seconds * 1000)\n\ndef _measure_execution_time(func, *args, **kwargs):\n    import time\n    start = time.perf_counter()\n    result = func(*args, **kwargs)\n    end = time.perf_counter()\n    return result, end - start\n\ndef benchmark_function_execution(function_to_run, *args, **kwargs):\n    # Dependency 1: Run the target function and measure its execution time in seconds\n    result, duration_s = _measure_execution_time(function_to_run, *args, **kwargs)\n    \n    # Dependency 2: Convert the duration from seconds to milliseconds\n    duration_ms = _format_time_difference_ms(duration_s)\n    \n    return {'result': result, 'duration_ms': duration_ms}", "summary": "Benchmarks a function's execution by running the **`function_to_run`** and measuring its duration in seconds using **`_measure_execution_time`** (defined in **`benchmark_function_execution.py`**) (which relies on the **`time`** library's `perf_counter`) and then converting the duration to milliseconds using **`_format_time_difference_ms`** (defined in **`benchmark_function_execution.py`**)."}
{"code": "def _get_config_value(config_dict, key_path, default=None):\n    keys = key_path.split('.')\n    value = config_dict\n    for key in keys:\n        if not isinstance(value, dict) or key not in value:\n            return default\n        value = value[key]\n    return value\n\ndef _cast_to_boolean(value):\n    if isinstance(value, str): return value.lower() in ('true', '1', 'yes')\n    return bool(value)\n\ndef extract_and_cast_config_boolean(config, path_key):\n    # Dependency 1: Extract a nested value from the config dictionary using a dot path\n    raw_value = _get_config_value(config, path_key)\n    \n    # Dependency 2: Cast the raw value (which might be a string like 'True') to a Python boolean\n    bool_value = _cast_to_boolean(raw_value)\n    \n    return bool_value", "summary": "Extracts and casts a configuration value by first extracting a potentially nested value from the **`config`** dictionary using a dot-separated **`path_key`** via **`_get_config_value`** (defined in **`extract_and_cast_config_boolean.py`**) and then converting the raw value (e.g., 'True', '1') into a native Python boolean using **`_cast_to_boolean`** (defined in **`extract_and_cast_config_boolean.py`**)."}
{"code": "def _compute_md5_hash(data_bytes):\n    import hashlib\n    return hashlib.md5(data_bytes).hexdigest()\n\ndef _check_hash_collision_list(hash_value, known_hashes):\n    return hash_value in known_hashes\n\ndef detect_hash_collision(byte_data, existing_hashes):\n    # Dependency 1: Compute the MD5 hash of the raw byte data\n    data_hash = _compute_md5_hash(byte_data)\n    \n    # Dependency 2: Check if the computed hash value is present in the list of known hashes\n    is_collision = _check_hash_collision_list(data_hash, existing_hashes)\n    \n    return {'hash': data_hash, 'is_collision': is_collision}", "summary": "Detects a hash collision by first computing the MD5 hash of the raw **`byte_data`** using **`_compute_md5_hash`** (defined in **`detect_hash_collision.py`**) (which relies on the **`hashlib`** library) and then checking if the computed hash value already exists within the **`existing_hashes`** list using **`_check_hash_collision_list`** (defined in **`detect_hash_collision.py`**)."}
{"code": "def _get_unique_list_of_dictionaries(dict_list, key_to_check):\n    seen = set()\n    unique_list = []\n    for d in dict_list:\n        value = d.get(key_to_check)\n        if value not in seen:\n            seen.add(value)\n            unique_list.append(d)\n    return unique_list\n\ndef _count_duplicates_removed(original_list, unique_list):\n    return len(original_list) - len(unique_list)\n\ndef filter_duplicate_records(record_list, unique_id_key):\n    # Dependency 1: Filter the list to keep only one dictionary per unique ID key value\n    unique_records = _get_unique_list_of_dictionaries(record_list, unique_id_key)\n    \n    # Dependency 2: Count how many duplicate records were removed\n    removed_count = _count_duplicates_removed(record_list, unique_records)\n    \n    return {'unique_records': unique_records, 'removed_count': removed_count}", "summary": "Filters a list of dictionaries (**`record_list`**) to remove duplicates based on a **`unique_id_key`** by first retrieving the unique records using **`_get_unique_list_of_dictionaries`** (defined in **`filter_duplicate_records.py`**) and then counting the number of records that were removed using **`_count_duplicates_removed`** (defined in **`filter_duplicate_records.py`**)."}
{"code": "def _connect_to_rabbitmq(host, port):\n    # Mock connection\n    print(f\"[MQ_CONN] Connected to RabbitMQ at {host}:{port}\")\n    return {'host': host, 'port': port, 'status': 'connected'}\n\ndef _publish_message(connection_info, queue_name, message):\n    # Mock publishing\n    print(f\"[MQ_SEND] Published to {queue_name}: {message[:20]}...\")\n    return True\n\ndef publish_to_message_queue(mq_host, mq_port, target_queue, payload):\n    # Dependency 1: Establish a mock connection to the message queue\n    conn = _connect_to_rabbitmq(mq_host, mq_port)\n    \n    # Dependency 2: Publish the payload to the specified queue\n    is_published = _publish_message(conn, target_queue, payload)\n    \n    return is_published", "summary": "Publishes a **`payload`** to a message queue by first establishing a mock connection using **`_connect_to_rabbitmq`** (defined in **`publish_to_message_queue.py`**) and then publishing the **`payload`** to the **`target_queue`** using the connection via **`_publish_message`** (defined in **`publish_to_message_queue.py`**)."}
{"code": "def _compute_simple_moving_average(data, window_size):\n    import numpy as np\n    # Requires external library numpy\n    weights = np.ones(window_size) / window_size\n    return np.convolve(data, weights, 'valid').tolist()\n\ndef _get_last_n_values(data, n=5):\n    return data[-n:]\n\ndef calculate_and_inspect_sma(data_series, window):\n    import numpy as np\n    # Dependency 1: Compute the Simple Moving Average (SMA) of the data series\n    sma_series = _compute_simple_moving_average(data_series, window)\n    \n    # Dependency 2: Get the last 5 computed SMA values for inspection\n    latest_sma = _get_last_n_values(sma_series)\n    \n    return {'sma_values': sma_series, 'latest_5': latest_sma}", "summary": "Calculates and inspects the Simple Moving Average (SMA) by first computing the SMA of the **`data_series`** using a specified **`window`** size via **`_compute_simple_moving_average`** (defined in **`calculate_and_inspect_sma.py`**) (which relies on the external **`numpy`** library for convolution) and then retrieving the last 5 calculated SMA values using **`_get_last_n_values`** (defined in **`calculate_and_inspect_sma.py`**)."}
{"code": "def _get_file_size_bytes(file_path):\n    import os\n    return os.path.getsize(file_path)\n\ndef _check_if_file_is_large(size_bytes, max_size_mb=100):\n    return size_bytes > (max_size_mb * 1024 * 1024)\n\ndef audit_file_size(target_file):\n    # Dependency 1: Get the size of the file in bytes\n    file_size = _get_file_size_bytes(target_file)\n    \n    # Dependency 2: Check if the file size exceeds a 'large' threshold (e.g., 100MB)\n    is_large = _check_if_file_is_large(file_size)\n    \n    return {'size_bytes': file_size, 'is_large': is_large}", "summary": "Audits a file size by first retrieving the size of the file in bytes using **`_get_file_size_bytes`** (defined in **`audit_file_size.py`**) (which relies on the **`os`** library) and then checking if this size exceeds a large file threshold (default 100MB) using **`_check_if_file_is_large`** (defined in **`audit_file_size.py`**)."}
{"code": "def _convert_timedelta_to_seconds(timedelta_obj):\n    return timedelta_obj.total_seconds()\n\ndef _get_time_since_epoch_seconds():\n    import datetime\n    # Use time.time() for high precision/performance, but datetime.datetime.now() for datetime obj\n    return datetime.datetime.now() - datetime.datetime(1970, 1, 1)\n\ndef get_current_epoch_seconds_from_dt():\n    import datetime\n    # Dependency 1: Calculate the timedelta since the epoch start (1970-01-01)\n    delta = _get_time_since_epoch_seconds()\n    \n    # Dependency 2: Convert the timedelta object to total seconds\n    total_seconds = _convert_timedelta_to_seconds(delta)\n    \n    return total_seconds", "summary": "Retrieves the current epoch time in seconds by first calculating the timedelta between the current time and the epoch start (1970-01-01) using **`_get_time_since_epoch_seconds`** (defined in **`get_current_epoch_seconds_from_dt.py`**) (which relies on the **`datetime`** library) and then converting that timedelta object into total seconds using **`_convert_timedelta_to_seconds`** (defined in **`get_current_epoch_seconds_from_dt.py`**)."}
{"code": "def _read_data_from_redis_key(redis_conn, key):\n    # Mock redis get operation\n    return f\"value for {key}\"\n\ndef _check_if_data_is_stale(value_string, ttl_seconds=3600):\n    # Mock staleness check based on value content\n    return 'stale' in value_string\n\ndef get_and_check_cache_staleness(redis_client, cache_key):\n    # Dependency 1: Retrieve the value associated with the key from mock Redis\n    cached_value = _read_data_from_redis_key(redis_client, cache_key)\n    \n    # Dependency 2: Check if the retrieved value is marked as 'stale' (mocked)\n    is_stale = _check_if_data_is_stale(cached_value)\n    \n    return {'value': cached_value, 'is_stale': is_stale}", "summary": "Retrieves a value from a mock Redis cache using **`_read_data_from_redis_key`** (defined in **`get_and_check_cache_staleness.py`**) and then checks if the retrieved **`cached_value`** is considered 'stale' based on a mock check against the string content using **`_check_if_data_is_stale`** (defined in **`get_and_check_cache_staleness.py`**)."}
{"code": "def _check_if_path_is_directory(file_path):\n    import os\n    return os.path.isdir(file_path)\n\ndef _log_path_type(path, is_dir):\n    type_str = 'Directory' if is_dir else 'File/Other'\n    print(f\"[PATH_CHECK] '{path}' is a {type_str}\")\n\ndef audit_path_type(target_path):\n    # Dependency 1: Check if the target path points to a directory\n    is_dir = _check_if_path_is_directory(target_path)\n    \n    # Dependency 2: Log the determined type of the path\n    _log_path_type(target_path, is_dir)\n    \n    return is_dir", "summary": "Audits a file system path by checking if the **`target_path`** is a directory using **`_check_if_path_is_directory`** (defined in **`audit_path_type.py`**) (which relies on the **`os`** library) and then logs the result (Directory/File) using **`_log_path_type`** (defined in **`audit_path_type.py`**)."}
{"code": "def _compute_least_common_multiple(a, b):\n    import math\n    if a == 0 or b == 0: return 0\n    return abs(a * b) // math.gcd(a, b)\n\ndef _check_if_multiple_is_even(lcm_value):\n    return lcm_value % 2 == 0\n\ndef analyze_lcm_parity(num1, num2):\n    import math\n    # Dependency 1: Compute the Least Common Multiple (LCM) of the two numbers\n    lcm = _compute_least_common_multiple(num1, num2)\n    \n    # Dependency 2: Check if the resulting LCM is an even number\n    is_even = _check_if_multiple_is_even(lcm)\n    \n    return {'lcm': lcm, 'is_even': is_even}", "summary": "Analyzes the parity of the Least Common Multiple (LCM) by first computing the LCM of **`num1`** and **`num2`** using **`_compute_least_common_multiple`** (defined in **`analyze_lcm_parity.py`**) (which relies on **`math.gcd`**) and then checking if the resulting LCM value is an even number using **`_check_if_multiple_is_even`** (defined in **`analyze_lcm_parity.py`**)."}
{"code": "def _get_data_from_hdfs(hdfs_path):\n    # Mock HDFS retrieval\n    return f\"Data from HDFS path: {hdfs_path}\"\n\ndef _write_data_to_local_file(local_path, data_content):\n    # Mock file write\n    with open(local_path, 'w') as f:\n        f.write(data_content)\n    return len(data_content)\n\ndef transfer_hdfs_to_local(remote_path, local_target_file):\n    # Dependency 1: Fetch the data content from the mock HDFS path\n    content = _get_data_from_hdfs(remote_path)\n    \n    # Dependency 2: Write the retrieved data to a local file path\n    bytes_written = _write_data_to_local_file(local_target_file, content)\n    \n    return bytes_written", "summary": "Transfers data from HDFS to a local file by first fetching the data content from the mock **`remote_path`** using **`_get_data_from_hdfs`** (defined in **`transfer_hdfs_to_local.py`**) and then writing that retrieved content to the **`local_target_file`** using **`_write_data_to_local_file`** (defined in **`transfer_hdfs_to_local.py`**)."}
{"code": "def _calculate_harmonic_mean(numbers):\n    import statistics\n    # Requires external library statistics\n    # Handle zero/negative numbers which are undefined for HM\n    if any(n <= 0 for n in numbers): return 0\n    return statistics.harmonic_mean(numbers)\n\ndef _check_if_less_than_arithmetic_mean(h_mean, data_points):\n    return h_mean < statistics.mean(data_points)\n\ndef analyze_mean_relationship(data_set):\n    import statistics\n    # Dependency 1: Calculate the harmonic mean\n    h_mean = _calculate_harmonic_mean(data_set)\n    \n    # Dependency 2: Check if the harmonic mean is less than the arithmetic mean (expected property)\n    is_less = _check_if_less_than_arithmetic_mean(h_mean, data_set)\n    \n    return {'harmonic_mean': h_mean, 'h_lt_arithmetic': is_less}", "summary": "Analyzes the mean relationship by first calculating the harmonic mean of the **`data_set`** using **`_calculate_harmonic_mean`** (defined in **`analyze_mean_relationship.py`**) (which relies on the **`statistics`** library) and then checking if the harmonic mean is less than the arithmetic mean (a fundamental property) using **`_check_if_less_than_arithmetic_mean`** (defined in **`analyze_mean_relationship.py`**)."}
{"code": "def _read_data_from_ftp_server(ftp_client, remote_path):\n    # Mock FTP download\n    return f\"FTP content for {remote_path}\"\n\ndef _count_lines_in_content(content_string):\n    return content_string.count('\\n') + 1\n\ndef fetch_and_count_ftp_lines(mock_ftp_client, file_path):\n    # Dependency 1: Fetch the file content from the mock FTP server\n    file_content = _read_data_from_ftp_server(mock_ftp_client, file_path)\n    \n    # Dependency 2: Count the number of lines in the retrieved content\n    line_count = _count_lines_in_content(file_content)\n    \n    return line_count", "summary": "Fetches data from a mock FTP server using **`_read_data_from_ftp_server`** (defined in **`fetch_and_count_ftp_lines.py`**) and then counts the number of lines in the retrieved content string using **`_count_lines_in_content`** (defined in **`fetch_and_count_ftp_lines.py`**)."}
{"code": "def _parse_url_query_parameters(url):\n    import urllib.parse\n    query = urllib.parse.urlparse(url).query\n    return urllib.parse.parse_qs(query)\n\ndef _extract_single_query_param(query_params, param_name, default=None):\n    # Returns the first value for the parameter, or default if not present\n    return query_params.get(param_name, [default])[0]\n\ndef get_url_single_query_parameter(full_url, target_param):\n    # Dependency 1: Parse the URL to extract all query parameters into a dictionary\n    query_params = _parse_url_query_parameters(full_url)\n    \n    # Dependency 2: Extract the value of a single, specified query parameter\n    param_value = _extract_single_query_param(query_params, target_param)\n    \n    return param_value", "summary": "Extracts a single query parameter from a URL by first parsing the **`full_url`** to extract all query parameters using **`_parse_url_query_parameters`** (defined in **`get_url_single_query_parameter.py`**) (which relies on the **`urllib.parse`** library) and then retrieving the value for the **`target_param`** using **`_extract_single_query_param`** (defined in **`get_url_single_query_parameter.py`**)."}
{"code": "def _get_process_child_count():\n    import psutil\n    process = psutil.Process()\n    return len(process.children(recursive=False))\n\ndef _check_if_process_is_parent(child_count, min_children=1):\n    return child_count >= min_children\n\ndef audit_process_child_status():\n    import psutil\n    # Dependency 1: Get the count of direct child processes\n    children_count = _get_process_child_count()\n    \n    # Dependency 2: Check if the process is acting as a parent (has at least one child)\n    is_parent = _check_if_process_is_parent(children_count)\n    \n    return {'child_count': children_count, 'is_parent': is_parent}", "summary": "Audits the process child status by first retrieving the count of direct (non-recursive) child processes using **`_get_process_child_count`** (defined in **`audit_process_child_status.py`**) (which relies on the external **`psutil`** library) and then checking if that count is greater than or equal to 1 using **`_check_if_process_is_parent`** (defined in **`audit_process_child_status.py`**)."}
{"code": "def _validate_if_string_is_json(text):\n    import json\n    try:\n        json.loads(text)\n        return True\n    except json.JSONDecodeError: return False\n\ndef _log_json_validation(text, is_valid):\n    status = 'Valid JSON' if is_valid else 'Invalid JSON'\n    print(f\"[JSON_VALIDATION] '{text[:15]}...' is {status}\")\n\ndef validate_and_log_json(text_candidate):\n    # Dependency 1: Check if the input text is structurally valid JSON\n    is_valid = _validate_if_string_is_json(text_candidate)\n    \n    # Dependency 2: Log the result of the JSON validation\n    _log_json_validation(text_candidate, is_valid)\n    \n    return is_valid", "summary": "Validates a **`text_candidate`** by checking if it can be successfully parsed as JSON using **`_validate_if_string_is_json`** (defined in **`validate_and_log_json.py`**) (which relies on the **`json`** library) and then logs the outcome of the validation using **`_log_json_validation`** (defined in **`validate_and_log_json.py`**)."}
{"code": "def _get_number_of_cpu_cores():\n    import os\n    return os.cpu_count() or 1\n\ndef _check_if_multi_core(core_count, min_cores=2):\n    return core_count >= min_cores\n\ndef audit_system_cpu_cores():\n    import os\n    # Dependency 1: Get the detected number of CPU cores\n    cores = _get_number_of_cpu_cores()\n    \n    # Dependency 2: Check if the system is multi-core (2 or more cores)\n    is_multi = _check_if_multi_core(cores)\n    \n    return {'core_count': cores, 'is_multi_core': is_multi}", "summary": "Audits the system's CPU configuration by retrieving the number of detected CPU cores using **`_get_number_of_cpu_cores`** (defined in **`audit_system_cpu_cores.py`**) (which relies on the **`os`** library) and then checking if that count is greater than or equal to 2 using **`_check_if_multi_core`** (defined in **`audit_system_cpu_cores.py`**)."}
{"code": "def _parse_semver_string(version_string):\n    import packaging.version\n    # Requires external library packaging\n    return packaging.version.parse(version_string)\n\ndef _check_if_major_version_matches(version_object, major_num=1):\n    return version_object.major == major_num\n\ndef verify_semver_major_version(version_text):\n    import packaging.version\n    # Dependency 1: Parse the semantic version string into a comparable object\n    version_obj = _parse_semver_string(version_text)\n    \n    # Dependency 2: Check if the major version number is 1\n    is_major_one = _check_if_major_version_matches(version_obj)\n    \n    return {'version_obj': version_obj, 'major_is_one': is_major_one}", "summary": "Verifies a semantic version by first parsing the **`version_text`** string into a comparable object using **`_parse_semver_string`** (defined in **`verify_semver_major_version.py`**) (which relies on the external **`packaging.version`** library) and then checking if the extracted major version number is equal to 1 using **`_check_if_major_version_matches`** (defined in **`verify_semver_major_version.py`**)."}
{"code": "def _get_local_timezone_name():\n    import datetime\n    import tzlocal\n    # Requires external library tzlocal\n    return tzlocal.get_localzone().zone\n\ndef _format_datetime_with_timezone(dt_obj, tz_name):\n    import pytz\n    # Requires external library pytz\n    local_tz = pytz.timezone(tz_name)\n    localized_dt = local_tz.localize(dt_obj)\n    return localized_dt.isoformat()\n\ndef get_current_time_with_local_tz(dt_now):\n    import datetime, tzlocal, pytz\n    # Dependency 1: Get the name of the local system timezone\n    local_tz_name = _get_local_timezone_name()\n    \n    # Dependency 2: Format the input datetime object using the local timezone\n    formatted_time = _format_datetime_with_timezone(dt_now, local_tz_name)\n    \n    return formatted_time", "summary": "Formats a datetime object with the local timezone by first retrieving the local system timezone name using **`_get_local_timezone_name`** (defined in **`get_current_time_with_local_tz.py`**) (which relies on the external **`tzlocal`** library) and then localizing and formatting the **`dt_now`** object using that timezone via **`_format_datetime_with_timezone`** (defined in **`get_current_time_with_local_tz.py`**) (which relies on the **`pytz`** library)."}
{"code": "def _read_data_from_s3_bucket(bucket_name, key):\n    # Mock S3 object retrieval\n    return b'S3 object content bytes'\n\ndef _decode_bytes_to_string(byte_data, encoding='utf-8'):\n    return byte_data.decode(encoding)\n\ndef fetch_s3_and_decode_content(s3_bucket, s3_key):\n    # Dependency 1: Fetch the content bytes from the mock S3 bucket\n    content_bytes = _read_data_from_s3_bucket(s3_bucket, s3_key)\n    \n    # Dependency 2: Decode the retrieved bytes into a string using UTF-8\n    content_string = _decode_bytes_to_string(content_bytes)\n    \n    return content_string", "summary": "Fetches data from a mock S3 bucket by retrieving the content bytes using **`_read_data_from_s3_bucket`** (defined in **`fetch_s3_and_decode_content.py`**) and then decoding those bytes into a string using UTF-8 encoding via **`_decode_bytes_to_string`** (defined in **`fetch_s3_and_decode_content.py`**)."}
{"code": "def _find_list_median(number_list):\n    import statistics\n    if not number_list: return None\n    return statistics.median(number_list)\n\ndef _check_if_median_is_integer(median_value):\n    return median_value == int(median_value)\n\ndef analyze_list_median_property(data_list):\n    import statistics\n    # Dependency 1: Find the median value of the list\n    median_val = _find_list_median(data_list)\n    \n    # Dependency 2: Check if the median value is an exact integer\n    is_int = _check_if_median_is_integer(median_val) if median_val is not None else False\n    \n    return {'median': median_val, 'is_integer': is_int}", "summary": "Analyzes the median property of a list by first calculating the median value using **`_find_list_median`** (defined in **`analyze_list_median_property.py`**) (which relies on the **`statistics`** library) and then checking if the calculated median value is an exact integer using **`_check_if_median_is_integer`** (defined in **`analyze_list_median_property.py`**)."}
{"code": "def _compute_sha512_hash(data):\n    import hashlib\n    return hashlib.sha512(data.encode('utf-8')).hexdigest()\n\ndef _check_hash_meets_security_criteria(hash_value, min_prefix='0000'):\n    # Mock check for leading zeros (Proof of Work)\n    return hash_value.startswith(min_prefix)\n\ndef audit_hash_proof_of_work(input_string):\n    # Dependency 1: Compute the SHA512 hash of the input string\n    data_hash = _compute_sha512_hash(input_string)\n    \n    # Dependency 2: Check if the hash meets a 'security' criteria (e.g., 4 leading zeros)\n    is_secure = _check_hash_meets_security_criteria(data_hash)\n    \n    return {'hash': data_hash, 'is_secure': is_secure}", "summary": "Audits a hash for 'Proof of Work' criteria by first computing the SHA512 hash of the **`input_string`** using **`_compute_sha512_hash`** (defined in **`audit_hash_proof_of_work.py`**) (which relies on the **`hashlib`** library) and then checking if the hash starts with a minimum prefix of leading zeros using **`_check_hash_meets_security_criteria`** (defined in **`audit_hash_proof_of_work.py`**)."}
{"code": "def _check_if_string_is_base64(text):\n    import base64\n    try:\n        # Check if length is multiple of 4 and successful decode\n        return len(text) % 4 == 0 and bool(base64.b64decode(text, validate=True))\n    except Exception: return False\n\ndef _decode_base64_to_string(b64_text):\n    import base64\n    return base64.b64decode(b64_text).decode('utf-8')\n\ndef validate_and_decode_base64(candidate_text):\n    # Dependency 1: Check if the string is valid Base64 encoding\n    is_valid_b64 = _check_if_string_is_base64(candidate_text)\n    \n    # Dependency 2: Decode the Base64 string if it is valid\n    decoded_content = _decode_base64_to_string(candidate_text) if is_valid_b64 else None\n    \n    return {'is_valid': is_valid_b64, 'decoded': decoded_content}", "summary": "Validates and decodes Base64 data by first checking if the **`candidate_text`** is valid Base64 encoding using **`_check_if_string_is_base64`** (defined in **`validate_and_decode_base64.py`**) (which relies on the **`base64`** library) and then decoding the valid text to a UTF-8 string using **`_decode_base64_to_string`** (defined in **`validate_and_decode_base64.py`**)."}
{"code": "def _create_list_of_perfect_squares(limit):\n    import math\n    return [n*n for n in range(1, int(math.sqrt(limit)) + 1)]\n\ndef _check_if_list_contains_number(number_list, target):\n    return target in number_list\n\ndef check_if_perfect_square_up_to_limit(target_num, max_search):\n    import math\n    # Dependency 1: Generate a list of perfect squares up to the maximum search limit\n    square_list = _create_list_of_perfect_squares(max_search)\n    \n    # Dependency 2: Check if the target number is present in the generated list\n    is_square = _check_if_list_contains_number(square_list, target_num)\n    \n    return is_square", "summary": "Checks if a **`target_num`** is a perfect square by first generating a list of all perfect squares up to a **`max_search`** limit using **`_create_list_of_perfect_squares`** (defined in **`check_if_perfect_square_up_to_limit.py`**) (which relies on the **`math`** library) and then checking for the presence of the **`target_num`** in that list using **`_check_if_list_contains_number`** (defined in **`check_if_perfect_square_up_to_limit.py`**)."}
{"code": "def _get_api_rate_limit_headers(response_dict):\n    # Mock header extraction\n    return {\n        'Limit': response_dict.get('X-RateLimit-Limit'),\n        'Remaining': response_dict.get('X-RateLimit-Remaining'),\n        'Reset': response_dict.get('X-RateLimit-Reset'),\n    }\n\ndef _check_if_limit_nearly_reached(rate_limit_info, threshold=10):\n    remaining = int(rate_limit_info.get('Remaining', 0))\n    return remaining < threshold\n\ndef audit_api_rate_limit(api_response_headers):\n    # Dependency 1: Extract relevant rate limit information from the headers\n    limit_info = _get_api_rate_limit_headers(api_response_headers)\n    \n    # Dependency 2: Check if the number of remaining requests is nearly depleted\n    is_low = _check_if_limit_nearly_reached(limit_info)\n    \n    return {'info': limit_info, 'limit_low': is_low}", "summary": "Audits the API rate limit status by first extracting the rate limit headers (Limit, Remaining, Reset) from the **`api_response_headers`** using **`_get_api_rate_limit_headers`** (defined in **`audit_api_rate_limit.py`**) and then checking if the number of remaining requests is below a low threshold (default 10) using **`_check_if_limit_nearly_reached`** (defined in **`audit_api_rate_limit.py`**)."}
{"code": "def _generate_uuid4_string():\n    import uuid\n    return str(uuid.uuid4())\n\ndef _prefix_uuid_with_resource_type(uuid_string, resource_name='res'):\n    return f'{resource_name}_{uuid_string.replace(\"-\", \"\")}'\n\ndef create_resource_id(resource_type):\n    # Dependency 1: Generate a standard UUID version 4 string\n    guid = _generate_uuid4_string()\n    \n    # Dependency 2: Prefix the UUID (removing hyphens) with the resource type\n    resource_id = _prefix_uuid_with_resource_type(guid, resource_type)\n    \n    return resource_id", "summary": "Creates a unique resource ID by first generating a UUID version 4 string using **`_generate_uuid4_string`** (defined in **`create_resource_id.py`**) (which relies on the **`uuid`** library) and then prefixing the hyphen-less UUID string with the specified **`resource_type`** using **`_prefix_uuid_with_resource_type`** (defined in **`create_resource_id.py`**)."}
{"code": "def _compute_exponential_moving_average(data, alpha):\n    import pandas as pd\n    # Requires external library pandas\n    series = pd.Series(data)\n    return series.ewm(alpha=alpha, adjust=False).mean().tolist()\n\ndef _compare_last_values(ema_values, data_points):\n    # Check if EMA is lagging behind the last raw data point\n    return ema_values[-1] < data_points[-1]\n\ndef analyze_ema_lag(data_series, smoothing_factor):\n    import pandas as pd\n    # Dependency 1: Compute the Exponential Moving Average (EMA) of the data series\n    ema_series = _compute_exponential_moving_average(data_series, smoothing_factor)\n    \n    # Dependency 2: Check if the last EMA value is lagging behind the last raw data point\n    is_lagging = _compare_last_values(ema_series, data_series)\n    \n    return {'ema_values': ema_series, 'is_lagging': is_lagging}", "summary": "Analyzes EMA lag by first computing the Exponential Moving Average (EMA) of the **`data_series`** using a **`smoothing_factor`** via **`_compute_exponential_moving_average`** (defined in **`analyze_ema_lag.py`**) (which relies on the external **`pandas`** library) and then checking if the last EMA value is less than the last raw data point using **`_compare_last_values`** (defined in **`analyze_ema_lag.py`**)."}
{"code": "def _get_process_open_ports():\n    import psutil\n    process = psutil.Process()\n    # returns a list of namedtuples with connections\n    return [conn.laddr.port for conn in process.connections(kind='inet') if conn.status == 'LISTEN']\n\ndef _check_if_port_is_open(port_list, target_port):\n    return target_port in port_list\n\ndef audit_process_listening_port(port_to_check):\n    import psutil\n    # Dependency 1: Get a list of all ports the current process is listening on\n    listening_ports = _get_process_open_ports()\n    \n    # Dependency 2: Check if the specific target port is in the list of open ports\n    is_listening = _check_if_port_is_open(listening_ports, port_to_check)\n    \n    return {'listening_ports': listening_ports, 'port_open': is_listening}", "summary": "Audits the process's listening ports by first retrieving a list of all currently open (listening) internet connections/ports using **`_get_process_open_ports`** (defined in **`audit_process_listening_port.py`**) (which relies on the external **`psutil`** library) and then checking if the **`port_to_check`** is present in that list using **`_check_if_port_is_open`** (defined in **`audit_process_listening_port.py`**)."}
{"code": "def _parse_xml_element_value(xml_string, tag_name):\n    import xml.etree.ElementTree as ET\n    root = ET.fromstring(xml_string)\n    element = root.find(tag_name)\n    return element.text if element is not None else None\n\ndef _log_extracted_value(tag, value):\n    print(f\"[XML_PARSE] Extracted value for <{tag}>: {value}\")\n\ndef extract_value_from_xml(xml_content, element_tag):\n    import xml.etree.ElementTree as ET\n    # Dependency 1: Parse the XML content and extract the text value of a specific tag\n    extracted_value = _parse_xml_element_value(xml_content, element_tag)\n    \n    # Dependency 2: Log the extracted value\n    _log_extracted_value(element_tag, extracted_value)\n    \n    return extracted_value", "summary": "Extracts a value from XML content by first parsing the **`xml_content`** string and retrieving the text value of the specified **`element_tag`** using **`_parse_xml_element_value`** (defined in **`extract_value_from_xml.py`**) (which relies on the **`xml.etree.ElementTree`** library) and then logging the extracted value using **`_log_extracted_value`** (defined in **`extract_value_from_xml.py`**)."}
{"code": "def _get_current_stack_trace_string():\n    import traceback\n    return traceback.format_stack()\n\ndef _filter_out_internal_frames(stack_frames, ignore_prefix='_'):\n    # Mock filtering: only show non-internal calls\n    return [frame for frame in stack_frames if not any(line.strip().startswith(f'File \"{ignore_prefix}') for line in frame.split('\\n'))]\n\ndef generate_clean_stack_trace():\n    import traceback\n    # Dependency 1: Get the full current stack trace as a list of strings\n    full_stack = _get_current_stack_trace_string()\n    \n    # Dependency 2: Filter out frames that start with internal/private names\n    clean_stack = _filter_out_internal_frames(full_stack)\n    \n    return clean_stack", "summary": "Generates a clean stack trace by first getting the full current stack trace (list of frame strings) using **`_get_current_stack_trace_string`** (defined in **`generate_clean_stack_trace.py`**) (which relies on the **`traceback`** library) and then filtering out frames that appear to be internal/private (starting with '\\_') using **`_filter_out_internal_frames`** (defined in **`generate_clean_stack_trace.py`**)."}
{"code": "def _compute_data_range(number_list):\n    if not number_list: return 0\n    return max(number_list) - min(number_list)\n\ndef _check_if_range_is_wide(data_range, mean, threshold_factor=4.0):\n    # Check if range is > 4 times the mean\n    import statistics\n    if not data_list: return False\n    return data_range > abs(statistics.mean(data_list)) * threshold_factor\n\ndef analyze_data_dispersion(data_list):\n    import statistics\n    # Dependency 1: Compute the range (Max - Min) of the data\n    data_range = _compute_data_range(data_list)\n    \n    # Dependency 2: Check if the range indicates wide dispersion relative to the mean\n    is_wide = _check_if_range_is_wide(data_range, statistics.mean(data_list))\n    \n    return {'data_range': data_range, 'is_wide_dispersion': is_wide}", "summary": "Analyzes data dispersion by first computing the range (Max - Min) of the **`data_list`** using **`_compute_data_range`** (defined in **`analyze_data_dispersion.py`**) and then checking if this range is 'wide' (greater than 4 times the mean) using **`_check_if_range_is_wide`** (defined in **`analyze_data_dispersion.py`**) (which relies on the **`statistics`** library for calculating the mean)."}
{"code": "def _generate_secure_random_bytes(length=32):\n    import os\n    return os.urandom(length)\n\ndef _convert_bytes_to_hex(byte_data):\n    return byte_data.hex()\n\ndef create_secure_random_hex_string(key_length_bytes):\n    import os\n    # Dependency 1: Generate cryptographically secure random bytes\n    random_bytes = _generate_secure_random_bytes(key_length_bytes)\n    \n    # Dependency 2: Convert the bytes into a hexadecimal string representation\n    hex_string = _convert_bytes_to_hex(random_bytes)\n    \n    return hex_string", "summary": "Creates a secure random hexadecimal string by first generating cryptographically secure random bytes of a specified **`key_length_bytes`** using **`_generate_secure_random_bytes`** (defined in **`create_secure_random_hex_string.py`**) (which relies on the **`os`** library's `urandom`) and then converting those bytes into a hexadecimal string using **`_convert_bytes_to_hex`** (defined in **`create_secure_random_hex_string.py`**)."}
{"code": "def _validate_if_version_is_deprecated(version_obj, deprecated_major=1):\n    return version_obj.major == deprecated_major\n\ndef _compare_semver_versions(version_a, version_b):\n    return version_a > version_b\n\ndef audit_version_deprecation(current_version_str, baseline_version_str):\n    import packaging.version\n    # Dependency 1: Parse and check if the current version is an old deprecated major version\n    current_version = packaging.version.parse(current_version_str)\n    is_deprecated = _validate_if_version_is_deprecated(current_version)\n    \n    # Dependency 2: Check if the current version is newer than the baseline version\n    baseline_version = packaging.version.parse(baseline_version_str)\n    is_newer = _compare_semver_versions(current_version, baseline_version)\n    \n    return {'is_deprecated': is_deprecated, 'is_newer_than_baseline': is_newer}", "summary": "Audits version deprecation by first checking if the parsed **`current_version_str`** is an old deprecated major version (default major=1) using **`_validate_if_version_is_deprecated`** (defined in **`audit_version_deprecation.py`**) and then comparing the **`current_version_str`** to a **`baseline_version_str`** to see if it is newer using **`_compare_semver_versions`** (defined in **`audit_version_deprecation.py`**). Both steps rely on parsing version strings via the **`packaging.version`** library."}
{"code": "def _get_process_io_counters():\n    import psutil\n    process = psutil.Process()\n    # Returns (read_count, write_count, read_bytes, write_bytes)\n    return process.io_counters()\n\ndef _check_if_io_is_intensive(io_counters, threshold_bytes=10000000):\n    # Check if total I/O bytes > threshold (e.g., 10MB)\n    total_bytes = io_counters.read_bytes + io_counters.write_bytes\n    return total_bytes > threshold_bytes\n\ndef audit_process_io_intensity():\n    import psutil\n    # Dependency 1: Get the I/O statistics counters for the process\n    io_stats = _get_process_io_counters()\n    \n    # Dependency 2: Check if the total I/O usage (read+write bytes) is intensive\n    is_intensive = _check_if_io_is_intensive(io_stats)\n    \n    return {'io_stats': io_stats, 'is_intensive': is_intensive}", "summary": "Audits process I/O intensity by first retrieving the I/O counters (read/write bytes/counts) using **`_get_process_io_counters`** (defined in **`audit_process_io_intensity.py`**) (which relies on the external **`psutil`** library) and then checking if the total I/O bytes exceed an intensive threshold (default 10MB) using **`_check_if_io_is_intensive`** (defined in **`audit_process_io_intensity.py`**)."}
{"code": "def _fetch_user_groups(username):\n    # Mock group retrieval\n    if username == 'admin': return ['admin', 'devops', 'users']\n    return ['users']\n\ndef _check_if_user_is_privileged(group_list, privileged_group='admin'):\n    return privileged_group in group_list\n\ndef authorize_user_privileged_access(user_name):\n    # Dependency 1: Fetch the list of groups the user belongs to\n    user_groups = _fetch_user_groups(user_name)\n    \n    # Dependency 2: Check if the user's groups contain the privileged group\n    is_authorized = _check_if_user_is_privileged(user_groups)\n    \n    return is_authorized", "summary": "Authorizes user privileged access by first fetching the list of groups the **`user_name`** belongs to using **`_fetch_user_groups`** (defined in **`authorize_user_privileged_access.py`**) and then checking if the 'admin' group is present in that list using **`_check_if_user_is_privileged`** (defined in **`authorize_user_privileged_access.py`**)."}
{"code": "def _read_data_from_ini_file(file_path):\n    import configparser\n    config = configparser.ConfigParser()\n    config.read(file_path)\n    return config\n\ndef _get_all_section_names(config_object):\n    return config_object.sections()\n\ndef inspect_ini_file_sections(ini_file_path):\n    # Dependency 1: Read and parse the INI file content\n    config_obj = _read_data_from_ini_file(ini_file_path)\n    \n    # Dependency 2: Get a list of all section names found in the configuration\n    section_names = _get_all_section_names(config_obj)\n    \n    return section_names", "summary": "Inspects an INI file by first reading and parsing its content into a configuration object using **`_read_data_from_ini_file`** (defined in **`inspect_ini_file_sections.py`**) (which relies on the **`configparser`** library) and then retrieving a list of all section names using **`_get_all_section_names`** (defined in **`inspect_ini_file_sections.py`**)."}
{"code": "def _compute_haversine_distance(lat1, lon1, lat2, lon2):\n    import geopy.distance\n    # Requires external library geopy\n    coords_1 = (lat1, lon1)\n    coords_2 = (lat2, lon2)\n    return geopy.distance.distance(coords_1, coords_2).km\n\ndef _check_if_points_are_nearby(distance_km, max_distance_km=10):\n    return distance_km <= max_distance_km\n\ndef analyze_geographic_proximity(lat_a, lon_a, lat_b, lon_b):\n    import geopy.distance\n    # Dependency 1: Compute the great-circle (Haversine) distance between the two points in kilometers\n    distance = _compute_haversine_distance(lat_a, lon_a, lat_b, lon_b)\n    \n    # Dependency 2: Check if the distance is less than or equal to 10km\n    is_nearby = _check_if_points_are_nearby(distance)\n    \n    return {'distance_km': distance, 'is_nearby': is_nearby}", "summary": "Analyzes geographic proximity by first computing the great-circle (Haversine) distance in kilometers between two sets of coordinates using **`_compute_haversine_distance`** (defined in **`analyze_geographic_proximity.py`**) (which relies on the external **`geopy.distance`** library) and then checking if the distance is below a threshold (default 10km) using **`_check_if_points_are_nearby`** (defined in **`analyze_geographic_proximity.py`**)."}
{"code": "def _parse_http_response_status(status_line):\n    # Example: 'HTTP/1.1 200 OK'\n    parts = status_line.split()\n    return int(parts[1]) if len(parts) > 1 else 0\n\ndef _check_if_status_is_success(status_code):\n    return 200 <= status_code < 300\n\ndef audit_http_response_status(raw_status_line):\n    # Dependency 1: Parse the raw HTTP status line to extract the integer status code\n    status_code = _parse_http_response_status(raw_status_line)\n    \n    # Dependency 2: Check if the status code falls within the 2xx success range\n    is_success = _check_if_status_is_success(status_code)\n    \n    return {'status_code': status_code, 'is_success': is_success}", "summary": "Audits an HTTP response status by first parsing the **`raw_status_line`** to extract the integer status code using **`_parse_http_response_status`** (defined in **`audit_http_response_status.py`**) and then checking if that status code falls within the 2xx success range using **`_check_if_status_is_success`** (defined in **`audit_http_response_status.py`**)."}
{"code": "def _read_data_from_xml_attribute(xml_string, element_tag, attribute_name):\n    import xml.etree.ElementTree as ET\n    root = ET.fromstring(xml_string)\n    element = root.find(element_tag)\n    return element.get(attribute_name) if element is not None else None\n\ndef _check_if_attribute_is_numeric(attribute_value):\n    return attribute_value.isdigit() if attribute_value else False\n\ndef extract_and_validate_xml_attribute(xml_content, tag, attr):\n    import xml.etree.ElementTree as ET\n    # Dependency 1: Extract the value of a specific attribute from an XML element\n    attr_value = _read_data_from_xml_attribute(xml_content, tag, attr)\n    \n    # Dependency 2: Check if the extracted attribute value consists only of digits\n    is_numeric = _check_if_attribute_is_numeric(attr_value)\n    \n    return {'attribute_value': attr_value, 'is_numeric': is_numeric}", "summary": "Extracts and validates an XML attribute by first reading the value of a specific **`attr`** from an **`element_tag`** in the **`xml_content`** using **`_read_data_from_xml_attribute`** (defined in **`extract_and_validate_xml_attribute.py`**) (which relies on the **`xml.etree.ElementTree`** library) and then checking if the extracted value is purely numeric using **`_check_if_attribute_is_numeric`** (defined in **`extract_and_validate_xml_attribute.py`**)."}
{"code": "def _read_data_from_sqs_queue(queue_url):\n    # Mock SQS API call\n    return [\n        {'id': 'msg1', 'body': '{\"action\": \"create\"}'},\n        {'id': 'msg2', 'body': '{\"action\": \"delete\"}'}\n    ]\n\ndef _filter_messages_by_action(message_list, required_action='create'):\n    import json\n    filtered = []\n    for msg in message_list:\n        try:\n            body = json.loads(msg['body'])\n            if body.get('action') == required_action:\n                filtered.append(msg)\n        except json.JSONDecodeError: continue\n    return filtered\n\ndef consume_and_filter_sqs_messages(queue_path, action_type):\n    # Dependency 1: Read a batch of messages from the mock SQS queue\n    raw_messages = _read_data_from_sqs_queue(queue_path)\n    \n    # Dependency 2: Filter the messages by parsing the JSON body and checking the 'action' field\n    filtered_messages = _filter_messages_by_action(raw_messages, action_type)\n    \n    return filtered_messages", "summary": "Consumes and filters SQS messages by first reading a batch of mock messages from the **`queue_path`** using **`_read_data_from_sqs_queue`** (defined in **`consume_and_filter_sqs_messages.py`**) and then filtering that list by parsing the JSON body of each message and checking for a specific **`action_type`** using **`_filter_messages_by_action`** (defined in **`consume_and_filter_sqs_messages.py`**) (which relies on the **`json`** library)."}
{"code": "def _compute_set_difference(set_a, set_b):\n    return list(set_a.difference(set_b))\n\ndef _log_difference_count(list_name, count):\n    print(f\"[SET_ANALYSIS] Unique items in {list_name}: {count}\")\n\ndef analyze_asymmetric_set_difference(list1, list2):\n    # Convert lists to sets\n    set1, set2 = set(list1), set(list2)\n    \n    # Dependency 1: Compute the items present in set1 but not in set2\n    only_in_1 = _compute_set_difference(set1, set2)\n    \n    # Dependency 2: Log the count of unique items found\n    _log_difference_count('List 1', len(only_in_1))\n    \n    return only_in_1", "summary": "Analyzes the asymmetric set difference by first converting two lists into sets and then computing the items present in the first set but not the second using **`_compute_set_difference`** (defined in **`analyze_asymmetric_set_difference.py`**) and finally logging the count of unique items found using **`_log_difference_count`** (defined in **`analyze_asymmetric_set_difference.py`**)."}
{"code": "def _get_process_cpu_time_percent():\n    import psutil\n    process = psutil.Process()\n    # returns (user, system) time in seconds since start\n    times = process.cpu_times()\n    # Mock calculation of total time spent in user/system mode\n    return times.user + times.system\n\ndef _convert_seconds_to_minutes(seconds):\n    return seconds / 60\n\ndef calculate_total_cpu_minutes():\n    import psutil\n    # Dependency 1: Get the total CPU time (user + system) consumed by the process in seconds\n    total_cpu_seconds = _get_process_cpu_time_percent()\n    \n    # Dependency 2: Convert the total seconds into minutes\n    total_cpu_minutes = _convert_seconds_to_minutes(total_cpu_seconds)\n    \n    return total_cpu_minutes", "summary": "Calculates the total CPU minutes consumed by the current process by first retrieving the total CPU time (user + system) in seconds using **`_get_process_cpu_time_percent`** (defined in **`calculate_total_cpu_minutes.py`**) (which relies on the external **`psutil`** library) and then converting that time from seconds to minutes using **`_convert_seconds_to_minutes`** (defined in **`calculate_total_cpu_minutes.py`**)."}
{"code": "def _convert_timestamp_to_utc_dt(timestamp):\n    import datetime\n    # Assuming timestamp is in seconds\n    return datetime.datetime.fromtimestamp(timestamp, tz=datetime.timezone.utc)\n\ndef _format_dt_to_iso8601(dt_object):\n    return dt_object.isoformat()\n\ndef format_epoch_to_utc_iso(epoch_seconds):\n    import datetime\n    # Dependency 1: Convert the epoch seconds to a UTC datetime object\n    utc_dt = _convert_timestamp_to_utc_dt(epoch_seconds)\n    \n    # Dependency 2: Format the UTC datetime object into the ISO 8601 string format\n    iso_string = _format_dt_to_iso8601(utc_dt)\n    \n    return iso_string", "summary": "Formats an epoch timestamp (in seconds) by first converting the **`epoch_seconds`** into a UTC datetime object using **`_convert_timestamp_to_utc_dt`** (defined in **`format_epoch_to_utc_iso.py`**) (which relies on the **`datetime`** library) and then formatting that UTC datetime object into an ISO 8601 string using **`_format_dt_to_iso8601`** (defined in **`format_epoch_to_utc_iso.py`**)."}
{"code": "def _get_api_token_from_env(env_var_name='MY_API_KEY'):\n    import os\n    return os.environ.get(env_var_name)\n\ndef _log_if_token_is_missing(token_value, token_name):\n    if token_value is None: print(f\"[ENV_CHECK] WARNING: Required token '{token_name}' is missing.\")\n\ndef load_and_verify_api_token(token_env_name):\n    # Dependency 1: Retrieve the API token value from the environment variables\n    api_token = _get_api_token_from_env(token_env_name)\n    \n    # Dependency 2: Log a warning if the required token is not found\n    _log_if_token_is_missing(api_token, token_env_name)\n    \n    return api_token", "summary": "Loads and verifies an API token by first retrieving its value from the environment variables using **`_get_api_token_from_env`** (defined in **`load_and_verify_api_token.py`**) (which relies on the **`os`** library) and then logging a warning if the token is found to be missing using **`_log_if_token_is_missing`** (defined in **`load_and_verify_api_token.py`**)."}
{"code": "def _read_data_from_queue_with_timeout(queue_client, timeout_seconds=10):\n    # Mock queue read operation\n    return 'message from queue' if timeout_seconds > 5 else None\n\ndef _process_queue_message(message_content):\n    # Mock processing logic\n    return f\"PROCESSED: {message_content}\"\n\ndef try_consume_and_process_queue(client, read_timeout):\n    # Dependency 1: Attempt to read a message from the queue with a timeout\n    message = _read_data_from_queue_with_timeout(client, read_timeout)\n    \n    # Dependency 2: If a message was received, process it\n    processed_result = _process_queue_message(message) if message else 'No message received'\n    \n    return processed_result", "summary": "Attempts to consume and process a queue message by first reading a message from a mock queue with a specified **`read_timeout`** using **`_read_data_from_queue_with_timeout`** (defined in **`try_consume_and_process_queue.py`**) and then, if a message is received, processing it using **`_process_queue_message`** (defined in **`try_consume_and_process_queue.py`**)."}
{"code": "def _get_file_permission_string(file_path):\n    import os\n    import stat\n    mode = os.stat(file_path).st_mode\n    # Example: 'rwxr-xr-x'\n    return stat.filemode(mode)\n\ndef _check_if_globally_writable(permission_string):\n    # Check for 'w' in the final three characters (other permissions)\n    return 'w' in permission_string[-3:]\n\ndef audit_file_global_write_permission(target_file):\n    import os, stat\n    # Dependency 1: Get the standard Unix file permission string\n    permissions = _get_file_permission_string(target_file)\n    \n    # Dependency 2: Check if the file is globally (for 'other') writable\n    is_writable = _check_if_globally_writable(permissions)\n    \n    return {'permissions': permissions, 'is_globally_writable': is_writable}", "summary": "Audits file permissions by first retrieving the standard Unix permission string for the **`target_file`** using **`_get_file_permission_string`** (defined in **`audit_file_global_write_permission.py`**) (which relies on the **`os`** and **`stat`** libraries) and then checking if the 'other' category has write permission using **`_check_if_globally_writable`** (defined in **`audit_file_global_write_permission.py`**)."}
{"code": "def _compute_cosine_of_angle(angle_radians):\n    import math\n    return math.cos(angle_radians)\n\ndef _check_if_angle_is_obtuse(cosine_value):\n    # An angle is obtuse if its cosine is negative\n    return cosine_value < 0\n\ndef analyze_angle_type(angle_in_radians):\n    import math\n    # Dependency 1: Compute the cosine of the angle\n    cosine = _compute_cosine_of_angle(angle_in_radians)\n    \n    # Dependency 2: Determine if the angle is obtuse (cosine < 0)\n    is_obtuse = _check_if_angle_is_obtuse(cosine)\n    \n    return {'cosine': cosine, 'is_obtuse': is_obtuse}", "summary": "Analyzes an angle by first computing the cosine of the **`angle_in_radians`** using **`_compute_cosine_of_angle`** (defined in **`analyze_angle_type.py`**) (which relies on the **`math`** library) and then determining if the angle is obtuse (i.e., its cosine is negative) using **`_check_if_angle_is_obtuse`** (defined in **`analyze_angle_type.py`**)."}
{"code": "def _get_dns_a_record(hostname):\n    import socket\n    try:\n        return socket.gethostbyname(hostname)\n    except socket.gaierror: return None\n\ndef _check_if_ip_is_local(ip_address):\n    return ip_address.startswith('127.0.0.') or ip_address.startswith('192.168.')\n\ndef resolve_and_verify_ip(target_hostname):\n    import socket\n    # Dependency 1: Resolve the hostname to an IPv4 (A) record\n    resolved_ip = _get_dns_a_record(target_hostname)\n    \n    # Dependency 2: Check if the resolved IP address is a local/private address\n    is_local = _check_if_ip_is_local(resolved_ip) if resolved_ip else False\n    \n    return {'ip_address': resolved_ip, 'is_local_ip': is_local}", "summary": "Resolves and verifies an IP address by first performing a mock DNS lookup to get the IPv4 A record for the **`target_hostname`** using **`_get_dns_a_record`** (defined in **`resolve_and_verify_ip.py`**) (which relies on the **`socket`** library) and then checking if the resolved IP address falls within a local/private range using **`_check_if_ip_is_local`** (defined in **`resolve_and_verify_ip.py`**)."}
{"code": "def _normalize_path_separators(path_string):\n    import os\n    return path_string.replace('/', os.sep).replace('\\\\', os.sep)\n\ndef _get_base_directory_name(path_string):\n    import os\n    return os.path.dirname(path_string)\n\ndef extract_normalized_base_path(raw_path):\n    import os\n    # Dependency 1: Normalize the path separators to the current OS standard\n    normalized_path = _normalize_path_separators(raw_path)\n    \n    # Dependency 2: Extract the base directory component of the normalized path\n    base_dir = _get_base_directory_name(normalized_path)\n    \n    return base_dir", "summary": "Extracts the normalized base path by first normalizing the **`raw_path`** separators (e.g., '/' to '\\\\' on Windows) to the current OS standard using **`_normalize_path_separators`** (defined in **`extract_normalized_base_path.py`**) (which relies on the **`os`** library) and then extracting the base directory using **`_get_base_directory_name`** (defined in **`extract_normalized_base_path.py`**)."}
{"code": "def _compute_poisson_probability(k, lambda_rate):\n    import scipy.stats\n    # Requires external library scipy\n    return scipy.stats.poisson.pmf(k, lambda_rate)\n\ndef _check_if_event_is_unlikely(probability, threshold=0.01):\n    return probability < threshold\n\ndef analyze_poisson_likelihood(events, rate):\n    import scipy.stats\n    # Dependency 1: Compute the probability of observing 'events' given the 'rate' (lambda)\n    probability = _compute_poisson_probability(events, rate)\n    \n    # Dependency 2: Check if the computed probability is very low (unlikely event)\n    is_unlikely = _check_if_event_is_unlikely(probability)\n    \n    return {'probability': probability, 'is_unlikely': is_unlikely}", "summary": "Analyzes the likelihood of an event by first computing the Poisson probability of observing a number of **`events`** given a **`rate`** ($\\lambda$) using **`_compute_poisson_probability`** (defined in **`analyze_poisson_likelihood.py`**) (which relies on the external **`scipy.stats`** library) and then checking if the probability is below an 'unlikely' threshold (default 0.01) using **`_check_if_event_is_unlikely`** (defined in **`analyze_poisson_likelihood.py`**)."}
{"code": "def _read_data_from_kafka_topic(topic_name, num_messages=1):\n    # Mock Kafka consumer\n    return [{'key': 'A', 'value': 'data 1'}, {'key': 'B', 'value': 'data 2'}]\n\ndef _extract_message_keys(message_list):\n    return [msg['key'] for msg in message_list]\n\ndef consume_and_extract_kafka_keys(target_topic):\n    # Dependency 1: Read a batch of mock messages from the Kafka topic\n    messages = _read_data_from_kafka_topic(target_topic)\n    \n    # Dependency 2: Extract the key field from each message in the batch\n    message_keys = _extract_message_keys(messages)\n    \n    return message_keys", "summary": "Consumes messages from a mock Kafka topic using **`_read_data_from_kafka_topic`** (defined in **`consume_and_extract_kafka_keys.py`**) and then extracts the 'key' field from each message in the received batch using **`_extract_message_keys`** (defined in **`consume_and_extract_kafka_keys.py`**)."}
{"code": "def _compute_list_percentile(number_list, percentile=75):\n    import numpy as np\n    # Requires external library numpy\n    return np.percentile(number_list, percentile)\n\ndef _check_if_value_is_outlier(value, percentile_75):\n    # Mock check: value is outlier if > 1.5 * 75th percentile\n    return value > (1.5 * percentile_75)\n\ndef analyze_outlier_check(data_points, target_value):\n    import numpy as np\n    # Dependency 1: Compute the 75th percentile (Q3) of the data points\n    p75 = _compute_list_percentile(data_points)\n    \n    # Dependency 2: Check if the target value is an outlier relative to the 75th percentile\n    is_outlier = _check_if_value_is_outlier(target_value, p75)\n    \n    return {'percentile_75': p75, 'is_outlier': is_outlier}", "summary": "Analyzes for an outlier by first computing the 75th percentile (Q3) of the **`data_points`** using **`_compute_list_percentile`** (defined in **`analyze_outlier_check.py`**) (which relies on the external **`numpy`** library) and then checking if the **`target_value`** exceeds a threshold based on that percentile using **`_check_if_value_is_outlier`** (defined in **`analyze_outlier_check.py`**)."}
{"code": "def _get_file_owner_username(file_path):\n    import os\n    import pwd\n    # Get UID from file stats\n    uid = os.stat(file_path).st_uid\n    # Convert UID to username\n    return pwd.getpwuid(uid).pw_name\n\ndef _check_if_owner_is_root(username):\n    return username == 'root'\n\ndef audit_file_ownership_status(target_file):\n    import os, pwd\n    # Dependency 1: Get the username of the file's owner\n    owner_name = _get_file_owner_username(target_file)\n    \n    # Dependency 2: Check if the file owner is the 'root' user\n    is_root_owned = _check_if_owner_is_root(owner_name)\n    \n    return {'owner': owner_name, 'is_root_owned': is_root_owned}", "summary": "Audits file ownership by first retrieving the username of the file's owner using **`_get_file_owner_username`** (defined in **`audit_file_ownership_status.py`**) (which relies on the **`os`** and **`pwd`** libraries) and then checking if that owner is the 'root' user using **`_check_if_owner_is_root`** (defined in **`audit_file_ownership_status.py`**)."}
{"code": "def _convert_float_to_decimal(float_value):\n    from decimal import Decimal, getcontext\n    getcontext().prec = 28 # Set high precision\n    return Decimal(str(float_value))\n\ndef _check_decimal_precision(decimal_obj, max_dp=10):\n    from decimal import Decimal\n    return len(decimal_obj.as_tuple().digits[decimal_obj.as_tuple().exponent:]) <= max_dp\n\ndef audit_float_precision(float_input):\n    from decimal import Decimal\n    # Dependency 1: Convert the standard float input to a high-precision Decimal object\n    decimal_val = _convert_float_to_decimal(float_input)\n    \n    # Dependency 2: Check if the number of decimal places exceeds a limit\n    is_precise = _check_decimal_precision(decimal_val)\n    \n    return {'decimal_value': decimal_val, 'is_high_precision': is_precise}", "summary": "Audits floating-point precision by first converting the **`float_input`** into a high-precision Decimal object using **`_convert_float_to_decimal`** (defined in **`audit_float_precision.py`**) (which relies on the **`decimal`** library) and then checking if the number of decimal places is within a limit (default 10) using **`_check_decimal_precision`** (defined in **`audit_float_precision.py`**)."}
{"code": "def _read_data_from_mongodb(db_client, collection_name, query):\n    # Mock MongoDB find operation\n    return [{'name': 'item A', 'status': 'active'}, {'name': 'item B', 'status': 'inactive'}]\n\ndef _count_matching_documents(document_list, status_field='status', target_status='active'):\n    return sum(1 for doc in document_list if doc.get(status_field) == target_status)\n\ndef count_active_mongo_documents(mongo_client, collection, filter_query):\n    # Dependency 1: Execute the mock MongoDB query and retrieve documents\n    documents = _read_data_from_mongodb(mongo_client, collection, filter_query)\n    \n    # Dependency 2: Count how many documents have the status 'active'\n    active_count = _count_matching_documents(documents)\n    \n    return active_count", "summary": "Counts active MongoDB documents by first executing a mock query to retrieve a list of documents using **`_read_data_from_mongodb`** (defined in **`count_active_mongo_documents.py`**) and then counting how many of those documents have the 'status' field set to 'active' using **`_count_matching_documents`** (defined in **`count_active_mongo_documents.py`**)."}
{"code": "def _check_if_date_is_in_past(date_obj):\n    import datetime\n    return date_obj < datetime.date.today()\n\ndef _format_date_for_display(date_obj):\n    return date_obj.strftime('%Y/%m/%d')\n\ndef audit_date_past_or_future(input_date):\n    import datetime\n    # Dependency 1: Check if the input date object is in the past compared to today\n    is_past = _check_if_date_is_in_past(input_date)\n    \n    # Dependency 2: Format the date object into a display string\n    display_date = _format_date_for_display(input_date)\n    \n    return {'is_past': is_past, 'formatted_date': display_date}", "summary": "Audits a date by first checking if the **`input_date`** object is in the past relative to the current date using **`_check_if_date_is_in_past`** (defined in **`audit_date_past_or_future.py`**) (which relies on the **`datetime`** library) and then formatting the date object into a 'YYYY/MM/DD' display string using **`_format_date_for_display`** (defined in **`audit_date_past_or_future.py`**)."}
{"code": "def _get_process_cmd_line():\n    import psutil\n    process = psutil.Process()\n    return process.cmdline()\n\ndef _check_if_cmd_contains_password(cmd_list):\n    # Simple check for common password flags\n    return any(('-p' in arg or '--password' in arg) for arg in cmd_list)\n\ndef audit_process_for_exposed_secrets():\n    import psutil\n    # Dependency 1: Get the full command line arguments used to start the process\n    cmd_args = _get_process_cmd_line()\n    \n    # Dependency 2: Check if any argument in the command line suggests an exposed password\n    is_exposed = _check_if_cmd_contains_password(cmd_args)\n    \n    return {'cmd_line': cmd_args, 'password_exposed': is_exposed}", "summary": "Audits a process for exposed secrets by first retrieving the full command line arguments used to launch the process using **`_get_process_cmd_line`** (defined in **`audit_process_for_exposed_secrets.py`**) (which relies on the external **`psutil`** library) and then checking if any argument contains common password flags using **`_check_if_cmd_contains_password`** (defined in **`audit_process_for_exposed_secrets.py`**)."}
{"code": "def _extract_all_emails_from_text(text):\n    import re\n    regex = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}'\n    return re.findall(regex, text)\n\ndef _check_if_emails_are_external(email_list, internal_domain='corp.com'):\n    return [e for e in email_list if not e.endswith(f'@{internal_domain}')]\n\ndef filter_external_emails(raw_text):\n    import re\n    # Dependency 1: Extract all email addresses found in the raw text\n    all_emails = _extract_all_emails_from_text(raw_text)\n    \n    # Dependency 2: Filter the list to include only emails from outside the internal domain\n    external_emails = _check_if_emails_are_external(all_emails)\n    \n    return external_emails", "summary": "Filters external emails from text by first extracting all email addresses using regex via **`_extract_all_emails_from_text`** (defined in **`filter_external_emails.py`**) (which relies on the **`re`** library) and then filtering the resulting list to keep only those emails that do *not* belong to the default internal domain ('corp.com') using **`_check_if_emails_are_external`** (defined in **`filter_external_emails.py`**)."}
{"code": "def _compute_geometric_series_sum(a, r, n):\n    if r == 1: return a * n\n    return a * (1 - r**n) / (1 - r)\n\ndef _check_if_series_converges(r, n):\n    # Converges if |r| < 1 AND n is large (conceptually infinite)\n    return abs(r) < 1\n\ndef analyze_geometric_series(first_term, ratio, num_terms):\n    # Dependency 1: Compute the sum of the finite geometric series\n    series_sum = _compute_geometric_series_sum(first_term, ratio, num_terms)\n    \n    # Dependency 2: Determine if the corresponding infinite series would converge\n    would_converge = _check_if_series_converges(ratio, num_terms)\n    \n    return {'sum': series_sum, 'converges': would_converge}", "summary": "Analyzes a geometric series by first computing the sum of the finite series using **`_compute_geometric_series_sum`** (defined in **`analyze_geometric_series.py`**) and then determining if the corresponding infinite series would converge (i.e., if the absolute value of the ratio $|r|<1$) using **`_check_if_series_converges`** (defined in **`analyze_geometric_series.py`**)."}
{"code": "def _get_system_uptime_seconds():\n    import time\n    import psutil\n    # Requires external library psutil\n    return time.time() - psutil.boot_time()\n\ndef _convert_seconds_to_days(seconds):\n    return seconds / (24 * 60 * 60)\n\ndef get_system_uptime_days():\n    import time, psutil\n    # Dependency 1: Calculate the total system uptime in seconds\n    uptime_seconds = _get_system_uptime_seconds()\n    \n    # Dependency 2: Convert the uptime from seconds into days\n    uptime_days = _convert_seconds_to_days(uptime_seconds)\n    \n    return uptime_days", "summary": "Retrieves the system uptime by first calculating the total uptime in seconds (current time - boot time) using **`_get_system_uptime_seconds`** (defined in **`get_system_uptime_days.py`**) (which relies on the **`time`** and external **`psutil`** libraries) and then converting that duration from seconds to days using **`_convert_seconds_to_days`** (defined in **`get_system_uptime_days.py`**)."}
{"code": "def _extract_all_phone_numbers(text):\n    import re\n    # Simple check for 10-digit numbers (xxx-xxx-xxxx)\n    regex = r'\\d{3}-\\d{3}-\\d{4}'\n    return re.findall(regex, text)\n\ndef _log_extracted_count(count):\n    print(f\"[CONTACTS] Found {count} potential phone numbers.\")\n\ndef find_and_log_phone_numbers(input_text):\n    import re\n    # Dependency 1: Extract all potential phone numbers using a pattern\n    phone_numbers = _extract_all_phone_numbers(input_text)\n    \n    # Dependency 2: Log the count of phone numbers found\n    _log_extracted_count(len(phone_numbers))\n    \n    return phone_numbers", "summary": "Finds and logs phone numbers by first extracting all potential 10-digit phone numbers (formatted xxx-xxx-xxxx) using regex via **`_extract_all_phone_numbers`** (defined in **`find_and_log_phone_numbers.py`**) (which relies on the **`re`** library) and then logging the total count of numbers found using **`_log_extracted_count`** (defined in **`find_and_log_phone_numbers.py`**)."}
{"code": "def _get_current_process_priority():\n    import psutil\n    process = psutil.Process()\n    # returns 'nice' value on Unix, 'priority class' on Windows\n    return process.nice()\n\ndef _check_if_priority_is_low(nice_value, low_threshold=10):\n    return nice_value >= low_threshold\n\ndef audit_process_scheduling_priority():\n    import psutil\n    # Dependency 1: Get the current process scheduling priority (nice value)\n    priority = _get_current_process_priority()\n    \n    # Dependency 2: Check if the priority is set to a low value (nice >= 10)\n    is_low_prio = _check_if_priority_is_low(priority)\n    \n    return {'priority': priority, 'is_low_priority': is_low_prio}", "summary": "Audits the process scheduling priority by first retrieving the current process 'nice' value (priority) using **`_get_current_process_priority`** (defined in **`audit_process_scheduling_priority.py`**) (which relies on the external **`psutil`** library) and then checking if the value meets a 'low priority' threshold (default 10) using **`_check_if_priority_is_low`** (defined in **`audit_process_scheduling_priority.py`**)."}
{"code": "def _read_data_from_json_array_file(file_path):\n    import json\n    with open(file_path, 'r') as f:\n        return json.load(f)\n\ndef _filter_array_by_key_presence(data_array, required_key):\n    return [item for item in data_array if required_key in item]\n\ndef load_and_filter_json_array(json_file, filter_key):\n    import json\n    # Dependency 1: Load the content of the file, assuming it's a JSON array\n    data_array = _read_data_from_json_array_file(json_file)\n    \n    # Dependency 2: Filter the array to keep only objects that contain the specified key\n    filtered_list = _filter_array_by_key_presence(data_array, filter_key)\n    \n    return filtered_list", "summary": "Loads and filters a JSON array by first reading the file content into a list of objects using **`_read_data_from_json_array_file`** (defined in **`load_and_filter_json_array.py`**) (which relies on the **`json`** library) and then filtering the resulting array to keep only the objects that contain the specified **`filter_key`** using **`_filter_array_by_key_presence`** (defined in **`load_and_filter_json_array.py`**)."}
{"code": "def _generate_random_integer(min_val, max_val):\n    import random\n    return random.randint(min_val, max_val)\n\ndef _check_if_number_is_within_range(number, low, high):\n    return low <= number <= high\n\ndef generate_and_validate_random_int(lower, upper):\n    import random\n    # Dependency 1: Generate a random integer within the specified range\n    random_num = _generate_random_integer(lower, upper)\n    \n    # Dependency 2: Verify that the generated number falls within the expected range\n    is_valid = _check_if_number_is_within_range(random_num, lower, upper)\n    \n    return {'random_number': random_num, 'is_valid': is_valid}", "summary": "Generates and validates a random integer by first creating a random number between **`lower`** and **`upper`** inclusive using **`_generate_random_integer`** (defined in **`generate_and_validate_random_int.py`**) (which relies on the **`random`** library) and then verifying that the generated number is actually within that specified range using **`_check_if_number_is_within_range`** (defined in **`generate_and_validate_random_int.py`**)."}
{"code": "def _read_data_from_prometheus_endpoint(metric_name):\n    # Mock Prometheus client\n    return {'up': 1, 'http_requests_total': 1500}\n\ndef _extract_metric_value(metrics_dict, key):\n    return metrics_dict.get(key)\n\ndef fetch_and_extract_prometheus_metric(target_metric):\n    # Dependency 1: Fetch mock data from a Prometheus endpoint\n    metric_data = _read_data_from_prometheus_endpoint(target_metric)\n    \n    # Dependency 2: Extract the value of the specific target metric\n    metric_value = _extract_metric_value(metric_data, target_metric)\n    \n    return metric_value", "summary": "Fetches and extracts a metric from a mock Prometheus endpoint by retrieving the full mock metric data using **`_read_data_from_prometheus_endpoint`** (defined in **`fetch_and_extract_prometheus_metric.py`**) and then extracting the specific value associated with the **`target_metric`** key using **`_extract_metric_value`** (defined in **`fetch_and_extract_prometheus_metric.py`**)."}
{"code": "def _compute_factorial_recursive(n):\n    if n == 0: return 1\n    return n * _compute_factorial_recursive(n-1)\n\ndef _check_if_number_is_power_of_two(n):\n    return n > 0 and (n & (n - 1) == 0)\n\ndef analyze_factorial_power_of_two(input_num):\n    # Dependency 1: Compute the factorial of the input number\n    factorial_result = _compute_factorial_recursive(input_num)\n    \n    # Dependency 2: Check if the resulting factorial is a power of two\n    is_power_of_two = _check_if_number_is_power_of_two(factorial_result)\n    \n    return {'factorial': factorial_result, 'is_power_of_two': is_power_of_two}", "summary": "Analyzes a number's factorial by first computing the factorial of the **`input_num`** recursively using **`_compute_factorial_recursive`** (defined in **`analyze_factorial_power_of_two.py`**) and then checking if the resulting factorial is a power of two using a bitwise operation via **`_check_if_number_is_power_of_two`** (defined in **`analyze_factorial_power_of_two.py`**)."}
{"code": "def _check_if_string_is_palindrome(text):\n    cleaned_text = ''.join(c.lower() for c in text if c.isalnum())\n    return cleaned_text == cleaned_text[::-1]\n\ndef _log_palindrome_status(text, is_palindrome):\n    status = 'IS Palindrome' if is_palindrome else 'NOT Palindrome'\n    print(f\"[STRING_CHECK] '{text[:15]}...' {status}\")\n\ndef validate_and_log_palindrome(test_string):\n    # Dependency 1: Check if the input string is a palindrome (ignoring case/punctuation)\n    is_pal = _check_if_string_is_palindrome(test_string)\n    \n    # Dependency 2: Log the result of the palindrome check\n    _log_palindrome_status(test_string, is_pal)\n    \n    return is_pal", "summary": "Validates and logs a palindrome check by first checking if the **`test_string`** is a palindrome (ignoring case and non-alphanumeric characters) using **`_check_if_string_is_palindrome`** (defined in **`validate_and_log_palindrome.py`**) and then logging the result of the check using **`_log_palindrome_status`** (defined in **`validate_and_log_palindrome.py`**)."}
{"code": "def _convert_json_to_query_string(data_dict):\n    import urllib.parse\n    # Returns a string like 'key1=value1&key2=value2'\n    return urllib.parse.urlencode(data_dict)\n\ndef _prefix_with_question_mark(query_string):\n    return '?' + query_string\n\ndef serialize_json_to_url_params(input_data):\n    import urllib.parse\n    # Dependency 1: Convert the dictionary data into a standard URL query string format\n    query_string = _convert_json_to_query_string(input_data)\n    \n    # Dependency 2: Prefix the query string with a '?' character\n    url_params = _prefix_with_question_mark(query_string)\n    \n    return url_params", "summary": "Serializes a JSON dictionary to URL query parameters by first converting the **`input_data`** dictionary into a standard URL-encoded query string (key=value&...) using **`_convert_json_to_query_string`** (defined in **`serialize_json_to_url_params.py`**) (which relies on the **`urllib.parse`** library) and then prepending a '?' character using **`_prefix_with_question_mark`** (defined in **`serialize_json_to_url_params.py`**)."}
{"code": "def _get_system_user_home_dir():\n    import os\n    return os.path.expanduser('~')\n\ndef _check_if_file_exists_in_home_dir(home_path, relative_file):\n    import os\n    full_path = os.path.join(home_path, relative_file)\n    return os.path.exists(full_path)\n\ndef audit_user_config_file(config_filename):\n    import os\n    # Dependency 1: Get the current user's home directory path\n    home_dir = _get_system_user_home_dir()\n    \n    # Dependency 2: Check if a specific configuration file exists within the home directory\n    file_exists = _check_if_file_exists_in_home_dir(home_dir, config_filename)\n    \n    return file_exists", "summary": "Audits a user configuration file by first retrieving the current user's home directory path using **`_get_system_user_home_dir`** (defined in **`audit_user_config_file.py`**) (which relies on the **`os`** library) and then checking if the specified **`config_filename`** exists within that directory using **`_check_if_file_exists_in_home_dir`** (defined in **`audit_user_config_file.py`**)."}
{"code": "def _compute_sample_variance(data_list):\n    import statistics\n    if len(data_list) < 2: return 0.0\n    return statistics.variance(data_list)\n\ndef _check_if_variance_is_high(variance_value, threshold=100):\n    return variance_value >= threshold\n\ndef analyze_data_variance(data_points):\n    import statistics\n    # Dependency 1: Compute the sample variance of the data points\n    s_variance = _compute_sample_variance(data_points)\n    \n    # Dependency 2: Check if the variance is considered 'high'\n    is_high = _check_if_variance_is_high(s_variance)\n    \n    return {'variance': s_variance, 'is_high_variance': is_high}", "summary": "Analyzes data variance by first computing the sample variance of the **`data_points`** using **`_compute_sample_variance`** (defined in **`analyze_data_variance.py`**) (which relies on the **`statistics`** library) and then checking if the resulting variance value exceeds a high threshold (default 100) using **`_check_if_variance_is_high`** (defined in **`analyze_data_variance.py`**)."}
{"code": "def _encode_url_component(text):\n    import urllib.parse\n    return urllib.parse.quote_plus(text)\n\ndef _log_encoding_comparison(original, encoded):\n    print(f\"[URL_ENCODE] Original: {original[:20]} | Encoded: {encoded[:20]}\")\n\ndef encode_and_log_url_component(raw_string):\n    import urllib.parse\n    # Dependency 1: URL-encode the input string (for use in query parameters)\n    encoded_string = _encode_url_component(raw_string)\n    \n    # Dependency 2: Log the original and encoded string for comparison\n    _log_encoding_comparison(raw_string, encoded_string)\n    \n    return encoded_string", "summary": "URL-encodes a string by first encoding the **`raw_string`** for use as a query component using **`_encode_url_component`** (defined in **`encode_and_log_url_component.py`**) (which relies on the **`urllib.parse`** library's `quote_plus`) and then logging the original and encoded versions using **`_log_encoding_comparison`** (defined in **`encode_and_log_url_component.py`**)."}
{"code": "def _find_max_value_in_list(number_list):\n    if not number_list: return None\n    return max(number_list)\n\ndef _get_index_of_first_occurrence(data_list, value):\n    try:\n        return data_list.index(value)\n    except ValueError: return -1\n\ndef analyze_list_maximum_and_index(data_points):\n    # Dependency 1: Find the maximum numerical value in the list\n    max_value = _find_max_value_in_list(data_points)\n    \n    # Dependency 2: Find the index of the first occurrence of the maximum value\n    max_index = _get_index_of_first_occurrence(data_points, max_value)\n    \n    return {'max_value': max_value, 'first_max_index': max_index}", "summary": "Analyzes a list by first finding the maximum value in the **`data_points`** using **`_find_max_value_in_list`** (defined in **`analyze_list_maximum_and_index.py`**) and then finding the index of the *first* occurrence of that maximum value using **`_get_index_of_first_occurrence`** (defined in **`analyze_list_maximum_and_index.py`**)."}
{"code": "def _get_process_threads_count():\n    import psutil\n    process = psutil.Process()\n    return process.num_threads()\n\ndef _check_if_thread_count_is_high(thread_count, max_threads=50):\n    return thread_count > max_threads\n\ndef audit_process_thread_count():\n    import psutil\n    # Dependency 1: Get the current number of threads used by the process\n    n_threads = _get_process_threads_count()\n    \n    # Dependency 2: Check if the number of threads exceeds a 'high' threshold\n    is_high = _check_if_thread_count_is_high(n_threads)\n    \n    return {'thread_count': n_threads, 'is_high_thread_count': is_high}", "summary": "Audits the process thread count by first retrieving the current number of threads used by the process using **`_get_process_threads_count`** (defined in **`audit_process_thread_count.py`**) (which relies on the external **`psutil`** library) and then checking if that count exceeds a 'high' threshold (default 50) using **`_check_if_thread_count_is_high`** (defined in **`audit_process_thread_count.py`**)."}
{"code": "def _read_data_from_git_config(config_key):\n    # Mock function to read git config\n    if config_key == 'user.email': return 'user@corp.com'\n    return None\n\ndef _check_if_email_is_corp(email_address, domain='corp.com'):\n    return email_address and email_address.endswith(f'@{domain}')\n\ndef verify_git_user_email(key_to_check):\n    # Dependency 1: Read the mock git configuration value (e.g., user.email)\n    config_value = _read_data_from_git_config(key_to_check)\n    \n    # Dependency 2: Check if the retrieved email address belongs to the corporate domain\n    is_corp_email = _check_if_email_is_corp(config_value)\n    \n    return {'email': config_value, 'is_corporate': is_corp_email}", "summary": "Verifies a Git user email by first retrieving the mock configuration value (default 'user.email') using **`_read_data_from_git_config`** (defined in **`verify_git_user_email.py`**) and then checking if the retrieved email address belongs to the default 'corp.com' domain using **`_check_if_email_is_corp`** (defined in **`verify_git_user_email.py`**)."}
{"code": "def _compute_shannon_entropy(data_string):\n    import math\n    import collections\n    counts = collections.Counter(data_string)\n    total = len(data_string)\n    if total == 0: return 0.0\n    \n    entropy = 0.0\n    for count in counts.values():\n        probability = count / total\n        entropy -= probability * math.log2(probability)\n    return entropy\n\ndef _check_if_string_is_random(entropy_value, min_entropy=4.0):\n    return entropy_value >= min_entropy\n\ndef analyze_string_randomness(input_text):\n    import math, collections\n    # Dependency 1: Compute the Shannon Entropy of the input string\n    entropy = _compute_shannon_entropy(input_text)\n    \n    # Dependency 2: Check if the entropy score suggests a highly random string\n    is_random = _check_if_string_is_random(entropy)\n    \n    return {'entropy': entropy, 'is_highly_random': is_random}", "summary": "Analyzes string randomness by first computing the Shannon Entropy of the **`input_text`** using **`_compute_shannon_entropy`** (defined in **`analyze_string_randomness.py`**) (which relies on the **`math`** and **`collections`** libraries) and then checking if the entropy score meets a threshold (default 4.0) indicating high randomness using **`_check_if_string_is_random`** (defined in **`analyze_string_randomness.py`**)."}
{"code": "def _parse_cookie_expiration(cookie_header):\n    import http.cookies\n    C = http.cookies.SimpleCookie(cookie_header)\n    return C.output().find('expires=') > -1\n\ndef _check_if_cookie_is_session_only(has_expiry):\n    # If it has no 'expires=' attribute, it's typically session-only\n    return not has_expiry\n\ndef audit_cookie_persistence(raw_set_cookie_header):\n    import http.cookies\n    # Dependency 1: Parse the header to determine if an 'expires=' attribute is present\n    has_expiration = _parse_cookie_expiration(raw_set_cookie_header)\n    \n    # Dependency 2: Determine if the absence of an expiry tag makes it session-only\n    is_session_only = _check_if_cookie_is_session_only(has_expiration)\n    \n    return {'has_expiry': has_expiration, 'is_session_only': is_session_only}", "summary": "Audits cookie persistence by first parsing the **`raw_set_cookie_header`** to detect the presence of an 'expires=' attribute using **`_parse_cookie_expiration`** (defined in **`audit_cookie_persistence.py`**) (which relies on the **`http.cookies`** library) and then checking if the cookie is session-only (i.e., lacks an explicit expiry date) using **`_check_if_cookie_is_session_only`** (defined in **`audit_cookie_persistence.py`**)."}
{"code": "def _compute_beta_function(a, b):\n    import scipy.special\n    # Requires external library scipy\n    return scipy.special.beta(a, b)\n\ndef _check_if_close_to_one(value, tolerance=1e-6):\n    return abs(value - 1.0) < tolerance\n\ndef analyze_beta_function_value(alpha, beta):\n    import scipy.special\n    # Dependency 1: Compute the Beta function value B(alpha, beta)\n    beta_val = _compute_beta_function(alpha, beta)\n    \n    # Dependency 2: Check if the computed value is numerically close to 1.0\n    is_close_to_one = _check_if_close_to_one(beta_val)\n    \n    return {'beta_value': beta_val, 'is_approx_one': is_close_to_one}", "summary": "Analyzes the Beta function by first computing the Beta function value $B(\\alpha, \\beta)$ using **`_compute_beta_function`** (defined in **`analyze_beta_function_value.py`**) (which relies on the external **`scipy.special`** library) and then checking if the computed value is numerically close to 1.0 using **`_check_if_close_to_one`** (defined in **`analyze_beta_function_value.py`**)."}
{"code": "def _fetch_all_process_pids():\n    import psutil\n    return psutil.pids()\n\ndef _log_process_count(pid_list):\n    print(f\"[OS_MONITOR] Current total processes: {len(pid_list)}\")\n\ndef monitor_total_process_count():\n    import psutil\n    # Dependency 1: Get a list of all current process IDs (PIDs)\n    all_pids = _fetch_all_process_pids()\n    \n    # Dependency 2: Log the total count of running processes\n    _log_process_count(all_pids)\n    \n    return len(all_pids)", "summary": "Monitors the total process count by first fetching a list of all currently running process IDs (PIDs) using **`_fetch_all_process_pids`** (defined in **`monitor_total_process_count.py`**) (which relies on the external **`psutil`** library) and then logging the total count of processes found using **`_log_process_count`** (defined in **`monitor_total_process_count.py`**)."}
{"code": "def _extract_list_from_dictionary(data_dict, list_key='items'):\n    return data_dict.get(list_key, [])\n\ndef _convert_to_set_and_count_unique(item_list):\n    return len(set(item_list))\n\ndef analyze_unique_items_in_dict_list(data_record, target_list_key):\n    # Dependency 1: Extract the list of items from the dictionary using the target key\n    item_list = _extract_list_from_dictionary(data_record, target_list_key)\n    \n    # Dependency 2: Convert the list to a set and count the number of unique items\n    unique_count = _convert_to_set_and_count_unique(item_list)\n    \n    return unique_count", "summary": "Analyzes unique items in a list within a dictionary by first extracting the list associated with the **`target_list_key`** from the **`data_record`** using **`_extract_list_from_dictionary`** (defined in **`analyze_unique_items_in_dict_list.py`**) and then counting the number of unique items in that list by converting it to a set using **`_convert_to_set_and_count_unique`** (defined in **`analyze_unique_items_in_dict_list.py`**)."}
{"code": "def _check_if_date_is_end_of_month(date_obj):\n    # Check if the next day is the 1st of the next month\n    import datetime\n    next_day = date_obj + datetime.timedelta(days=1)\n    return next_day.day == 1\n\ndef _get_date_weekday_name(date_obj):\n    # Returns full weekday name (Monday, Tuesday, etc.)\n    return date_obj.strftime('%A')\n\ndef analyze_date_properties(input_date):\n    import datetime\n    # Dependency 1: Check if the input date is the last day of its month\n    is_end_of_month = _check_if_date_is_end_of_month(input_date)\n    \n    # Dependency 2: Get the full name of the weekday for the input date\n    weekday_name = _get_date_weekday_name(input_date)\n    \n    return {'is_end_of_month': is_end_of_month, 'weekday_name': weekday_name}", "summary": "Analyzes date properties by first checking if the **`input_date`** is the end of its month using **`_check_if_date_is_end_of_month`** (defined in **`analyze_date_properties.py`**) (which relies on the **`datetime`** library) and then retrieving the full name of the weekday for that date using **`_get_date_weekday_name`** (defined in **`analyze_date_properties.py`**)."}
{"code": "def _send_metrics_to_datadog(metric_name, value):\n    # Mock Datadog API call\n    print(f\"[DD_METRIC] Sent {metric_name}: {value}\")\n\ndef _check_if_metric_is_zero(value):\n    return value == 0\n\ndef report_metric_and_check_zero(metric_id, metric_value):\n    # Dependency 1: Send the metric and value to the mock Datadog API\n    _send_metrics_to_datadog(metric_id, metric_value)\n    \n    # Dependency 2: Check if the reported metric value was zero\n    was_zero = _check_if_metric_is_zero(metric_value)\n    \n    return was_zero", "summary": "Reports a metric by first sending the **`metric_value`** to a mock Datadog endpoint using **`_send_metrics_to_datadog`** (defined in **`report_metric_and_check_zero.py`**) and then checking if the reported **`metric_value`** was equal to zero using **`_check_if_metric_is_zero`** (defined in **`report_metric_and_check_zero.py`**)."}
{"code": "def _extract_ipv6_address(text):\n    import re\n    # Simple regex for finding potential IPv6 addresses\n    regex = r'([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}'\n    return re.findall(regex, text)\n\ndef _log_ipv6_found(ip_list):\n    print(f\"[NETWORK] Found {len(ip_list)} potential IPv6 addresses.\")\n\ndef find_and_log_ipv6_addresses(input_text):\n    import re\n    # Dependency 1: Extract all potential full IPv6 addresses using a regex pattern\n    ipv6_list = _extract_ipv6_address(input_text)\n    \n    # Dependency 2: Log the count of IPv6 addresses found\n    _log_ipv6_found(ipv6_list)\n    \n    return ipv6_list", "summary": "Finds and logs IPv6 addresses by first extracting all potential full IPv6 addresses using regex via **`_extract_ipv6_address`** (defined in **`find_and_log_ipv6_addresses.py`**) (which relies on the **`re`** library) and then logging the total count of addresses found using **`_log_ipv6_found`** (defined in **`find_and_log_ipv6_addresses.py`**)."}
{"code": "def _get_api_response_time_ms(response_dict):\n    # Mock header extraction\n    return int(response_dict.get('X-Response-Time-Ms', 0))\n\ndef _check_if_latency_is_high(latency_ms, max_ms=500):\n    return latency_ms > max_ms\n\ndef audit_api_latency(api_response_headers):\n    # Dependency 1: Extract the response time in milliseconds from the mock headers\n    latency_ms = _get_api_response_time_ms(api_response_headers)\n    \n    # Dependency 2: Check if the extracted latency exceeds a high threshold\n    is_high_latency = _check_if_latency_is_high(latency_ms)\n    \n    return {'latency_ms': latency_ms, 'is_high_latency': is_high_latency}", "summary": "Audits API latency by first extracting the response time in milliseconds from mock HTTP response headers using **`_get_api_response_time_ms`** (defined in **`audit_api_latency.py`**) and then checking if the extracted latency exceeds a high threshold (default 500ms) using **`_check_if_latency_is_high`** (defined in **`audit_api_latency.py`**)."}
{"code": "def _compute_chi_squared_test(observed, expected):\n    import scipy.stats\n    # Requires external library scipy\n    return scipy.stats.chisquare(observed, f_exp=expected)\n\ndef _check_if_distribution_is_significantly_different(p_value, alpha=0.05):\n    return p_value < alpha\n\ndef analyze_chi_squared_difference(observed_counts, expected_counts):\n    import scipy.stats\n    # Dependency 1: Perform the Chi-Squared goodness-of-fit test\n    chi2_stat, p_value = _compute_chi_squared_test(observed_counts, expected_counts)\n    \n    # Dependency 2: Check if the p-value indicates a significant difference from expected\n    is_different = _check_if_distribution_is_significantly_different(p_value)\n    \n    return {'p_value': p_value, 'is_significantly_different': is_different}", "summary": "Analyzes the Chi-Squared difference by first performing the Chi-Squared goodness-of-fit test to get the p-value using **`_compute_chi_squared_test`** (defined in **`analyze_chi_squared_difference.py`**) (which relies on the external **`scipy.stats`** library) and then checking if the p-value is below the significance level ($\\alpha=0.05$) using **`_check_if_distribution_is_significantly_different`** (defined in **`analyze_chi_squared_difference.py`**)."}
{"code": "def _remove_specific_punctuation(text, punct_list=['.', ',', '!']):\n    return ''.join(c for c in text if c not in punct_list)\n\ndef _count_word_tokens(text):\n    return len(text.split())\n\ndef preprocess_and_count_tokens(raw_text):\n    # Dependency 1: Remove specific punctuation marks from the text\n    cleaned_text = _remove_specific_punctuation(raw_text)\n    \n    # Dependency 2: Count the number of word tokens in the cleaned text\n    token_count = _count_word_tokens(cleaned_text)\n    \n    return token_count", "summary": "Preprocesses text by first removing specific punctuation marks (default '.', ',', '!') using **`_remove_specific_punctuation`** (defined in **`preprocess_and_count_tokens.py`**) and then counting the number of word tokens remaining in the cleaned text using **`_count_word_tokens`** (defined in **`preprocess_and_count_tokens.py`**)."}
{"code": "def _parse_duration_string(duration_str):\n    # Mock parser for strings like '3h30m'\n    import re\n    match = re.match(r'(\\d+)h(\\d+)m', duration_str)\n    if match: return int(match.group(1)) * 3600 + int(match.group(2)) * 60\n    return 0\n\ndef _check_if_duration_is_long(total_seconds, max_seconds=7200):\n    return total_seconds > max_seconds\n\ndef analyze_duration_string(time_string):\n    import re\n    # Dependency 1: Parse the human-readable duration string into total seconds\n    duration_s = _parse_duration_string(time_string)\n    \n    # Dependency 2: Check if the total duration exceeds a 'long' threshold (e.g., 2 hours)\n    is_long = _check_if_duration_is_long(duration_s)\n    \n    return {'total_seconds': duration_s, 'is_long': is_long}", "summary": "Analyzes a duration string by first parsing the human-readable string (e.g., '3h30m') into the total number of seconds using **`_parse_duration_string`** (defined in **`analyze_duration_string.py`**) (which relies on the **`re`** library) and then checking if the total duration exceeds a 'long' threshold (default 2 hours) using **`_check_if_duration_is_long`** (defined in **`analyze_duration_string.py`**)."}
{"code": "def _compute_t_test_p_value(sample1, sample2):\n    import scipy.stats\n    # Requires external library scipy\n    stat, p = scipy.stats.ttest_ind(sample1, sample2)\n    return p\n\ndef _check_if_means_are_significantly_different(p_value, alpha=0.05):\n    return p_value < alpha\n\ndef analyze_two_sample_difference(data_group_a, data_group_b):\n    import scipy.stats\n    # Dependency 1: Perform the independent two-sample t-test and get the p-value\n    p_val = _compute_t_test_p_value(data_group_a, data_group_b)\n    \n    # Dependency 2: Check if the p-value indicates a significant difference between the means\n    is_different = _check_if_means_are_significantly_different(p_val)\n    \n    return {'p_value': p_val, 'is_significantly_different': is_different}", "summary": "Analyzes the difference between two sample means by first performing an independent two-sample t-test to obtain the p-value using **`_compute_t_test_p_value`** (defined in **`analyze_two_sample_difference.py`**) (which relies on the external **`scipy.stats`** library) and then checking if the p-value is below the significance level ($\\alpha=0.05$) using **`_check_if_means_are_significantly_different`** (defined in **`analyze_two_sample_difference.py`**)."}
{"code": "def _read_data_from_redis_hgetall(redis_conn, hash_key):\n    # Mock redis HGETALL operation\n    return {'field1': 'value A', 'field2': 'value B'}\n\ndef _convert_redis_hash_to_list(hash_dict):\n    return list(hash_dict.items())\n\ndef fetch_and_list_redis_hash(redis_client, h_key):\n    # Dependency 1: Retrieve all field-value pairs for the hash key from mock Redis\n    hash_data = _read_data_from_redis_hgetall(redis_client, h_key)\n    \n    # Dependency 2: Convert the resulting dictionary into a list of (field, value) tuples\n    data_list = _convert_redis_hash_to_list(hash_data)\n    \n    return data_list", "summary": "Fetches and lists a Redis hash by first retrieving all field-value pairs for the **`h_key`** from a mock Redis using **`_read_data_from_redis_hgetall`** (defined in **`fetch_and_list_redis_hash.py`**) and then converting the resulting dictionary into a list of (field, value) tuples using **`_convert_redis_hash_to_list`** (defined in **`fetch_and_list_redis_hash.py`**)."}
{"code": "def _check_if_string_is_valid_mac(mac_str):\n    import re\n    # Regex for standard MAC address formats (e.g., AA:BB:CC:DD:EE:FF or AA-BB-CC-DD-EE-FF)\n    regex = r'^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$'\n    return bool(re.match(regex, mac_str))\n\ndef _normalize_mac_to_colon_format(mac_str):\n    return mac_str.replace('-', ':').upper()\n\ndef validate_and_normalize_mac_address(mac_candidate):\n    import re\n    # Dependency 1: Check if the string matches a valid MAC address format\n    is_valid = _check_if_string_is_valid_mac(mac_candidate)\n    \n    # Dependency 2: If valid, normalize the MAC address to uppercase and colon format\n    normalized_mac = _normalize_mac_to_colon_format(mac_candidate) if is_valid else None\n    \n    return {'is_valid': is_valid, 'normalized': normalized_mac}", "summary": "Validates and normalizes a MAC address by first checking if the **`mac_candidate`** matches a valid format using **`_check_if_string_is_valid_mac`** (defined in **`validate_and_normalize_mac_address.py`**) (which relies on the **`re`** library) and then normalizing the valid address to uppercase and colon-separated format using **`_normalize_mac_to_colon_format`** (defined in **`validate_and_normalize_mac_address.py`**)."}
{"code": "def _get_process_memory_percent():\n    import psutil\n    process = psutil.Process()\n    return process.memory_percent()\n\ndef _check_if_memory_is_critical(mem_percent, critical_threshold=90.0):\n    return mem_percent >= critical_threshold\n\ndef audit_process_memory_consumption():\n    import psutil\n    # Dependency 1: Get the current process memory usage as a percentage of total RAM\n    mem_use_percent = _get_process_memory_percent()\n    \n    # Dependency 2: Check if the memory usage is at a critical level (e.g., >= 90%)\n    is_critical = _check_if_memory_is_critical(mem_use_percent)\n    \n    return {'memory_percent': mem_use_percent, 'is_critical': is_critical}", "summary": "Audits process memory consumption by first retrieving the current process memory usage as a percentage of total RAM using **`_get_process_memory_percent`** (defined in **`audit_process_memory_consumption.py`**) (which relies on the external **`psutil`** library) and then checking if that percentage meets a critical threshold (default 90.0%) using **`_check_if_memory_is_critical`** (defined in **`audit_process_memory_consumption.py`**)."}
{"code": "def _read_data_from_environment_and_split(env_key, delimiter=','):\n    import os\n    value = os.environ.get(env_key, '')\n    return [item.strip() for item in value.split(delimiter) if item.strip()]\n\ndef _check_if_list_is_empty(data_list):\n    return not bool(data_list)\n\ndef load_and_verify_env_list(env_name):\n    import os\n    # Dependency 1: Read the environment variable and split it into a list by comma\n    env_list = _read_data_from_environment_and_split(env_name)\n    \n    # Dependency 2: Check if the resulting list is empty\n    is_empty = _check_if_list_is_empty(env_list)\n    \n    return {'list_content': env_list, 'is_empty': is_empty}", "summary": "Loads and verifies an environment list by first retrieving the environment variable **`env_name`** and splitting its content by a delimiter (default comma) into a list using **`_read_data_from_environment_and_split`** (defined in **`load_and_verify_env_list.py`**) (which relies on the **`os`** library) and then checking if the resulting list is empty using **`_check_if_list_is_empty`** (defined in **`load_and_verify_env_list.py`**)."}
{"code": "def _compute_linear_regression(x_data, y_data):\n    import scipy.stats\n    # Requires external library scipy\n    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x_data, y_data)\n    return {'slope': slope, 'r_value': r_value, 'p_value': p_value}\n\ndef _check_if_correlation_is_strong(r_value, min_r=0.9):\n    return abs(r_value) >= min_r\n\ndef analyze_linear_correlation(x_points, y_points):\n    import scipy.stats\n    # Dependency 1: Perform linear regression and get the correlation coefficient (r-value)\n    results = _compute_linear_regression(x_points, y_points)\n    r_value = results['r_value']\n    \n    # Dependency 2: Check if the absolute correlation is strong (e.g., >= 0.9)\n    is_strong = _check_if_correlation_is_strong(r_value)\n    \n    return {'r_value': r_value, 'is_strong_correlation': is_strong}", "summary": "Analyzes linear correlation by first performing linear regression on the **`x_points`** and **`y_points`** to extract the correlation coefficient ($r$-value) using **`_compute_linear_regression`** (defined in **`analyze_linear_correlation.py`**) (which relies on the external **`scipy.stats`** library) and then checking if the absolute $r$-value indicates a strong correlation (default $|r| \\geq 0.9$) using **`_check_if_correlation_is_strong`** (defined in **`analyze_linear_correlation.py`**)."}
{"code": "def _fetch_all_running_containers():\n    # Mock Docker API call\n    return [\n        {'name': 'web_app', 'status': 'running', 'image': 'nginx'},\n        {'name': 'db_server', 'status': 'running', 'image': 'postgres'},\n        {'name': 'stopped_app', 'status': 'exited', 'image': 'old_app'},\n    ]\n\ndef _filter_containers_by_status(container_list, target_status='running'):\n    return [c for c in container_list if c.get('status') == target_status]\n\ndef list_running_containers():\n    # Dependency 1: Fetch the list of all mock containers and their status\n    all_containers = _fetch_all_running_containers()\n    \n    # Dependency 2: Filter the list to include only containers with 'running' status\n    running_containers = _filter_containers_by_status(all_containers)\n    \n    return running_containers", "summary": "Lists running containers by first fetching the full list of mock container records and their statuses using **`_fetch_all_running_containers`** (defined in **`list_running_containers.py`**) and then filtering that list to include only containers whose 'status' field is 'running' using **`_filter_containers_by_status`** (defined in **`list_running_containers.py`**)."}
{"code": "def _get_system_disk_usage(path):\n    import psutil\n    # Returns namedtuple with total, used, free, percent\n    return psutil.disk_usage(path)\n\ndef _check_if_disk_is_full(usage_info, percent_limit=90.0):\n    return usage_info.percent >= percent_limit\n\ndef audit_disk_space(disk_path):\n    import psutil\n    # Dependency 1: Get the disk usage statistics for the specified path\n    usage = _get_system_disk_usage(disk_path)\n    \n    # Dependency 2: Check if the disk usage percentage is critically high (e.g., >= 90%)\n    is_full = _check_if_disk_is_full(usage)\n    \n    return {'usage_percent': usage.percent, 'is_critically_full': is_full}", "summary": "Audits disk space by first retrieving the disk usage statistics (total, used, percent, etc.) for the **`disk_path`** using **`_get_system_disk_usage`** (defined in **`audit_disk_space.py`**) (which relies on the external **`psutil`** library) and then checking if the usage percentage meets a critical limit (default 90.0%) using **`_check_if_disk_is_full`** (defined in **`audit_disk_space.py`**)."}
{"code": "def _convert_xml_to_json_string(xml_string):\n    import xmltodict\n    # Requires external library xmltodict\n    data_dict = xmltodict.parse(xml_string)\n    import json\n    return json.dumps(data_dict)\n\ndef _log_conversion_status(original_type, target_type):\n    print(f\"[CONVERSION] Data successfully converted from {original_type} to {target_type}.\")\n\ndef convert_xml_to_json_and_log(xml_input):\n    import xmltodict, json\n    # Dependency 1: Convert the input XML string to a JSON string\n    json_output = _convert_xml_to_json_string(xml_input)\n    \n    # Dependency 2: Log the successful conversion\n    _log_conversion_status('XML', 'JSON')\n    \n    return json_output", "summary": "Converts XML to JSON by first converting the **`xml_input`** string into an equivalent JSON string using **`_convert_xml_to_json_string`** (defined in **`convert_xml_to_json_and_log.py`**) (which relies on the external **`xmltodict`** and **`json`** libraries) and then logging the successful conversion using **`_log_conversion_status`** (defined in **`convert_xml_to_json_and_log.py`**)."}
{"code": "def _get_list_unique_elements(input_list):\n    return sorted(list(set(input_list)))\n\ndef _count_elements_in_list(input_list, target_element):\n    return input_list.count(target_element)\n\ndef analyze_unique_count_in_list(data_list, target_item):\n    # Dependency 1: Get the list of unique elements present in the data\n    unique_elements = _get_list_unique_elements(data_list)\n    \n    # Dependency 2: Count the total occurrences of the specific target item\n    occurrence_count = _count_elements_in_list(data_list, target_item)\n    \n    return {'unique_elements': unique_elements, 'target_count': occurrence_count}", "summary": "Analyzes a list by first extracting a sorted list of unique elements using **`_get_list_unique_elements`** (defined in **`analyze_unique_count_in_list.py`**) and then counting the total number of occurrences of a specific **`target_item`** within the original list using **`_count_elements_in_list`** (defined in **`analyze_unique_count_in_list.py`**)."}
{"code": "def _compute_word_frequency(text):\n    import collections\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    return collections.Counter(words)\n\ndef _get_top_n_words(word_counts, n=5):\n    return word_counts.most_common(n)\n\ndef analyze_text_word_frequency(raw_text):\n    import collections, re\n    # Dependency 1: Compute the frequency count for every word in the text\n    word_counts = _compute_word_frequency(raw_text)\n    \n    # Dependency 2: Get the top 5 most frequently occurring words\n    top_words = _get_top_n_words(word_counts)\n    \n    return top_words", "summary": "Analyzes text word frequency by first computing the frequency count for every word in the **`raw_text`** using **`_compute_word_frequency`** (defined in **`analyze_text_word_frequency.py`**) (which relies on the **`collections`** and **`re`** libraries) and then retrieving the top 5 most common words using **`_get_top_n_words`** (defined in **`analyze_text_word_frequency.py`**)."}
{"code": "def _read_data_from_git_branch():\n    # Mock git operation\n    return 'feature/new-api-v2'\n\ndef _check_if_branch_is_safe(branch_name, safe_prefixes=['feature/', 'bugfix/', 'hotfix/']):\n    return any(branch_name.startswith(p) for p in safe_prefixes)\n\ndef audit_current_git_branch():\n    # Dependency 1: Get the name of the current mock git branch\n    current_branch = _read_data_from_git_branch()\n    \n    # Dependency 2: Check if the branch name starts with an allowed safe prefix\n    is_safe = _check_if_branch_is_safe(current_branch)\n    \n    return {'branch_name': current_branch, 'is_safe': is_safe}", "summary": "Audits the current Git branch by first retrieving the mock branch name using **`_read_data_from_git_branch`** (defined in **`audit_current_git_branch.py`**) and then checking if the branch name starts with one of the allowed 'safe' prefixes (e.g., 'feature/', 'bugfix/') using **`_check_if_branch_is_safe`** (defined in **`audit_current_git_branch.py`**)."}
{"code": "def _compute_md5_hash_of_string(text):\n    import hashlib\n    return hashlib.md5(text.encode('utf-8')).hexdigest()\n\ndef _compare_hashes(hash_a, hash_b):\n    return hash_a == hash_b\n\ndef compare_string_hashes(string1, string2):\n    import hashlib\n    # Dependency 1: Compute the MD5 hash of the first string\n    hash_a = _compute_md5_hash_of_string(string1)\n    \n    # Dependency 2: Compute the MD5 hash of the second string and compare them\n    hash_b = _compute_md5_hash_of_string(string2)\n    are_equal = _compare_hashes(hash_a, hash_b)\n    \n    return {'hashes_equal': are_equal, 'hash_a': hash_a}", "summary": "Compares two strings by first computing the MD5 hash of **`string1`** and **`string2`** using **`_compute_md5_hash_of_string`** (defined in **`compare_string_hashes.py`**) (which relies on the **`hashlib`** library) and then checking if the two resulting hash values are equal using **`_compare_hashes`** (defined in **`compare_string_hashes.py`**)."}
{"code": "def _extract_list_from_csv_column(file_path, column_name):\n    import csv\n    with open(file_path, 'r', newline='') as f:\n        reader = csv.DictReader(f)\n        return [row[column_name] for row in reader if column_name in row]\n\ndef _convert_to_integer_list(string_list):\n    try: return [int(s) for s in string_list]\n    except ValueError: return []\n\ndef load_and_cast_csv_column(csv_file, column):\n    import csv\n    # Dependency 1: Extract all values from the specified CSV column as strings\n    string_values = _extract_list_from_csv_column(csv_file, column)\n    \n    # Dependency 2: Attempt to cast the list of strings into a list of integers\n    integer_list = _convert_to_integer_list(string_values)\n    \n    return integer_list", "summary": "Loads and casts a CSV column by first extracting all values from the specified **`column`** as a list of strings using **`_extract_list_from_csv_column`** (defined in **`load_and_cast_csv_column.py`**) (which relies on the **`csv`** library) and then attempting to convert every string in that list into an integer using **`_convert_to_integer_list`** (defined in **`load_and_cast_csv_column.py`**)."}
{"code": "def _compute_least_squares_line(x_data, y_data):\n    import numpy as np\n    # Requires external library numpy\n    A = np.vstack([x_data, np.ones(len(x_data))]).T\n    m, c = np.linalg.lstsq(A, y_data, rcond=None)[0]\n    return m, c\n\ndef _predict_y_value(slope, intercept, x_value):\n    return slope * x_value + intercept\n\ndef predict_next_value_linear(x_series, y_series, next_x):\n    import numpy as np\n    # Dependency 1: Compute the slope (m) and intercept (c) of the least-squares line\n    slope, intercept = _compute_least_squares_line(x_series, y_series)\n    \n    # Dependency 2: Use the computed line to predict the y-value for a given next x\n    predicted_y = _predict_y_value(slope, intercept, next_x)\n    \n    return predicted_y", "summary": "Predicts the next value using linear regression by first computing the slope and intercept of the least-squares line fit to the **`x_series`** and **`y_series`** using **`_compute_least_squares_line`** (defined in **`predict_next_value_linear.py`**) (which relies on the external **`numpy`** library for `linalg.lstsq`) and then using those parameters to predict the $y$-value for **`next_x`** via **`_predict_y_value`** (defined in **`predict_next_value_linear.py`**)."}
{"code": "def _read_data_from_redis_list(redis_conn, list_key, start=0, end=-1):\n    # Mock redis LRANGE operation\n    return ['item 1', 'item 2', 'item 3']\n\ndef _log_list_length(item_list, key):\n    print(f\"[REDIS_LIST] List '{key}' length: {len(item_list)}\")\n\ndef fetch_and_log_redis_list(redis_client, l_key):\n    # Dependency 1: Retrieve all items from the mock Redis list\n    list_items = _read_data_from_redis_list(redis_client, l_key)\n    \n    # Dependency 2: Log the number of items retrieved from the list\n    _log_list_length(list_items, l_key)\n    \n    return list_items", "summary": "Fetches and logs a Redis list by first retrieving all items from the mock list associated with **`l_key`** using **`_read_data_from_redis_list`** (defined in **`fetch_and_log_redis_list.py`**) and then logging the total number of items retrieved using **`_log_list_length`** (defined in **`fetch_and_log_redis_list.py`**)."}
{"code": "def _check_if_list_contains_duplicates(input_list):\n    return len(input_list) != len(set(input_list))\n\ndef _log_duplicate_status(list_name, has_duplicates):\n    status = 'Duplicates Found' if has_duplicates else 'All Unique'\n    print(f\"[DATA_QUALITY] List '{list_name}': {status}\")\n\ndef audit_list_uniqueness(data_list, list_identifier):\n    # Dependency 1: Check if the list contains any duplicate elements\n    has_dupes = _check_if_list_contains_duplicates(data_list)\n    \n    # Dependency 2: Log the result of the uniqueness check\n    _log_duplicate_status(list_identifier, has_dupes)\n    \n    return has_dupes", "summary": "Audits list uniqueness by first checking if the **`data_list`** contains any duplicate elements by comparing its length to the length of its set conversion using **`_check_if_list_contains_duplicates`** (defined in **`audit_list_uniqueness.py`**) and then logging the result using **`_log_duplicate_status`** (defined in **`audit_list_uniqueness.py`**)."}
{"code": "def _get_api_response_body_size(response_dict):\n    # Mock calculation of body size in bytes\n    body = response_dict.get('body', '')\n    return len(body.encode('utf-8'))\n\ndef _check_if_response_is_large(size_bytes, max_size_kb=500):\n    return size_bytes > (max_size_kb * 1024)\n\ndef audit_api_response_size(mock_http_response):\n    # Dependency 1: Calculate the size of the response body in bytes\n    body_size = _get_api_response_body_size(mock_http_response)\n    \n    # Dependency 2: Check if the response body size exceeds a large threshold (e.g., 500KB)\n    is_large = _check_if_response_is_large(body_size)\n    \n    return {'size_bytes': body_size, 'is_large_response': is_large}", "summary": "Audits API response size by first calculating the size of the mock HTTP response body in bytes using **`_get_api_response_body_size`** (defined in **`audit_api_response_size.py`**) and then checking if this size exceeds a large threshold (default 500KB) using **`_check_if_response_is_large`** (defined in **`audit_api_response_size.py`**)."}
{"code": "def _extract_git_commit_hash(log_line):\n    # Example: 'commit 6e98797f1f31f94d1a580a133d264f3d2f9d8384'\n    return log_line.split()[1] if log_line.startswith('commit') else None\n\ndef _check_hash_meets_length(commit_hash, min_length=7):\n    return commit_hash and len(commit_hash) >= min_length\n\ndef audit_commit_hash_length(git_log_entry):\n    # Dependency 1: Extract the commit hash string from the log line\n    commit_hash = _extract_git_commit_hash(git_log_entry)\n    \n    # Dependency 2: Check if the extracted hash meets a minimum required length\n    is_valid = _check_hash_meets_length(commit_hash)\n    \n    return {'commit_hash': commit_hash, 'is_valid_length': is_valid}", "summary": "Audits Git commit hash length by first extracting the full commit hash string from a **`git_log_entry`** using **`_extract_git_commit_hash`** (defined in **`audit_commit_hash_length.py`**) and then checking if the hash meets a minimum required length (default 7 characters) using **`_check_hash_meets_length`** (defined in **`audit_commit_hash_length.py`**)."}
{"code": "def _get_number_of_process_threads():\n    import psutil\n    process = psutil.Process()\n    return process.num_threads()\n\ndef _check_if_threads_exceed_cores(thread_count, core_count):\n    return thread_count > core_count * 2\n\ndef audit_thread_core_ratio(num_cores):\n    import psutil\n    # Dependency 1: Get the current number of threads used by the process\n    thread_count = _get_number_of_process_threads()\n    \n    # Dependency 2: Check if the thread count significantly exceeds the core count (e.g., > 2x cores)\n    is_overloaded = _check_if_threads_exceed_cores(thread_count, num_cores)\n    \n    return {'thread_count': thread_count, 'is_core_overloaded': is_overloaded}", "summary": "Audits the thread-to-core ratio by first retrieving the current number of threads used by the process using **`_get_number_of_process_threads`** (defined in **`audit_thread_core_ratio.py`**) (which relies on the external **`psutil`** library) and then checking if that count significantly exceeds the provided **`num_cores`** (default $>2\times$ cores) using **`_check_if_threads_exceed_cores`** (defined in **`audit_thread_core_ratio.py`**)."}
{"code": "def _generate_uuid1_from_time():\n    import uuid\n    # UUID1 incorporates node ID (MAC address) and current time\n    return str(uuid.uuid1())\n\ndef _check_if_uuid_is_v1(uuid_string):\n    return uuid_string.split('-')[2].startswith('1')\n\ndef create_and_verify_time_based_uuid():\n    import uuid\n    # Dependency 1: Generate a time-based UUID version 1\n    uuid1_string = _generate_uuid1_from_time()\n    \n    # Dependency 2: Verify that the generated UUID is indeed version 1 (by checking the version nibble)\n    is_v1 = _check_if_uuid_is_v1(uuid1_string)\n    \n    return {'uuid': uuid1_string, 'is_version_1': is_v1}", "summary": "Creates a time-based UUID by first generating a UUID version 1 string using **`_generate_uuid1_from_time`** (defined in **`create_and_verify_time_based_uuid.py`**) (which relies on the **`uuid`** library) and then verifying that the UUID's version nibble indicates it is version 1 using **`_check_if_uuid_is_v1`** (defined in **`create_and_verify_time_based_uuid.py`**)."}
{"code": "def _compute_rsa_decryption(ciphertext, private_key):\n    # Mock decryption (just reverses the text)\n    return ciphertext[::-1]\n\ndef _base64_decode_string(b64_string):\n    import base64\n    return base64.b64decode(b64_string).decode('utf-8')\n\ndef decode_and_decrypt_data(b64_encrypted_data, decryption_key):\n    import base64\n    # Dependency 1: Base64 decode the input string to get the ciphertext bytes\n    ciphertext = _base64_decode_string(b64_encrypted_data)\n    \n    # Dependency 2: Decrypt the ciphertext using the mock RSA private key\n    plaintext = _compute_rsa_decryption(ciphertext, decryption_key)\n    \n    return plaintext", "summary": "Decrypts data by first Base64 decoding the **`b64_encrypted_data`** string to get the ciphertext using **`_base64_decode_string`** (defined in **`decode_and_decrypt_data.py`**) (which relies on the **`base64`** library) and then performing a mock RSA decryption of the ciphertext using **`_compute_rsa_decryption`** (defined in **`decode_and_decrypt_data.py`**)."}
{"code": "def _read_data_from_remote_api_with_auth(endpoint, token):\n    # Mock API call with token\n    return {'status': 200, 'user': 'data_user'}\n\ndef _check_if_api_call_is_successful(response_dict):\n    return response_dict.get('status') == 200\n\ndef execute_authenticated_api_call(api_url, auth_token):\n    # Dependency 1: Execute the mock remote API call with the authentication token\n    api_response = _read_data_from_remote_api_with_auth(api_url, auth_token)\n    \n    # Dependency 2: Check if the response status code indicates success (200)\n    is_successful = _check_if_api_call_is_successful(api_response)\n    \n    return is_successful", "summary": "Executes an authenticated API call by performing a mock remote API request, including the **`auth_token`**, using **`_read_data_from_remote_api_with_auth`** (defined in **`execute_authenticated_api_call.py`**) and then checking if the response status code is 200 (Success) using **`_check_if_api_call_is_successful`** (defined in **`execute_authenticated_api_call.py`**)."}
{"code": "def _get_list_chunks_exact(data_list, chunk_size):\n    # Only returns full chunks, ignores remainder\n    return [data_list[i:i + chunk_size] for i in range(0, len(data_list) - len(data_list) % chunk_size, chunk_size)]\n\ndef _count_full_chunks(data_list, chunk_size):\n    return len(data_list) // chunk_size\n\ndef analyze_exact_list_chunking(full_list, size):\n    # Dependency 1: Split the list into chunks of the specified size, ignoring the remainder\n    full_chunks = _get_list_chunks_exact(full_list, size)\n    \n    # Dependency 2: Calculate the exact number of full chunks produced\n    chunk_count = _count_full_chunks(full_list, size)\n    \n    return {'full_chunks': full_chunks, 'chunk_count': chunk_count}", "summary": "Analyzes exact list chunking by first splitting the **`full_list`** into chunks of the specified **`size`**, explicitly ignoring any remainder, using **`_get_list_chunks_exact`** (defined in **`analyze_exact_list_chunking.py`**) and then calculating the exact number of full chunks produced using **`_count_full_chunks`** (defined in **`analyze_exact_list_chunking.py`**)."}
{"code": "def _convert_float_to_percentage_string(float_val, precision=1):\n    return f\"{float_val * 100:.{precision}f}%\"\n\ndef _log_percentage_value(label, percent_string):\n    print(f\"[PERCENTAGE] {label}: {percent_string}\")\n\ndef format_and_log_float_as_percent(float_input, identifier):\n    # Dependency 1: Convert the float value (e.g., 0.5) to a formatted percentage string (e.g., '50.0%')\n    percent_str = _convert_float_to_percentage_string(float_input)\n    \n    # Dependency 2: Log the formatted percentage string\n    _log_percentage_value(identifier, percent_str)\n    \n    return percent_str", "summary": "Formats and logs a float as a percentage by first converting the **`float_input`** (assuming 0 to 1 range) into a formatted percentage string (e.g., '50.0%') using **`_convert_float_to_percentage_string`** (defined in **`format_and_log_float_as_percent.py`**) and then logging that formatted string using **`_log_percentage_value`** (defined in **`format_and_log_float_as_percent.py`**)."}
{"code": "def _check_if_all_elements_are_positive(number_list):\n    return all(n > 0 for n in number_list)\n\ndef _log_positivity_status(list_name, is_positive):\n    status = 'All Positive' if is_positive else 'Contains Zero/Negative'\n    print(f\"[DATA_CHECK] List '{list_name}': {status}\")\n\ndef audit_list_positivity(data_list, list_identifier):\n    # Dependency 1: Check if all elements in the list are strictly greater than zero\n    all_positive = _check_if_all_elements_are_positive(data_list)\n    \n    # Dependency 2: Log the result of the positivity check\n    _log_positivity_status(list_identifier, all_positive)\n    \n    return all_positive", "summary": "Audits list positivity by first checking if all elements in the **`data_list`** are strictly positive (greater than zero) using **`_check_if_all_elements_are_positive`** (defined in **`audit_list_positivity.py`**) and then logging the result of the check using **`_log_positivity_status`** (defined in **`audit_list_positivity.py`**)."}
{"code": "def _get_file_access_time(file_path):\n    import os\n    import datetime\n    timestamp = os.path.getatime(file_path)\n    return datetime.datetime.fromtimestamp(timestamp)\n\ndef _check_if_recently_accessed(access_time, days_limit=30):\n    import datetime\n    return (datetime.datetime.now() - access_time).days < days_limit\n\ndef audit_file_recent_access(target_file):\n    # Dependency 1: Get the last access datetime object for the file\n    access_dt = _get_file_access_time(target_file)\n    \n    # Dependency 2: Check if the file was accessed within the last 30 days\n    is_recent = _check_if_recently_accessed(access_dt)\n    \n    return {'access_time': access_dt, 'is_recent': is_recent}", "summary": "Audits file access by first retrieving its last access time as a datetime object using **`_get_file_access_time`** (defined in **`audit_file_recent_access.py`**) (which relies on the **`os`** and **`datetime`** libraries) and then checking if this time is within the last 30 days using **`_check_if_recently_accessed`** (defined in **`audit_file_recent_access.py`**)."}
{"code": "def _compute_list_range_excluding_outliers(data_list, percentile=10):\n    import numpy as np\n    # Requires external library numpy\n    p10 = np.percentile(data_list, percentile)\n    p90 = np.percentile(data_list, 100 - percentile)\n    return p90 - p10\n\ndef _check_if_trimmed_range_is_small(trimmed_range, max_range=5.0):\n    return trimmed_range <= max_range\n\ndef analyze_trimmed_range(data_points):\n    import numpy as np\n    # Dependency 1: Compute the range of data after trimming the bottom/top 10% (P90 - P10)\n    trim_range = _compute_list_range_excluding_outliers(data_points)\n    \n    # Dependency 2: Check if the trimmed range indicates low dispersion\n    is_small = _check_if_trimmed_range_is_small(trim_range)\n    \n    return {'trimmed_range': trim_range, 'is_low_dispersion': is_small}", "summary": "Analyzes data dispersion by first computing the range of the data points after excluding the bottom and top 10th percentiles ($P_{90} - P_{10}$) using **`_compute_list_range_excluding_outliers`** (defined in **`analyze_trimmed_range.py`**) (which relies on the external **`numpy`** library) and then checking if this trimmed range is small (default $\\leq 5.0$) using **`_check_if_trimmed_range_is_small`** (defined in **`analyze_trimmed_range.py`**)."}
{"code": "def _get_process_cpu_affinity():\n    import psutil\n    process = psutil.Process()\n    return process.cpu_affinity()\n\ndef _check_if_affinity_is_restricted(affinity_list, total_cores):\n    return len(affinity_list) < total_cores\n\ndef audit_process_cpu_affinity(system_core_count):\n    import psutil\n    # Dependency 1: Get the list of CPU cores the process is currently allowed to run on\n    affinity = _get_process_cpu_affinity()\n    \n    # Dependency 2: Check if the affinity list is smaller than the total core count (i.e., restricted)\n    is_restricted = _check_if_affinity_is_restricted(affinity, system_core_count)\n    \n    return {'affinity_list': affinity, 'is_restricted': is_restricted}", "summary": "Audits process CPU affinity by first retrieving the list of CPU cores the current process is allowed to run on using **`_get_process_cpu_affinity`** (defined in **`audit_process_cpu_affinity.py`**) (which relies on the external **`psutil`** library) and then checking if the size of that list is smaller than the **`system_core_count`** using **`_check_if_affinity_is_restricted`** (defined in **`audit_process_cpu_affinity.py`**)."}
{"code": "def _get_system_network_stats():\n    import psutil\n    # Returns namedtuple with bytes_sent, bytes_recv, etc.\n    return psutil.net_io_counters()\n\ndef _check_if_bytes_received_is_high(net_stats, threshold_gb=100):\n    return net_stats.bytes_recv > (threshold_gb * 1024 * 1024 * 1024)\n\ndef audit_network_receive_volume(max_gb_threshold):\n    import psutil\n    # Dependency 1: Get the total system network I/O statistics\n    net_stats = _get_system_network_stats()\n    \n    # Dependency 2: Check if the total bytes received exceeds a high threshold\n    is_high_volume = _check_if_bytes_received_is_high(net_stats, max_gb_threshold)\n    \n    return {'bytes_received': net_stats.bytes_recv, 'is_high_volume': is_high_volume}", "summary": "Audits network receive volume by first retrieving the total system network I/O statistics (bytes sent/received) using **`_get_system_network_stats`** (defined in **`audit_network_receive_volume.py`**) (which relies on the external **`psutil`** library) and then checking if the total bytes received exceeds a specified Gigabyte threshold using **`_check_if_bytes_received_is_high`** (defined in **`audit_network_receive_volume.py`**)."}
{"code": "def _read_data_from_redis_set(redis_conn, set_key):\n    # Mock redis SMEMBERS operation\n    return {'user1', 'user2', 'user3'}\n\ndef _check_if_set_contains_member(set_members, target_member):\n    return target_member in set_members\n\ndef verify_member_in_redis_set(redis_client, s_key, member):\n    # Dependency 1: Retrieve all members of the mock Redis set\n    set_members = _read_data_from_redis_set(redis_client, s_key)\n    \n    # Dependency 2: Check if the specific target member is present in the set\n    is_member = _check_if_set_contains_member(set_members, member)\n    \n    return is_member", "summary": "Verifies membership in a Redis set by first retrieving all members of the mock set associated with **`s_key`** using **`_read_data_from_redis_set`** (defined in **`verify_member_in_redis_set.py`**) and then checking if the **`member`** is present in the retrieved set using **`_check_if_set_contains_member`** (defined in **`verify_member_in_redis_set.py`**)."}
{"code": "def _convert_float_to_currency_string(float_val, currency='$'):\n    return f\"{currency}{float_val:,.2f}\"\n\ndef _log_currency_value(label, currency_string):\n    print(f\"[FINANCE] {label}: {currency_string}\")\n\ndef format_and_log_float_as_currency(float_input, identifier):\n    # Dependency 1: Convert the float value to a formatted currency string (e.g., '$1,234.56')\n    currency_str = _convert_float_to_currency_string(float_input)\n    \n    # Dependency 2: Log the formatted currency string\n    _log_currency_value(identifier, currency_str)\n    \n    return currency_str", "summary": "Formats and logs a float as currency by first converting the **`float_input`** into a formatted currency string (default '$' prefix, two decimal places, comma separators) using **`_convert_float_to_currency_string`** (defined in **`format_and_log_float_as_currency.py`**) and then logging that formatted string using **`_log_currency_value`** (defined in **`format_and_log_float_as_currency.py`**)."}
{"code": "def _extract_list_from_dictionary_deep(data_dict, key_path):\n    keys = key_path.split('.')\n    value = data_dict\n    for key in keys:\n        if not isinstance(value, dict) or key not in value: return []\n        value = value[key]\n    return value if isinstance(value, list) else []\n\ndef _check_if_list_has_minimum_length(input_list, min_len=5):\n    return len(input_list) >= min_len\n\ndef analyze_deep_list_length(data_record, path_to_list):\n    # Dependency 1: Extract a nested list from the dictionary using a dot path\n    nested_list = _extract_list_from_dictionary_deep(data_record, path_to_list)\n    \n    # Dependency 2: Check if the extracted list meets a minimum required length\n    is_sufficient = _check_if_list_has_minimum_length(nested_list)\n    \n    return {'list_length': len(nested_list), 'is_sufficient': is_sufficient}", "summary": "Analyzes deep list length by first extracting a potentially nested list from the **`data_record`** using a dot-separated **`path_to_list`** via **`_extract_list_from_dictionary_deep`** (defined in **`analyze_deep_list_length.py`**) and then checking if the extracted list meets a minimum required length (default 5) using **`_check_if_list_has_minimum_length`** (defined in **`analyze_deep_list_length.py`**)."}
{"code": "def _compute_gcd_of_list(number_list):\n    import math\n    if not number_list: return 0\n    result = number_list[0]\n    for i in range(1, len(number_list)): result = math.gcd(result, number_list[i])\n    return result\n\ndef _check_if_all_numbers_are_odd(number_list):\n    return all(n % 2 != 0 for n in number_list)\n\ndef analyze_list_gcd_and_parity(data_set):\n    import math\n    # Dependency 1: Compute the Greatest Common Divisor (GCD) of all numbers in the list\n    list_gcd = _compute_gcd_of_list(data_set)\n    \n    # Dependency 2: Check if all numbers in the list are odd\n    all_odd = _check_if_all_numbers_are_odd(data_set)\n    \n    return {'gcd': list_gcd, 'all_odd': all_odd}", "summary": "Analyzes a list of numbers by first computing the Greatest Common Divisor (GCD) of all numbers in the **`data_set`** using **`_compute_gcd_of_list`** (defined in **`analyze_list_gcd_and_parity.py`**) (which relies on the **`math`** library's `gcd`) and then checking if every number in the list is odd using **`_check_if_all_numbers_are_odd`** (defined in **`analyze_list_gcd_and_parity.py`**)."}
{"code": "def _check_if_file_is_executable(file_path):\n    import os\n    return os.access(file_path, os.X_OK)\n\ndef _log_executable_status(path, is_executable):\n    status = 'EXECUTABLE' if is_executable else 'NOT Executable'\n    print(f\"[SECURITY_AUDIT] {path}: {status}\")\n\ndef audit_file_executable_bit(target_file):\n    import os\n    # Dependency 1: Check if the file has the executable bit set for the current user/group/other\n    can_execute = _check_if_file_is_executable(target_file)\n    \n    # Dependency 2: Log the result of the executable check\n    _log_executable_status(target_file, can_execute)\n    \n    return can_execute", "summary": "Audits a file's executable bit by first checking if the **`target_file`** has the executable permission set using **`_check_if_file_is_executable`** (defined in **`audit_file_executable_bit.py`**) (which relies on the **`os`** library's `access`) and then logging the result of the check using **`_log_executable_status`** (defined in **`audit_file_executable_bit.py`**)."}
{"code": "def _compute_sha256_hmac(key, message):\n    import hmac\n    import hashlib\n    # Ensure key and message are bytes\n    key_bytes = key.encode('utf-8')\n    msg_bytes = message.encode('utf-8')\n    return hmac.new(key_bytes, msg_bytes, hashlib.sha256).hexdigest()\n\ndef _log_hmac_length(hmac_value):\n    print(f\"[AUTH] Generated HMAC-SHA256 of length {len(hmac_value)}\")\n\ndef generate_and_log_hmac_sha256(secret_key, data_payload):\n    import hmac, hashlib\n    # Dependency 1: Compute the HMAC-SHA256 of the data payload using the secret key\n    hmac_signature = _compute_sha256_hmac(secret_key, data_payload)\n    \n    # Dependency 2: Log the length of the generated HMAC signature\n    _log_hmac_length(hmac_signature)\n    \n    return hmac_signature", "summary": "Generates and logs an HMAC-SHA256 signature by first computing the HMAC-SHA256 of the **`data_payload`** using the **`secret_key`** via **`_compute_sha256_hmac`** (defined in **`generate_and_log_hmac_sha256.py`**) (which relies on the **`hmac`** and **`hashlib`** libraries) and then logging the length of the resulting signature using **`_log_hmac_length`** (defined in **`generate_and_log_hmac_sha256.py`**)."}
{"code": "def _get_process_memory_swap_usage():\n    import psutil\n    process = psutil.Process()\n    # returns swap usage in bytes\n    return process.memory_info().vms\n\ndef _check_if_swap_is_high(swap_bytes, max_bytes=536870912):\n    # Check if VMS > 512MB\n    return swap_bytes > max_bytes\n\ndef audit_process_swap_usage():\n    import psutil\n    # Dependency 1: Get the Virtual Memory Size (VMS) which includes swap usage\n    vms_bytes = _get_process_memory_swap_usage()\n    \n    # Dependency 2: Check if the swap usage is high (e.g., > 512MB)\n    is_high_swap = _check_if_swap_is_high(vms_bytes)\n    \n    return {'vms_bytes': vms_bytes, 'is_high_swap': is_high_swap}", "summary": "Audits process swap usage by first retrieving the Virtual Memory Size (VMS), which includes swap, in bytes using **`_get_process_memory_swap_usage`** (defined in **`audit_process_swap_usage.py`**) (which relies on the external **`psutil`** library) and then checking if the VMS exceeds a 'high swap' threshold (default 512MB) using **`_check_if_swap_is_high`** (defined in **`audit_process_swap_usage.py`**)."}
{"code": "def _compute_list_mode(number_list):\n    import statistics\n    import collections\n    # statistics.mode() may fail on multimodal data\n    counts = collections.Counter(number_list)\n    if not counts: return None\n    # Return the mode(s) with the highest count\n    max_count = max(counts.values())\n    return [k for k, v in counts.items() if v == max_count]\n\ndef _check_if_unimodal(mode_list):\n    return len(mode_list) == 1\n\ndef analyze_list_modality(data_set):\n    import collections\n    # Dependency 1: Compute the list of modes (most frequent elements)\n    modes = _compute_list_mode(data_set)\n    \n    # Dependency 2: Check if the list of modes contains only one element (unimodal)\n    is_unimodal = _check_if_unimodal(modes)\n    \n    return {'modes': modes, 'is_unimodal': is_unimodal}", "summary": "Analyzes list modality by first computing the list of all modes (most frequent elements) using **`_compute_list_mode`** (defined in **`analyze_list_modality.py`**) (which relies on the **`collections`** library) and then checking if the resulting list contains exactly one mode (i.e., is unimodal) using **`_check_if_unimodal`** (defined in **`analyze_list_modality.py`**)."}
{"code": "def _extract_all_integers(text):\n    import re\n    return [int(n) for n in re.findall(r'\\d+', text)]\n\ndef _log_integer_count(count):\n    print(f\"[PARSE] Found {count} distinct integers.\")\n\ndef find_and_log_integers(input_text):\n    import re\n    # Dependency 1: Extract all sequences of digits and convert them to integers\n    integer_list = _extract_all_integers(input_text)\n    \n    # Dependency 2: Log the total count of integers found\n    _log_integer_count(len(integer_list))\n    \n    return integer_list", "summary": "Finds and logs integers by first extracting all sequences of digits from the **`input_text`** and converting them into a list of integers using **`_extract_all_integers`** (defined in **`find_and_log_integers.py`**) (which relies on the **`re`** library) and then logging the total count of integers found using **`_log_integer_count`** (defined in **`find_and_log_integers.py`**)."}
{"code": "def _convert_timedelta_to_hms(timedelta_obj):\n    total_seconds = int(timedelta_obj.total_seconds())\n    hours = total_seconds // 3600\n    minutes = (total_seconds % 3600) // 60\n    seconds = total_seconds % 60\n    return f\"{hours:02d}h{minutes:02d}m{seconds:02d}s\"\n\ndef _compute_time_difference(dt1, dt2):\n    return abs(dt1 - dt2)\n\ndef format_datetime_difference(dt_start, dt_end):\n    # Dependency 1: Compute the absolute difference between the two datetime objects\n    time_delta = _compute_time_difference(dt_start, dt_end)\n    \n    # Dependency 2: Convert the timedelta object into a human-readable H:M:S string\n    hms_string = _convert_timedelta_to_hms(time_delta)\n    \n    return hms_string", "summary": "Formats a datetime difference by first computing the absolute timedelta between **`dt_start`** and **`dt_end`** using **`_compute_time_difference`** (defined in **`format_datetime_difference.py`**) and then converting the resulting timedelta object into a human-readable 'HHhMMmSSs' string using **`_convert_timedelta_to_hms`** (defined in **`format_datetime_difference.py`**)."}
{"code": "def _get_system_user_gid_list(username):\n    import grp\n    import os\n    # Mock function to get group IDs\n    if username == 'admin': return [0, 10, 1000]\n    return [1000]\n\ndef _check_if_user_has_root_group(gid_list, root_gid=0):\n    return root_gid in gid_list\n\ndef audit_user_root_group_membership(user):\n    import grp, os\n    # Dependency 1: Get the list of Group IDs (GIDs) the user belongs to\n    user_gids = _get_system_user_gid_list(user)\n    \n    # Dependency 2: Check if the user is a member of the root group (GID 0)\n    is_root_group_member = _check_if_user_has_root_group(user_gids)\n    \n    return is_root_group_member", "summary": "Audits user root group membership by first retrieving the list of Group IDs (GIDs) the **`user`** belongs to using **`_get_system_user_gid_list`** (defined in **`audit_user_root_group_membership.py`**) (which relies on the **`grp`** and **`os`** libraries) and then checking if GID 0 (root group) is present in that list using **`_check_if_user_has_root_group`** (defined in **`audit_user_root_group_membership.py`**)."}
{"code": "def _compute_list_sum(number_list):\n    return sum(number_list)\n\ndef _check_if_sum_is_large(total_sum, threshold=1000000):\n    return total_sum > threshold\n\ndef analyze_list_sum_magnitude(data_points):\n    # Dependency 1: Compute the sum of all numbers in the list\n    total_sum = _compute_list_sum(data_points)\n    \n    # Dependency 2: Check if the total sum exceeds a 'large' threshold\n    is_large = _check_if_sum_is_large(total_sum)\n    \n    return {'total_sum': total_sum, 'is_large_sum': is_large}", "summary": "Analyzes the list sum magnitude by first computing the sum of all numbers in the **`data_points`** using **`_compute_list_sum`** (defined in **`analyze_list_sum_magnitude.py`**) and then checking if the total sum exceeds a large threshold (default 1,000,000) using **`_check_if_sum_is_large`** (defined in **`analyze_list_sum_magnitude.py`**)."}
{"code": "def _extract_list_from_dictionary_default(data_dict, key='items', default_list=[]):\n    return data_dict.get(key, default_list)\n\ndef _check_if_list_has_duplicates(input_list):\n    return len(input_list) != len(set(input_list))\n\ndef audit_list_duplicates_in_dict(data_record, target_key):\n    # Dependency 1: Extract the list from the dictionary, using an empty list if the key is missing\n    extracted_list = _extract_list_from_dictionary_default(data_record, target_key)\n    \n    # Dependency 2: Check if the extracted list contains any duplicate elements\n    has_duplicates = _check_if_list_has_duplicates(extracted_list)\n    \n    return {'list_content': extracted_list, 'has_duplicates': has_duplicates}", "summary": "Audits list duplicates in a dictionary by first extracting the list associated with the **`target_key`** from the **`data_record`** (defaulting to an empty list if the key is missing) using **`_extract_list_from_dictionary_default`** (defined in **`audit_list_duplicates_in_dict.py`**) and then checking if the extracted list contains any duplicates using **`_check_if_list_has_duplicates`** (defined in **`audit_list_duplicates_in_dict.py`**)."}
{"code": "def _compute_time_to_expire_seconds(expiry_datetime):\n    import datetime\n    # Returns 0 if expired\n    time_diff = expiry_datetime - datetime.datetime.now()\n    return max(0, int(time_diff.total_seconds()))\n\ndef _check_if_expires_soon(seconds_remaining, soon_threshold_hours=24):\n    return seconds_remaining < (soon_threshold_hours * 3600)\n\ndef audit_time_to_expiration(expiry_dt):\n    import datetime\n    # Dependency 1: Compute the time remaining until the expiry datetime, in seconds\n    seconds_left = _compute_time_to_expire_seconds(expiry_dt)\n    \n    # Dependency 2: Check if the expiration is 'soon' (e.g., within 24 hours)\n    is_expiring_soon = _check_if_expires_soon(seconds_left)\n    \n    return {'seconds_remaining': seconds_left, 'expiring_soon': is_expiring_soon}", "summary": "Audits time to expiration by first computing the time remaining until the **`expiry_dt`** in total seconds (returning 0 if expired) using **`_compute_time_to_expire_seconds`** (defined in **`audit_time_to_expiration.py`**) (which relies on the **`datetime`** library) and then checking if the remaining time is less than a 'soon' threshold (default 24 hours) using **`_check_if_expires_soon`** (defined in **`audit_time_to_expiration.py`**)."}
{"code": "def _compute_rsa_key_exponent(public_key):\n    # Mock function to inspect key properties\n    return 65537\n\ndef _check_if_exponent_is_standard(exponent, standard=65537):\n    return exponent == standard\n\ndef audit_encryption_key_exponent(key_string):\n    # Dependency 1: Compute the mock public exponent of the key\n    key_exponent = _compute_rsa_key_exponent(key_string)\n    \n    # Dependency 2: Check if the exponent is the standard value (65537)\n    is_standard = _check_if_exponent_is_standard(key_exponent)\n    \n    return {'exponent': key_exponent, 'is_standard': is_standard}", "summary": "Audits an encryption key's properties by first computing the mock public exponent of the key using **`_compute_rsa_key_exponent`** (defined in **`audit_encryption_key_exponent.py`**) and then checking if that exponent is equal to the standard value (65537 or $2^{16} + 1$) using **`_check_if_exponent_is_standard`** (defined in **`audit_encryption_key_exponent.py`**)."}
{"code": "def _format_datetime_to_rfc3339(dt_object):\n    # RFC 3339 is essentially ISO 8601 with Z for UTC\n    return dt_object.isoformat('T') + 'Z'\n\ndef _get_current_utc_datetime():\n    import datetime\n    return datetime.datetime.utcnow().replace(microsecond=0)\n\ndef generate_rfc3339_timestamp():\n    import datetime\n    # Dependency 1: Get the current datetime object in UTC (without microseconds)\n    utc_dt = _get_current_utc_datetime()\n    \n    # Dependency 2: Format the UTC datetime object into the RFC 3339 string format\n    rfc3339_string = _format_datetime_to_rfc3339(utc_dt)\n    \n    return rfc3339_string", "summary": "Generates an RFC 3339 timestamp by first retrieving the current UTC datetime object (without microseconds) using **`_get_current_utc_datetime`** (defined in **`generate_rfc3339_timestamp.py`**) (which relies on the **`datetime`** library) and then formatting that object into the RFC 3339 string format (ISO 8601 with trailing 'Z') using **`_format_datetime_to_rfc3339`** (defined in **`generate_rfc3339_timestamp.py`**)."}
{"code": "def _get_system_user_list_from_file(file_path):\n    # Mock reading /etc/passwd\n    with open(file_path, 'r') as f:\n        return [line.split(':')[0] for line in f.readlines()]\n\ndef _filter_users_starting_with_prefix(user_list, prefix='app_'):\n    return [u for u in user_list if u.startswith(prefix)]\n\ndef find_users_with_prefix(passwd_file):\n    # Dependency 1: Get the list of all mock system users from the passwd file\n    all_users = _get_system_user_list_from_file(passwd_file)\n    \n    # Dependency 2: Filter the list to include only users starting with the 'app_' prefix\n    prefixed_users = _filter_users_starting_with_prefix(all_users)\n    \n    return prefixed_users", "summary": "Finds users with a specific prefix by first retrieving the list of all mock system users from the **`passwd_file`** using **`_get_system_user_list_from_file`** (defined in **`find_users_with_prefix.py`**) and then filtering that list to include only usernames that start with the 'app\\_' prefix using **`_filter_users_starting_with_prefix`** (defined in **`find_users_with_prefix.py`**)."}
{"code": "def _compute_list_median_absolute_deviation(data_list):\n    import numpy as np\n    # Requires external library numpy\n    median = np.median(data_list)\n    return np.median(np.abs(data_list - median))\n\ndef _check_if_mad_is_high(mad_value, threshold=1.0):\n    return mad_value > threshold\n\ndef analyze_list_mad(data_points):\n    import numpy as np\n    # Dependency 1: Compute the Median Absolute Deviation (MAD)\n    mad = _compute_list_median_absolute_deviation(data_points)\n    \n    # Dependency 2: Check if the MAD value suggests high dispersion\n    is_high = _check_if_mad_is_high(mad)\n    \n    return {'mad': mad, 'is_high_dispersion': is_high}", "summary": "Analyzes data dispersion by first computing the Median Absolute Deviation (MAD) of the **`data_points`** using **`_compute_list_median_absolute_deviation`** (defined in **`analyze_list_mad.py`**) (which relies on the external **`numpy`** library for median and absolute difference) and then checking if the MAD value exceeds a threshold (default 1.0) using **`_check_if_mad_is_high`** (defined in **`analyze_list_mad.py`**)."}
{"code": "def _get_file_mime_type(file_path):\n    import mimetypes\n    # Requires external library mimetypes\n    mime, encoding = mimetypes.guess_type(file_path)\n    return mime\n\ndef _check_if_mime_type_is_binary(mime_type, binary_types=['application/', 'image/', 'video/', 'audio/']):\n    return any(mime_type and mime_type.startswith(t) for t in binary_types)\n\ndef audit_file_binary_type(target_file):\n    import mimetypes\n    # Dependency 1: Guess the MIME type of the file\n    file_mime = _get_file_mime_type(target_file)\n    \n    # Dependency 2: Check if the MIME type belongs to a common binary category\n    is_binary = _check_if_mime_type_is_binary(file_mime)\n    \n    return {'mime_type': file_mime, 'is_binary': is_binary}", "summary": "Audits a file's type by first guessing the MIME type of the **`target_file`** using **`_get_file_mime_type`** (defined in **`audit_file_binary_type.py`**) (which relies on the **`mimetypes`** library) and then checking if that MIME type starts with a common binary category prefix (e.g., 'application/', 'image/') using **`_check_if_mime_type_is_binary`** (defined in **`audit_file_binary_type.py`**)."}
{"code": "def _read_data_from_remote_url(url):\n    # Mock HTTP GET request\n    return '<h1>Data from Remote URL</h1>'\n\ndef _check_if_content_is_html(content_string):\n    return content_string.strip().startswith('<') and '>' in content_string\n\ndef fetch_and_verify_content_type(target_url):\n    # Dependency 1: Fetch the content string from the mock remote URL\n    content = _read_data_from_remote_url(target_url)\n    \n    # Dependency 2: Perform a basic check to see if the content looks like HTML\n    is_html = _check_if_content_is_html(content)\n    \n    return {'is_html': is_html}", "summary": "Fetches and verifies content type by first retrieving the raw content string from a mock remote **`target_url`** using **`_read_data_from_remote_url`** (defined in **`fetch_and_verify_content_type.py`**) and then performing a basic check to see if the content string starts with '<' and contains '>' (indicating potential HTML) using **`_check_if_content_is_html`** (defined in **`fetch_and_verify_content_type.py`**)."}
{"code": "def _extract_all_dates_iso8601(text):\n    import re\n    # Simple regex for YYYY-MM-DD or YYYY-MM-DDTHH:MM:SS\n    regex = r'\\d{4}-\\d{2}-\\d{2}(T\\d{2}:\\d{2}:\\d{2})?'\n    return re.findall(regex, text)\n\ndef _log_date_count(count):\n    print(f\"[PARSE] Found {count} ISO 8601 dates.\")\n\ndef find_and_log_iso_dates(input_text):\n    import re\n    # Dependency 1: Extract all strings matching a simple ISO 8601 date/datetime pattern\n    date_list = _extract_all_dates_iso8601(input_text)\n    \n    # Dependency 2: Log the total count of dates found\n    _log_date_count(len(date_list))\n    \n    return date_list", "summary": "Finds and logs ISO 8601 dates by first extracting all strings matching a simple ISO 8601 date/datetime pattern using regex via **`_extract_all_dates_iso8601`** (defined in **`find_and_log_iso_dates.py`**) (which relies on the **`re`** library) and then logging the total count of matches found using **`_log_date_count`** (defined in **`find_and_log_iso_dates.py`**)."}
{"code": "def _compute_list_mean_absolute_deviation(data_list):\n    import statistics\n    if not data_list: return 0.0\n    mean = statistics.mean(data_list)\n    return statistics.mean([abs(x - mean) for x in data_list])\n\ndef _check_if_mad_lt_std(mad, std_dev):\n    return mad < std_dev\n\ndef analyze_mean_and_std_deviation_relationship(data_set):\n    import statistics\n    # Dependency 1: Compute the Mean Absolute Deviation (MAD)\n    mad_value = _compute_list_mean_absolute_deviation(data_set)\n    \n    # Dependency 2: Compare the MAD to the standard deviation (STD) (MAD <= STD is expected)\n    std_dev = statistics.stdev(data_set) if len(data_set) > 1 else 0\n    is_mad_lt_std = _check_if_mad_lt_std(mad_value, std_dev)\n    \n    return {'mad': mad_value, 'std_dev': std_dev, 'mad_lt_std': is_mad_lt_std}", "summary": "Analyzes the relationship between MAD and STD by first computing the Mean Absolute Deviation (MAD) of the **`data_set`** using **`_compute_list_mean_absolute_deviation`** (defined in **`analyze_mean_and_std_deviation_relationship.py`**) (which relies on the **`statistics`** library) and then comparing the calculated MAD to the sample standard deviation (STD) using **`_check_if_mad_lt_std`** (defined in **`analyze_mean_and_std_deviation_relationship.py`**)."}
{"code": "def _send_data_to_webhook(webhook_url, payload):\n    # Mock HTTP POST request\n    return {'status': 202, 'message': 'Accepted'}\n\ndef _check_if_webhook_is_accepted(response_dict):\n    return response_dict.get('status') == 202\n\ndef trigger_webhook_and_verify(url, data_payload):\n    # Dependency 1: Send the data payload to the mock webhook URL\n    webhook_response = _send_data_to_webhook(url, data_payload)\n    \n    # Dependency 2: Check if the webhook response status indicates acceptance (202)\n    is_accepted = _check_if_webhook_is_accepted(webhook_response)\n    \n    return is_accepted", "summary": "Triggers a webhook by first sending the **`data_payload`** to the mock **`webhook_url`** using **`_send_data_to_webhook`** (defined in **`trigger_webhook_and_verify.py`**) and then checking if the mock response status code is 202 (Accepted) using **`_check_if_webhook_is_accepted`** (defined in **`trigger_webhook_and_verify.py`**)."}
{"code": "def _check_if_list_is_sorted_ascending(input_list):\n    return all(input_list[i] <= input_list[i+1] for i in range(len(input_list) - 1))\n\ndef _log_sort_status(list_name, is_sorted):\n    status = 'Sorted' if is_sorted else 'Unsorted'\n    print(f\"[ORDER_CHECK] List '{list_name}': {status}\")\n\ndef audit_list_sort_order(data_list, list_identifier):\n    # Dependency 1: Check if the list elements are in ascending order\n    is_sorted_asc = _check_if_list_is_sorted_ascending(data_list)\n    \n    # Dependency 2: Log the result of the sort order check\n    _log_sort_status(list_identifier, is_sorted_asc)\n    \n    return is_sorted_asc", "summary": "Audits list sort order by first checking if the **`data_list`** elements are in ascending order using **`_check_if_list_is_sorted_ascending`** (defined in **`audit_list_sort_order.py`**) and then logging the result (Sorted/Unsorted) using **`_log_sort_status`** (defined in **`audit_list_sort_order.py`**)."}
{"code": "def _square_value(n):\n    return n * n\n\ndef _multiply_by_pi(squared_radius):\n    import math\n    return squared_radius * math.pi\n\ndef calculate_circle_area(radius):\n    # Dependency 1: Square the radius\n    r_squared = _square_value(radius)\n    \n    # Dependency 2: Multiply the result by Pi\n    area = _multiply_by_pi(r_squared)\n    \n    return area", "summary": "Calculates the area of a circle given a **`radius`**. It first squares the radius using **`_square_value`** (defined in **`calculate_circle_area.py`**) and then multiplies that result by Pi using **`_multiply_by_pi`** (defined in **`calculate_circle_area.py`**) (which relies on the **`math`** library)."}
{"code": "def _generate_salt(length=16):\n    import secrets\n    return secrets.token_hex(length)\n\ndef _hash_with_salt(password, salt):\n    import hashlib\n    return hashlib.sha256((password + salt).encode()).hexdigest()\n\ndef create_salted_password_hash(raw_password):\n    # Dependency 1: Generate a random salt string\n    salt = _generate_salt()\n    \n    # Dependency 2: Hash the password combined with the salt\n    hashed_pw = _hash_with_salt(raw_password, salt)\n    \n    return {'salt': salt, 'hash': hashed_pw}", "summary": "Creates a secure password hash by first generating a random salt using **`_generate_salt`** (defined in **`create_salted_password_hash.py`**) (relying on the **`secrets`** library) and then computing the SHA256 hash of the combined password and salt using **`_hash_with_salt`** (defined in **`create_salted_password_hash.py`**)."}
{"code": "def _get_files_in_directory(dir_path):\n    import os\n    return [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n\ndef _count_items_in_list(item_list):\n    return len(item_list)\n\ndef count_files_in_folder(target_directory):\n    # Dependency 1: Retrieve a list of all files in the directory\n    file_list = _get_files_in_directory(target_directory)\n    \n    # Dependency 2: Count the number of files in the list\n    total_files = _count_items_in_list(file_list)\n    \n    return total_files", "summary": "Counts the total number of files in a **`target_directory`** by first retrieving the list of files using **`_get_files_in_directory`** (defined in **`count_files_in_folder.py`**) (which uses the **`os`** library) and then calculating the length of that list using **`_count_items_in_list`** (defined in **`count_files_in_folder.py`**)."}
{"code": "def _convert_celsius_to_fahrenheit(celsius):\n    return (celsius * 9/5) + 32\n\ndef _format_temperature_string(value, unit='F'):\n    return f\"{value:.1f}{unit}\"\n\ndef process_temperature_conversion(celsius_input):\n    # Dependency 1: Convert the numeric Celsius value to Fahrenheit\n    fahrenheit_val = _convert_celsius_to_fahrenheit(celsius_input)\n    \n    # Dependency 2: Format the resulting value into a display string\n    display_string = _format_temperature_string(fahrenheit_val)\n    \n    return display_string", "summary": "Converts a **`celsius_input`** to Fahrenheit string format. It first performs the mathematical conversion using **`_convert_celsius_to_fahrenheit`** (defined in **`process_temperature_conversion.py`**) and then formats the result into a readable string using **`_format_temperature_string`** (defined in **`process_temperature_conversion.py`**)."}
{"code": "def _parse_csv_line(line):\n    return line.strip().split(',')\n\ndef _convert_row_to_dict(headers, row_values):\n    return dict(zip(headers, row_values))\n\ndef parse_csv_string_to_record(csv_string, header_list):\n    # Dependency 1: Parse the raw CSV string line into a list of values\n    values = _parse_csv_line(csv_string)\n    \n    # Dependency 2: Map the header list to the values to create a dictionary\n    record_dict = _convert_row_to_dict(header_list, values)\n    \n    return record_dict", "summary": "Parses a single **`csv_string`** into a dictionary record by first splitting the line into values using **`_parse_csv_line`** (defined in **`parse_csv_string_to_record.py`**) and then zipping those values with a **`header_list`** using **`_convert_row_to_dict`** (defined in **`parse_csv_string_to_record.py`**)."}
{"code": "def _calculate_total_seconds(minutes, seconds):\n    return (minutes * 60) + seconds\n\ndef _check_if_timer_exceeded(total_time, limit=300):\n    return total_time > limit\n\ndef validate_timer_duration(mins, secs):\n    # Dependency 1: Calculate the total duration in seconds\n    total_duration = _calculate_total_seconds(mins, secs)\n    \n    # Dependency 2: Check if the total duration exceeds the 5-minute limit (300s)\n    is_exceeded = _check_if_timer_exceeded(total_duration)\n    \n    return {'total_seconds': total_duration, 'limit_exceeded': is_exceeded}", "summary": "Validates a timer duration by first calculating the total seconds from **`mins`** and **`secs`** using **`_calculate_total_seconds`** (defined in **`validate_timer_duration.py`**) and then checking if that total exceeds a predefined limit using **`_check_if_timer_exceeded`** (defined in **`validate_timer_duration.py`**)."}
{"code": "def _remove_special_characters(text):\n    import re\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\ndef _convert_to_title_case(text):\n    return text.title()\n\ndef clean_and_format_title(raw_title):\n    # Dependency 1: Remove special characters from the input string\n    clean_text = _remove_special_characters(raw_title)\n    \n    # Dependency 2: Convert the cleaned text to Title Case\n    formatted_title = _convert_to_title_case(clean_text)\n    \n    return formatted_title", "summary": "Cleans and formats a **`raw_title`** by first removing special characters using regex via **`_remove_special_characters`** (defined in **`clean_and_format_title.py`**) and then converting the result to Title Case using **`_convert_to_title_case`** (defined in **`clean_and_format_title.py`**)."}
{"code": "def _calculate_interest_amount(principal, rate, time):\n    return (principal * rate * time) / 100\n\ndef _add_interest_to_principal(principal, interest):\n    return principal + interest\n\ndef calculate_simple_interest_total(p, r, t):\n    # Dependency 1: Calculate the simple interest amount\n    interest = _calculate_interest_amount(p, r, t)\n    \n    # Dependency 2: Add the interest to the principal to get the final amount\n    total_amount = _add_interest_to_principal(p, interest)\n    \n    return total_amount", "summary": "Calculates the final amount after simple interest by first computing the interest value using **`_calculate_interest_amount`** (defined in **`calculate_simple_interest_total.py`**) and then adding it to the principal using **`_add_interest_to_principal`** (defined in **`calculate_simple_interest_total.py`**)."}
{"code": "def _get_system_hostname():\n    import socket\n    return socket.gethostname()\n\ndef _resolve_ip_from_hostname(hostname):\n    import socket\n    return socket.gethostbyname(hostname)\n\ndef get_local_system_network_info():\n    # Dependency 1: Retrieve the local system's hostname\n    host = _get_system_hostname()\n    \n    # Dependency 2: Resolve the IP address associated with that hostname\n    ip_addr = _resolve_ip_from_hostname(host)\n    \n    return {'hostname': host, 'ip_address': ip_addr}", "summary": "Retrieves local system network information by first getting the hostname using **`_get_system_hostname`** (defined in **`get_local_system_network_info.py`**) and then resolving that hostname to an IP address using **`_resolve_ip_from_hostname`** (defined in **`get_local_system_network_info.py`**) (both relying on the **`socket`** library)."}
{"code": "def _filter_dictionaries_by_key(dict_list, required_key):\n    return [d for d in dict_list if required_key in d]\n\ndef _extract_values_for_key(filtered_list, key):\n    return [d[key] for d in filtered_list]\n\ndef extract_valid_values_from_dict_list(data, target_key):\n    # Dependency 1: Filter the list to only include dictionaries containing the target key\n    valid_dicts = _filter_dictionaries_by_key(data, target_key)\n    \n    # Dependency 2: Extract the values of the target key from the valid dictionaries\n    values = _extract_values_from_key(valid_dicts, target_key)\n    \n    return values", "summary": "Extracts values from a list of dictionaries by first filtering for records that contain **`target_key`** using **`_filter_dictionaries_by_key`** (defined in **`extract_valid_values_from_dict_list.py`**) and then extracting the specific values from those valid records using **`_extract_values_for_key`** (defined in **`extract_valid_values_from_dict_list.py`**)."}
{"code": "def _compute_vector_dot_product(v1, v2):\n    return sum(a * b for a, b in zip(v1, v2))\n\ndef _check_if_vectors_are_orthogonal(dot_product):\n    # Orthogonal vectors have a dot product of 0\n    return dot_product == 0\n\ndef check_orthogonality(vector_a, vector_b):\n    # Dependency 1: Compute the dot product of the two vectors\n    dot_prod = _compute_vector_dot_product(vector_a, vector_b)\n    \n    # Dependency 2: Check if the dot product is zero\n    is_orthogonal = _check_if_vectors_are_orthogonal(dot_prod)\n    \n    return is_orthogonal", "summary": "Checks if two vectors are orthogonal by first computing their dot product using **`_compute_vector_dot_product`** (defined in **`check_orthogonality.py`**) and then verifying if the result is zero using **`_check_if_vectors_are_orthogonal`** (defined in **`check_orthogonality.py`**)."}
{"code": "def _read_json_file(file_path):\n    import json\n    with open(file_path, 'r') as f:\n        return json.load(f)\n\ndef _get_keys_from_dict(data_dict):\n    return list(data_dict.keys())\n\ndef extract_keys_from_json_file(target_file):\n    # Dependency 1: Load the JSON content from the file\n    json_data = _read_json_file(target_file)\n    \n    # Dependency 2: Extract the top-level keys from the loaded dictionary\n    keys = _get_keys_from_dict(json_data)\n    \n    return keys", "summary": "Extracts top-level keys from a JSON file by first reading and parsing the file using **`_read_json_file`** (defined in **`extract_keys_from_json_file.py`**) (which relies on the **`json`** library) and then retrieving the dictionary keys using **`_get_keys_from_dict`** (defined in **`extract_keys_from_json_file.py`**)."}
{"code": "def _find_minimum_value(data_list):\n    return min(data_list) if data_list else None\n\ndef _calculate_difference_from_min(value, min_val):\n    return value - min_val\n\ndef normalize_value_against_min(current_val, dataset):\n    # Dependency 1: Find the minimum value in the dataset\n    min_val = _find_minimum_value(dataset)\n    \n    # Dependency 2: Calculate the difference between the current value and the minimum\n    diff = _calculate_difference_from_min(current_val, min_val)\n    \n    return diff", "summary": "Normalizes a **`current_val`** against a dataset by first identifying the minimum value in the **`dataset`** using **`_find_minimum_value`** (defined in **`normalize_value_against_min.py`**) and then calculating the difference between the input value and that minimum using **`_calculate_difference_from_min`** (defined in **`normalize_value_against_min.py`**)."}
{"code": "def _validate_isbn_length(isbn_string):\n    # Remove hyphens and check length\n    clean_isbn = isbn_string.replace('-', '')\n    return len(clean_isbn) == 13\n\ndef _check_isbn_prefix(isbn_string):\n    return isbn_string.startswith('978') or isbn_string.startswith('979')\n\ndef basic_isbn_validation(isbn_candidate):\n    # Dependency 1: Validate the length of the ISBN (ignoring hyphens)\n    is_valid_len = _validate_isbn_length(isbn_candidate)\n    \n    # Dependency 2: Check if the ISBN starts with a valid standard prefix\n    has_valid_prefix = _check_isbn_prefix(isbn_candidate)\n    \n    return is_valid_len and has_valid_prefix", "summary": "Performs a basic validation of an **`isbn_candidate`** by first checking its length (excluding hyphens) using **`_validate_isbn_length`** (defined in **`basic_isbn_validation.py`**) and then verifying it starts with a standard prefix ('978' or '979') using **`_check_isbn_prefix`** (defined in **`basic_isbn_validation.py`**)."}
{"code": "def _compute_absolute_difference(a, b):\n    return abs(a - b)\n\ndef _check_if_within_tolerance(diff, tolerance=0.01):\n    return diff <= tolerance\n\ndef compare_floats_safely(val1, val2, tol):\n    # Dependency 1: Compute the absolute difference between the two floats\n    difference = _compute_absolute_difference(val1, val2)\n    \n    # Dependency 2: Check if that difference is within the allowed tolerance\n    is_equal = _check_if_within_tolerance(difference, tol)\n    \n    return is_equal", "summary": "Safely compares two floating-point numbers (**`val1`**, **`val2`**) by first computing their absolute difference using **`_compute_absolute_difference`** (defined in **`compare_floats_safely.py`**) and then checking if that difference falls within a specified **`tol`** (tolerance) using **`_check_if_within_tolerance`** (defined in **`compare_floats_safely.py`**)."}
