{"code": "def _get_spill_dir(self, n):\n        \"\"\" Choose one directory for spill by number n \"\"\"\n        return os.path.join(self.localdirs[n % len(self.localdirs)], str(n))", "summary": "Choose one directory for spill by number n", "name": "_get_spill_dir", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L219-L221", "code_lines": 3, "summary_length": 42}
{"code": "def _trim_css_to_bounds(css, image_shape):\n    \"\"\"\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)", "summary": "Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order", "name": "_trim_css_to_bounds", "complexity": 1, "num_dependencies": 4, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L52-L60", "code_lines": 8, "summary_length": 334}
{"code": "def saveAsSequenceFile(self, path, compressionCodecClass=None):\n        \"\"\"\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n        RDD's key and value types. The mechanism is as follows:\n\n            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n            2. Keys and values of this Java RDD are converted to Writables and written out.\n\n        :param path: path to sequence file\n        :param compressionCodecClass: (None by default)\n        \"\"\"\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,\n                                                   path, compressionCodecClass)", "summary": "Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n        RDD's key and value types. The mechanism is as follows:\n\n            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n            2. Keys and values of this Java RDD are converted to Writables and written out.\n\n        :param path: path to sequence file\n        :param compressionCodecClass: (None by default)", "name": "saveAsSequenceFile", "complexity": 1, "num_dependencies": 2, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1489-L1503", "code_lines": 13, "summary_length": 517}
{"code": "def _spill(self):\n        \"\"\"\n        dump already partitioned data into disks.\n        \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        path = self._get_spill_dir(self.spills)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        used_memory = get_used_memory()\n        if not self.pdata:\n            # The data has not been partitioned, it will iterator the\n            # data once, write them into different files, has no\n            # additional memory. It only called when the memory goes\n            # above limit at the first time.\n\n            # open all the files for writing\n            streams = [open(os.path.join(path, str(i)), 'wb')\n                       for i in range(self.partitions)]\n\n            # If the number of keys is small, then the overhead of sort is small\n            # sort them before dumping into disks\n            self._sorted = len(self.data) < self.SORT_KEY_LIMIT\n            if self._sorted:\n                self.serializer = self.flattened_serializer()\n                for k in sorted(self.data.keys()):\n                    h = self._partition(k)\n                    self.serializer.dump_stream([(k, self.data[k])], streams[h])\n            else:\n                for k, v in self.data.items():\n                    h = self._partition(k)\n                    self.serializer.dump_stream([(k, v)], streams[h])\n\n            for s in streams:\n                DiskBytesSpilled += s.tell()\n                s.close()\n\n            self.data.clear()\n            # self.pdata is cached in `mergeValues` and `mergeCombiners`\n            self.pdata.extend([{} for i in range(self.partitions)])\n\n        else:\n            for i in range(self.partitions):\n                p = os.path.join(path, str(i))\n                with open(p, \"wb\") as f:\n                    # dump items in batch\n                    if self._sorted:\n                        # sort by key only (stable)\n                        sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))\n                        self.serializer.dump_stream(sorted_items, f)\n                    else:\n                        self.serializer.dump_stream(self.pdata[i].items(), f)\n                self.pdata[i].clear()\n                DiskBytesSpilled += os.path.getsize(p)\n\n        self.spills += 1\n        gc.collect()  # release the memory as much as possible\n        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20", "summary": "dump already partitioned data into disks.", "name": "_spill", "complexity": 9, "num_dependencies": 37, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L709-L766", "code_lines": 51, "summary_length": 41}
{"code": "def min(self, key=None):\n        \"\"\"\n        Find the minimum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n        >>> rdd.min()\n        2.0\n        >>> rdd.min(key=str)\n        10.0\n        \"\"\"\n        if key is None:\n            return self.reduce(min)\n        return self.reduce(lambda a, b: min(a, b, key=key))", "summary": "Find the minimum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n        >>> rdd.min()\n        2.0\n        >>> rdd.min(key=str)\n        10.0", "name": "min", "complexity": 2, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1025-L1039", "code_lines": 13, "summary_length": 235}
{"code": "def _rect_to_css(rect):\n    \"\"\"\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return rect.top(), rect.right(), rect.bottom(), rect.left()", "summary": "Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order", "name": "_rect_to_css", "complexity": 1, "num_dependencies": 4, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L32-L39", "code_lines": 7, "summary_length": 210}
{"code": "def _get_path(self, n):\n        \"\"\" Choose one directory for spill by number n \"\"\"\n        d = self.local_dirs[n % len(self.local_dirs)]\n        if not os.path.exists(d):\n            os.makedirs(d)\n        return os.path.join(d, str(n))", "summary": "Choose one directory for spill by number n", "name": "_get_path", "complexity": 2, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L440-L445", "code_lines": 6, "summary_length": 42}
{"code": "def sorted(self, iterator, key=None, reverse=False):\n        \"\"\"\n        Sort the elements in iterator, do external sort when the memory\n        goes above the limit.\n        \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        batch, limit = 100, self._next_limit()\n        chunks, current_chunk = [], []\n        iterator = iter(iterator)\n        while True:\n            # pick elements in batch\n            chunk = list(itertools.islice(iterator, batch))\n            current_chunk.extend(chunk)\n            if len(chunk) < batch:\n                break\n\n            used_memory = get_used_memory()\n            if used_memory > limit:\n                # sort them inplace will save memory\n                current_chunk.sort(key=key, reverse=reverse)\n                path = self._get_path(len(chunks))\n                with open(path, 'wb') as f:\n                    self.serializer.dump_stream(current_chunk, f)\n\n                def load(f):\n                    for v in self.serializer.load_stream(f):\n                        yield v\n                    # close the file explicit once we consume all the items\n                    # to avoid ResourceWarning in Python3\n                    f.close()\n                chunks.append(load(open(path, 'rb')))\n                current_chunk = []\n                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20\n                DiskBytesSpilled += os.path.getsize(path)\n                os.unlink(path)  # data will be deleted after close\n\n            elif not chunks:\n                batch = min(int(batch * 1.5), 10000)\n\n        current_chunk.sort(key=key, reverse=reverse)\n        if not chunks:\n            return current_chunk\n\n        if current_chunk:\n            chunks.append(iter(current_chunk))\n\n        return heapq.merge(chunks, key=key, reverse=reverse)", "summary": "Sort the elements in iterator, do external sort when the memory\n        goes above the limit.", "name": "sorted", "complexity": 8, "num_dependencies": 29, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L455-L501", "code_lines": 41, "summary_length": 93}
{"code": "def items(self):\n        \"\"\" Return all merged items as iterator \"\"\"\n        if not self.pdata and not self.spills:\n            return iter(self.data.items())\n        return self._external_items()", "summary": "Return all merged items as iterator", "name": "items", "complexity": 3, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L339-L343", "code_lines": 5, "summary_length": 35}
{"code": "def partitionBy(self, numPartitions, partitionFunc=portable_hash):\n        \"\"\"\n        Return a copy of the RDD partitioned using the specified partitioner.\n\n        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n        >>> sets = pairs.partitionBy(2).glom().collect()\n        >>> len(set(sets[0]).intersection(set(sets[1])))\n        0\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n        partitioner = Partitioner(numPartitions, partitionFunc)\n        if self.partitioner == partitioner:\n            return self\n\n        # Transferring O(n) objects to Java is too expensive.\n        # Instead, we'll form the hash buckets in Python,\n        # transferring O(numPartitions) objects to Java.\n        # Each object is a (splitNumber, [objects]) pair.\n        # In order to avoid too huge objects, the objects are\n        # grouped into chunks.\n        outputSerializer = self.ctx._unbatched_serializer\n\n        limit = (_parse_memory(self.ctx._conf.get(\n            \"spark.python.worker.memory\", \"512m\")) / 2)\n\n        def add_shuffle_key(split, iterator):\n\n            buckets = defaultdict(list)\n            c, batch = 0, min(10 * numPartitions, 1000)\n\n            for k, v in iterator:\n                buckets[partitionFunc(k) % numPartitions].append((k, v))\n                c += 1\n\n                # check used memory and avg size of chunk of objects\n                if (c % 1000 == 0 and get_used_memory() > limit\n                        or c > batch):\n                    n, size = len(buckets), 0\n                    for split in list(buckets.keys()):\n                        yield pack_long(split)\n                        d = outputSerializer.dumps(buckets[split])\n                        del buckets[split]\n                        yield d\n                        size += len(d)\n\n                    avg = int(size / n) >> 20\n                    # let 1M < avg < 10M\n                    if avg < 1:\n                        batch *= 1.5\n                    elif avg > 10:\n                        batch = max(int(batch / 1.5), 1)\n                    c = 0\n\n            for split, items in buckets.items():\n                yield pack_long(split)\n                yield outputSerializer.dumps(items)\n\n        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)\n        keyed._bypass_serializer = True\n        with SCCallSiteSync(self.context) as css:\n            pairRDD = self.ctx._jvm.PairwiseRDD(\n                keyed._jrdd.rdd()).asJavaPairRDD()\n            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,\n                                                           id(partitionFunc))\n        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))\n        rdd = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))\n        rdd.partitioner = partitioner\n        return rdd", "summary": "Return a copy of the RDD partitioned using the specified partitioner.\n\n        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n        >>> sets = pairs.partitionBy(2).glom().collect()\n        >>> len(set(sets[0]).intersection(set(sets[1])))\n        0", "name": "partitionBy", "complexity": 11, "num_dependencies": 45, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1742-L1810", "code_lines": 59, "summary_length": 274}
