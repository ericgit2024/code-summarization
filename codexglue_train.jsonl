{"code": "def show_prediction_labels_on_image(img_path, predictions):\n    \"\"\"\n    Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:\n    \"\"\"\n    pil_image = Image.open(img_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(pil_image)\n\n    for name, (top, right, bottom, left) in predictions:\n        # Draw a box around the face using the Pillow module\n        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n        # There's a bug in Pillow where it blows up with non-UTF-8 text\n        # when using the default bitmap font\n        name = name.encode(\"UTF-8\")\n\n        # Draw a label with a name below the face\n        text_width, text_height = draw.textsize(name)\n        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n    # Remove the drawing library from memory as per the Pillow docs\n    del draw\n\n    # Display the resulting image\n    pil_image.show()", "summary": "Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:", "name": "show_prediction_labels_on_image", "complexity": 2, "num_dependencies": 8, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L153-L181", "code_lines": 23, "summary_length": 166}
{"code": "def worker(sock, authenticated):\n    \"\"\"\n    Called by a worker process after the fork().\n    \"\"\"\n    signal.signal(SIGHUP, SIG_DFL)\n    signal.signal(SIGCHLD, SIG_DFL)\n    signal.signal(SIGTERM, SIG_DFL)\n    # restore the handler for SIGINT,\n    # it's useful for debugging (show the stacktrace before exit)\n    signal.signal(SIGINT, signal.default_int_handler)\n\n    # Read the socket using fdopen instead of socket.makefile() because the latter\n    # seems to be very slow; note that we need to dup() the file descriptor because\n    # otherwise writes also cause a seek that makes us miss data on the read side.\n    infile = os.fdopen(os.dup(sock.fileno()), \"rb\", 65536)\n    outfile = os.fdopen(os.dup(sock.fileno()), \"wb\", 65536)\n\n    if not authenticated:\n        client_secret = UTF8Deserializer().loads(infile)\n        if os.environ[\"PYTHON_WORKER_FACTORY_SECRET\"] == client_secret:\n            write_with_length(\"ok\".encode(\"utf-8\"), outfile)\n            outfile.flush()\n        else:\n            write_with_length(\"err\".encode(\"utf-8\"), outfile)\n            outfile.flush()\n            sock.close()\n            return 1\n\n    exit_code = 0\n    try:\n        worker_main(infile, outfile)\n    except SystemExit as exc:\n        exit_code = compute_real_exit_code(exc.code)\n    finally:\n        try:\n            outfile.flush()\n        except Exception:\n            pass\n    return exit_code", "summary": "Called by a worker process after the fork().", "name": "worker", "complexity": 5, "num_dependencies": 21, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/daemon.py#L43-L81", "code_lines": 36, "summary_length": 44}
{"code": "def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n    \"\"\"\n    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param img: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    def convert_cnn_detections_to_css(detections):\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))", "summary": "Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param img: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order", "name": "batch_face_locations", "complexity": 1, "num_dependencies": 7, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L135-L151", "code_lines": 14, "summary_length": 634}
{"code": "def countByValue(self):\n        \"\"\"\n        Return the count of each unique value in this RDD as a dictionary of\n        (value, count) pairs.\n\n        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n        [(1, 2), (2, 3)]\n        \"\"\"\n        def countPartition(iterator):\n            counts = defaultdict(int)\n            for obj in iterator:\n                counts[obj] += 1\n            yield counts\n\n        def mergeMaps(m1, m2):\n            for k, v in m2.items():\n                m1[k] += v\n            return m1\n        return self.mapPartitions(countPartition).reduce(mergeMaps)", "summary": "Return the count of each unique value in this RDD as a dictionary of\n        (value, count) pairs.\n\n        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n        [(1, 2), (2, 3)]", "name": "countByValue", "complexity": 3, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1245-L1263", "code_lines": 17, "summary_length": 202}
{"code": "def saveAsPickleFile(self, path, batchSize=10):\n        \"\"\"\n        Save this RDD as a SequenceFile of serialized objects. The serializer\n        used is L{pyspark.serializers.PickleSerializer}, default batch size\n        is 10.\n\n        >>> tmpFile = NamedTemporaryFile(delete=True)\n        >>> tmpFile.close()\n        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n        ['1', '2', 'rdd', 'spark']\n        \"\"\"\n        if batchSize == 0:\n            ser = AutoBatchedSerializer(PickleSerializer())\n        else:\n            ser = BatchedSerializer(PickleSerializer(), batchSize)\n        self._reserialize(ser)._jrdd.saveAsObjectFile(path)", "summary": "Save this RDD as a SequenceFile of serialized objects. The serializer\n        used is L{pyspark.serializers.PickleSerializer}, default batch size\n        is 10.\n\n        >>> tmpFile = NamedTemporaryFile(delete=True)\n        >>> tmpFile.close()\n        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n        ['1', '2', 'rdd', 'spark']", "name": "saveAsPickleFile", "complexity": 2, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1505-L1521", "code_lines": 16, "summary_length": 433}
{"code": "def cache(self):\n        \"\"\"\n        Persist this RDD with the default storage level (C{MEMORY_ONLY}).\n        \"\"\"\n        self.is_cached = True\n        self.persist(StorageLevel.MEMORY_ONLY)\n        return self", "summary": "Persist this RDD with the default storage level (C{MEMORY_ONLY}).", "name": "cache", "complexity": 1, "num_dependencies": 1, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L223-L229", "code_lines": 7, "summary_length": 65}
{"code": "def _create_converter(dataType):\n    \"\"\"Create a converter to drop the names of fields in obj \"\"\"\n    if not _need_converter(dataType):\n        return lambda x: x\n\n    if isinstance(dataType, ArrayType):\n        conv = _create_converter(dataType.elementType)\n        return lambda row: [conv(v) for v in row]\n\n    elif isinstance(dataType, MapType):\n        kconv = _create_converter(dataType.keyType)\n        vconv = _create_converter(dataType.valueType)\n        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())\n\n    elif isinstance(dataType, NullType):\n        return lambda x: None\n\n    elif not isinstance(dataType, StructType):\n        return lambda x: x\n\n    # dataType must be StructType\n    names = [f.name for f in dataType.fields]\n    converters = [_create_converter(f.dataType) for f in dataType.fields]\n    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)\n\n    def convert_struct(obj):\n        if obj is None:\n            return\n\n        if isinstance(obj, (tuple, list)):\n            if convert_fields:\n                return tuple(conv(v) for v, conv in zip(obj, converters))\n            else:\n                return tuple(obj)\n\n        if isinstance(obj, dict):\n            d = obj\n        elif hasattr(obj, \"__dict__\"):  # object\n            d = obj.__dict__\n        else:\n            raise TypeError(\"Unexpected obj type: %s\" % type(obj))\n\n        if convert_fields:\n            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])\n        else:\n            return tuple([d.get(name) for name in names])\n\n    return convert_struct", "summary": "Create a converter to drop the names of fields in obj", "name": "_create_converter", "complexity": 12, "num_dependencies": 46, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1133-L1180", "code_lines": 38, "summary_length": 53}
{"code": "def reduceByKeyLocally(self, func):\n        \"\"\"\n        Merge the values for each key using an associative and commutative reduce function, but\n        return the results immediately to the master as a dictionary.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.reduceByKeyLocally(add).items())\n        [('a', 2), ('b', 1)]\n        \"\"\"\n        func = fail_on_stopiteration(func)\n\n        def reducePartition(iterator):\n            m = {}\n            for k, v in iterator:\n                m[k] = func(m[k], v) if k in m else v\n            yield m\n\n        def mergeMaps(m1, m2):\n            for k, v in m2.items():\n                m1[k] = func(m1[k], v) if k in m1 else v\n            return m1\n        return self.mapPartitions(reducePartition).reduce(mergeMaps)", "summary": "Merge the values for each key using an associative and commutative reduce function, but\n        return the results immediately to the master as a dictionary.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.reduceByKeyLocally(add).items())\n        [('a', 2), ('b', 1)]", "name": "reduceByKeyLocally", "complexity": 3, "num_dependencies": 8, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1629-L1654", "code_lines": 22, "summary_length": 497}
{"code": "def _infer_type(obj):\n    \"\"\"Infer the DataType from obj\n    \"\"\"\n    if obj is None:\n        return NullType()\n\n    if hasattr(obj, '__UDT__'):\n        return obj.__UDT__\n\n    dataType = _type_mappings.get(type(obj))\n    if dataType is DecimalType:\n        # the precision and scale of `obj` may be different from row to row.\n        return DecimalType(38, 18)\n    elif dataType is not None:\n        return dataType()\n\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            if key is not None and value is not None:\n                return MapType(_infer_type(key), _infer_type(value), True)\n        return MapType(NullType(), NullType(), True)\n    elif isinstance(obj, list):\n        for v in obj:\n            if v is not None:\n                return ArrayType(_infer_type(obj[0]), True)\n        return ArrayType(NullType(), True)\n    elif isinstance(obj, array):\n        if obj.typecode in _array_type_mappings:\n            return ArrayType(_array_type_mappings[obj.typecode](), False)\n        else:\n            raise TypeError(\"not supported type: array(%s)\" % obj.typecode)\n    else:\n        try:\n            return _infer_schema(obj)\n        except TypeError:\n            raise TypeError(\"not supported type: %s\" % type(obj))", "summary": "Infer the DataType from obj", "name": "_infer_type", "complexity": 15, "num_dependencies": 26, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1003-L1038", "code_lines": 33, "summary_length": 27}
{"code": "def takeSample(self, withReplacement, num, seed=None):\n        \"\"\"\n        Return a fixed-size sampled subset of this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> rdd = sc.parallelize(range(0, 10))\n        >>> len(rdd.takeSample(True, 20, 1))\n        20\n        >>> len(rdd.takeSample(False, 5, 2))\n        5\n        >>> len(rdd.takeSample(False, 15, 3))\n        10\n        \"\"\"\n        numStDev = 10.0\n\n        if num < 0:\n            raise ValueError(\"Sample size cannot be negative.\")\n        elif num == 0:\n            return []\n\n        initialCount = self.count()\n        if initialCount == 0:\n            return []\n\n        rand = random.Random(seed)\n\n        if (not withReplacement) and num >= initialCount:\n            # shuffle current RDD and return\n            samples = self.collect()\n            rand.shuffle(samples)\n            return samples\n\n        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))\n        if num > maxSampleSize:\n            raise ValueError(\n                \"Sample size cannot be greater than %d.\" % maxSampleSize)\n\n        fraction = RDD._computeFractionForSampleSize(\n            num, initialCount, withReplacement)\n        samples = self.sample(withReplacement, fraction, seed).collect()\n\n        # If the first sample didn't turn out large enough, keep trying to take samples;\n        # this shouldn't happen often because we use a big multiplier for their initial size.\n        # See: scala/spark/RDD.scala\n        while len(samples) < num:\n            # TODO: add log warning for when more than one iteration was run\n            seed = rand.randint(0, sys.maxsize)\n            samples = self.sample(withReplacement, fraction, seed).collect()\n\n        rand.shuffle(samples)\n\n        return samples[0:num]", "summary": "Return a fixed-size sampled subset of this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> rdd = sc.parallelize(range(0, 10))\n        >>> len(rdd.takeSample(True, 20, 1))\n        20\n        >>> len(rdd.takeSample(False, 5, 2))\n        5\n        >>> len(rdd.takeSample(False, 15, 3))\n        10", "name": "takeSample", "complexity": 8, "num_dependencies": 14, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L465-L518", "code_lines": 43, "summary_length": 426}
{"code": "def union(self, other):\n        \"\"\"\n        Return the union of this RDD and another one.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3])\n        >>> rdd.union(rdd).collect()\n        [1, 1, 2, 3, 1, 1, 2, 3]\n        \"\"\"\n        if self._jrdd_deserializer == other._jrdd_deserializer:\n            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,\n                      self._jrdd_deserializer)\n        else:\n            # These RDDs contain data in different serialized formats, so we\n            # must normalize them to the default serializer.\n            self_copy = self._reserialize()\n            other_copy = other._reserialize()\n            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,\n                      self.ctx.serializer)\n        if (self.partitioner == other.partitioner and\n                self.getNumPartitions() == rdd.getNumPartitions()):\n            rdd.partitioner = self.partitioner\n        return rdd", "summary": "Return the union of this RDD and another one.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3])\n        >>> rdd.union(rdd).collect()\n        [1, 1, 2, 3, 1, 1, 2, 3]", "name": "union", "complexity": 4, "num_dependencies": 8, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L553-L574", "code_lines": 21, "summary_length": 163}
{"code": "def stats(self):\n        \"\"\"\n        Return a L{StatCounter} object that captures the mean, variance\n        and count of the RDD's elements in one operation.\n        \"\"\"\n        def redFunc(left_counter, right_counter):\n            return left_counter.mergeStats(right_counter)\n\n        return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)", "summary": "Return a L{StatCounter} object that captures the mean, variance\n        and count of the RDD's elements in one operation.", "name": "stats", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1059-L1067", "code_lines": 8, "summary_length": 121}
{"code": "def _int_size_to_type(size):\n    \"\"\"\n    Return the Catalyst datatype from the size of integers.\n    \"\"\"\n    if size <= 8:\n        return ByteType\n    if size <= 16:\n        return ShortType\n    if size <= 32:\n        return IntegerType\n    if size <= 64:\n        return LongType", "summary": "Return the Catalyst datatype from the size of integers.", "name": "_int_size_to_type", "complexity": 5, "num_dependencies": 0, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L944-L955", "code_lines": 12, "summary_length": 55}
{"code": "def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        Merge the values for each key using an associative function \"func\"\n        and a neutral \"zeroValue\" which may be added to the result an\n        arbitrary number of times, and must not change the result\n        (e.g., 0 for addition, or 1 for multiplication.).\n\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> from operator import add\n        >>> sorted(rdd.foldByKey(0, add).collect())\n        [('a', 2), ('b', 1)]\n        \"\"\"\n        def createZero():\n            return copy.deepcopy(zeroValue)\n\n        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions,\n                                 partitionFunc)", "summary": "Merge the values for each key using an associative function \"func\"\n        and a neutral \"zeroValue\" which may be added to the result an\n        arbitrary number of times, and must not change the result\n        (e.g., 0 for addition, or 1 for multiplication.).\n\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> from operator import add\n        >>> sorted(rdd.foldByKey(0, add).collect())\n        [('a', 2), ('b', 1)]", "name": "foldByKey", "complexity": 1, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1893-L1909", "code_lines": 15, "summary_length": 444}
{"code": "def reduce(self, f):\n        \"\"\"\n        Reduces the elements of this RDD using the specified commutative and\n        associative binary operator. Currently reduces partitions locally.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n        15\n        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n        10\n        >>> sc.parallelize([]).reduce(add)\n        Traceback (most recent call last):\n            ...\n        ValueError: Can not reduce() empty RDD\n        \"\"\"\n        f = fail_on_stopiteration(f)\n\n        def func(iterator):\n            iterator = iter(iterator)\n            try:\n                initial = next(iterator)\n            except StopIteration:\n                return\n            yield reduce(f, iterator, initial)\n\n        vals = self.mapPartitions(func).collect()\n        if vals:\n            return reduce(f, vals)\n        raise ValueError(\"Can not reduce() empty RDD\")", "summary": "Reduces the elements of this RDD using the specified commutative and\n        associative binary operator. Currently reduces partitions locally.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n        15\n        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n        10\n        >>> sc.parallelize([]).reduce(add)\n        Traceback (most recent call last):\n            ...\n        ValueError: Can not reduce() empty RDD", "name": "reduce", "complexity": 3, "num_dependencies": 10, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L821-L849", "code_lines": 26, "summary_length": 496}
{"code": "def sample(self, withReplacement, fraction, seed=None):\n        \"\"\"\n        Return a sampled subset of this RDD.\n\n        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n        :param fraction: expected size of the sample as a fraction of this RDD's size\n            without replacement: probability that each element is chosen; fraction must be [0, 1]\n            with replacement: expected number of times each element is chosen; fraction must be >= 0\n        :param seed: seed for the random number generator\n\n        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n            count of the given :class:`DataFrame`.\n\n        >>> rdd = sc.parallelize(range(100), 4)\n        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n        True\n        \"\"\"\n        assert fraction >= 0.0, \"Negative fraction value: %s\" % fraction\n        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)", "summary": "Return a sampled subset of this RDD.\n\n        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n        :param fraction: expected size of the sample as a fraction of this RDD's size\n            without replacement: probability that each element is chosen; fraction must be [0, 1]\n            with replacement: expected number of times each element is chosen; fraction must be >= 0\n        :param seed: seed for the random number generator\n\n        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n            count of the given :class:`DataFrame`.\n\n        >>> rdd = sc.parallelize(range(100), 4)\n        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n        True", "name": "sample", "complexity": 1, "num_dependencies": 1, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L418-L436", "code_lines": 16, "summary_length": 747}
{"code": "def to_arrow_schema(schema):\n    \"\"\" Convert a schema from Spark to Arrow\n    \"\"\"\n    import pyarrow as pa\n    fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)\n              for field in schema]\n    return pa.schema(fields)", "summary": "Convert a schema from Spark to Arrow", "name": "to_arrow_schema", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1625-L1631", "code_lines": 7, "summary_length": 36}
{"code": "def _make_type_verifier(dataType, nullable=True, name=None):\n    \"\"\"\n    Make a verifier that checks the type of obj against dataType and raises a TypeError if they do\n    not match.\n\n    This verifier also checks the value of obj against datatype and raises a ValueError if it's not\n    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is\n    not checked, so it will become infinity when cast to Java float if it overflows.\n\n    >>> _make_type_verifier(StructType([]))(None)\n    >>> _make_type_verifier(StringType())(\"\")\n    >>> _make_type_verifier(LongType())(0)\n    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))\n    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})\n    >>> _make_type_verifier(StructType([]))(())\n    >>> _make_type_verifier(StructType([]))([])\n    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> # Check if numeric values are within the allowed range.\n    >>> _make_type_verifier(ByteType())(12)\n    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(\n    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> schema = StructType().add(\"a\", IntegerType()).add(\"b\", StringType(), False)\n    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    \"\"\"\n\n    if name is None:\n        new_msg = lambda msg: msg\n        new_name = lambda n: \"field %s\" % n\n    else:\n        new_msg = lambda msg: \"%s: %s\" % (name, msg)\n        new_name = lambda n: \"field %s in %s\" % (n, name)\n\n    def verify_nullability(obj):\n        if obj is None:\n            if nullable:\n                return True\n            else:\n                raise ValueError(new_msg(\"This field is not nullable, but got None\"))\n        else:\n            return False\n\n    _type = type(dataType)\n\n    def assert_acceptable_types(obj):\n        assert _type in _acceptable_types, \\\n            new_msg(\"unknown datatype: %s for object %r\" % (dataType, obj))\n\n    def verify_acceptable_types(obj):\n        # subclass of them can not be fromInternal in JVM\n        if type(obj) not in _acceptable_types[_type]:\n            raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\n                                    % (dataType, obj, type(obj))))\n\n    if isinstance(dataType, StringType):\n        # StringType can work with any types\n        verify_value = lambda _: _\n\n    elif isinstance(dataType, UserDefinedType):\n        verifier = _make_type_verifier(dataType.sqlType(), name=name)\n\n        def verify_udf(obj):\n            if not (hasattr(obj, '__UDT__') and obj.__UDT__ == dataType):\n                raise ValueError(new_msg(\"%r is not an instance of type %r\" % (obj, dataType)))\n            verifier(dataType.toInternal(obj))\n\n        verify_value = verify_udf\n\n    elif isinstance(dataType, ByteType):\n        def verify_byte(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -128 or obj > 127:\n                raise ValueError(new_msg(\"object of ByteType out of range, got: %s\" % obj))\n\n        verify_value = verify_byte\n\n    elif isinstance(dataType, ShortType):\n        def verify_short(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -32768 or obj > 32767:\n                raise ValueError(new_msg(\"object of ShortType out of range, got: %s\" % obj))\n\n        verify_value = verify_short\n\n    elif isinstance(dataType, IntegerType):\n        def verify_integer(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -2147483648 or obj > 2147483647:\n                raise ValueError(\n                    new_msg(\"object of IntegerType out of range, got: %s\" % obj))\n\n        verify_value = verify_integer\n\n    elif isinstance(dataType, ArrayType):\n        element_verifier = _make_type_verifier(\n            dataType.elementType, dataType.containsNull, name=\"element in array %s\" % name)\n\n        def verify_array(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            for i in obj:\n                element_verifier(i)\n\n        verify_value = verify_array\n\n    elif isinstance(dataType, MapType):\n        key_verifier = _make_type_verifier(dataType.keyType, False, name=\"key of map %s\" % name)\n        value_verifier = _make_type_verifier(\n            dataType.valueType, dataType.valueContainsNull, name=\"value of map %s\" % name)\n\n        def verify_map(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            for k, v in obj.items():\n                key_verifier(k)\n                value_verifier(v)\n\n        verify_value = verify_map\n\n    elif isinstance(dataType, StructType):\n        verifiers = []\n        for f in dataType.fields:\n            verifier = _make_type_verifier(f.dataType, f.nullable, name=new_name(f.name))\n            verifiers.append((f.name, verifier))\n\n        def verify_struct(obj):\n            assert_acceptable_types(obj)\n\n            if isinstance(obj, dict):\n                for f, verifier in verifiers:\n                    verifier(obj.get(f))\n            elif isinstance(obj, Row) and getattr(obj, \"__from_dict__\", False):\n                # the order in obj could be different than dataType.fields\n                for f, verifier in verifiers:\n                    verifier(obj[f])\n            elif isinstance(obj, (tuple, list)):\n                if len(obj) != len(verifiers):\n                    raise ValueError(\n                        new_msg(\"Length of object (%d) does not match with \"\n                                \"length of fields (%d)\" % (len(obj), len(verifiers))))\n                for v, (_, verifier) in zip(obj, verifiers):\n                    verifier(v)\n            elif hasattr(obj, \"__dict__\"):\n                d = obj.__dict__\n                for f, verifier in verifiers:\n                    verifier(d.get(f))\n            else:\n                raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\n                                        % (obj, type(obj))))\n        verify_value = verify_struct\n\n    else:\n        def verify_default(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n\n        verify_value = verify_default\n\n    def verify(obj):\n        if not verify_nullability(obj):\n            verify_value(obj)\n\n    return verify", "summary": "Make a verifier that checks the type of obj against dataType and raises a TypeError if they do\n    not match.\n\n    This verifier also checks the value of obj against datatype and raises a ValueError if it's not\n    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is\n    not checked, so it will become infinity when cast to Java float if it overflows.\n\n    >>> _make_type_verifier(StructType([]))(None)\n    >>> _make_type_verifier(StringType())(\"\")\n    >>> _make_type_verifier(LongType())(0)\n    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))\n    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})\n    >>> _make_type_verifier(StructType([]))(())\n    >>> _make_type_verifier(StructType([]))([])\n    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> # Check if numeric values are within the allowed range.\n    >>> _make_type_verifier(ByteType())(12)\n    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(\n    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> schema = StructType().add(\"a\", IntegerType()).add(\"b\", StringType(), False)\n    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...", "name": "_make_type_verifier", "complexity": 35, "num_dependencies": 133, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1202-L1391", "code_lines": 160, "summary_length": 2080}
{"code": "def _get_local_dirs(sub):\n    \"\"\" Get all the directories \"\"\"\n    path = os.environ.get(\"SPARK_LOCAL_DIRS\", \"/tmp\")\n    dirs = path.split(\",\")\n    if len(dirs) > 1:\n        # different order in different processes and instances\n        rnd = random.Random(os.getpid() + id(dirs))\n        random.shuffle(dirs, rnd.random)\n    return [os.path.join(d, \"python\", str(os.getpid()), sub) for d in dirs]", "summary": "Get all the directories", "name": "_get_local_dirs", "complexity": 2, "num_dependencies": 10, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L71-L79", "code_lines": 9, "summary_length": 23}
{"code": "def flatMap(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD by first applying a function to all elements of this\n        RDD, and then flattening the results.\n\n        >>> rdd = sc.parallelize([2, 3, 4])\n        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n        [1, 1, 1, 2, 2, 3]\n        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n        \"\"\"\n        def func(s, iterator):\n            return chain.from_iterable(map(fail_on_stopiteration(f), iterator))\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)", "summary": "Return a new RDD by first applying a function to all elements of this\n        RDD, and then flattening the results.\n\n        >>> rdd = sc.parallelize([2, 3, 4])\n        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n        [1, 1, 1, 2, 2, 3]\n        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]", "name": "flatMap", "complexity": 1, "num_dependencies": 7, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L329-L342", "code_lines": 13, "summary_length": 379}
{"code": "def collect(self):\n        \"\"\"\n        Return a list that contains all of the elements in this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n        \"\"\"\n        with SCCallSiteSync(self.context) as css:\n            sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n        return list(_load_from_socket(sock_info, self._jrdd_deserializer))", "summary": "Return a list that contains all of the elements in this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.", "name": "collect", "complexity": 1, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L810-L819", "code_lines": 9, "summary_length": 223}
{"code": "def from_arrow_schema(arrow_schema):\n    \"\"\" Convert schema from Arrow to Spark.\n    \"\"\"\n    return StructType(\n        [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)\n         for field in arrow_schema])", "summary": "Convert schema from Arrow to Spark.", "name": "from_arrow_schema", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1677-L1682", "code_lines": 6, "summary_length": 35}
{"code": "def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    if model == \"cnn\":\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n    else:\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]", "summary": "Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order", "name": "face_locations", "complexity": 2, "num_dependencies": 6, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L108-L121", "code_lines": 13, "summary_length": 557}
{"code": "def groupBy(self, f, numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        Return an RDD of grouped items.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n        >>> result = rdd.groupBy(lambda x: x % 2).collect()\n        >>> sorted([(x, sorted(y)) for (x, y) in result])\n        [(0, [2, 8]), (1, [1, 1, 3, 5])]\n        \"\"\"\n        return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)", "summary": "Return an RDD of grouped items.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n        >>> result = rdd.groupBy(lambda x: x % 2).collect()\n        >>> sorted([(x, sorted(y)) for (x, y) in result])\n        [(0, [2, 8]), (1, [1, 1, 3, 5])]", "name": "groupBy", "complexity": 1, "num_dependencies": 1, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L731-L740", "code_lines": 9, "summary_length": 244}
{"code": "def portable_hash(x):\n    \"\"\"\n    This function returns consistent hash code for builtin types, especially\n    for None and tuple with None.\n\n    The algorithm is similar to that one used by CPython 2.7\n\n    >>> portable_hash(None)\n    0\n    >>> portable_hash((None, 1)) & 0xffffffff\n    219750521\n    \"\"\"\n\n    if sys.version_info >= (3, 2, 3) and 'PYTHONHASHSEED' not in os.environ:\n        raise Exception(\"Randomness of hash of string should be disabled via PYTHONHASHSEED\")\n\n    if x is None:\n        return 0\n    if isinstance(x, tuple):\n        h = 0x345678\n        for i in x:\n            h ^= portable_hash(i)\n            h *= 1000003\n            h &= sys.maxsize\n        h ^= len(x)\n        if h == -1:\n            h = -2\n        return int(h)\n    return hash(x)", "summary": "This function returns consistent hash code for builtin types, especially\n    for None and tuple with None.\n\n    The algorithm is similar to that one used by CPython 2.7\n\n    >>> portable_hash(None)\n    0\n    >>> portable_hash((None, 1)) & 0xffffffff\n    219750521", "name": "portable_hash", "complexity": 7, "num_dependencies": 6, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L78-L106", "code_lines": 25, "summary_length": 263}
{"code": "def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):\n    \"\"\"\n    Convert timestamp to timezone-naive in the specified timezone or local timezone\n\n    :param s: a pandas.Series\n    :param from_timezone: the timezone to convert from. if None then use local timezone\n    :param to_timezone: the timezone to convert to. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been converted to tz-naive\n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    import pandas as pd\n    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype\n    from_tz = from_timezone or _get_local_timezone()\n    to_tz = to_timezone or _get_local_timezone()\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64tz_dtype(s.dtype):\n        return s.dt.tz_convert(to_tz).dt.tz_localize(None)\n    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:\n        # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.\n        return s.apply(\n            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)\n            if ts is not pd.NaT else pd.NaT)\n    else:\n        return s", "summary": "Convert timestamp to timezone-naive in the specified timezone or local timezone\n\n    :param s: a pandas.Series\n    :param from_timezone: the timezone to convert from. if None then use local timezone\n    :param to_timezone: the timezone to convert to. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been converted to tz-naive", "name": "_check_series_convert_timestamps_localize", "complexity": 6, "num_dependencies": 8, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1792-L1817", "code_lines": 24, "summary_length": 367}
{"code": "def fold(self, zeroValue, op):\n        \"\"\"\n        Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given associative function and a neutral \"zero value.\"\n\n        The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        This behaves somewhat differently from fold operations implemented\n        for non-distributed collections in functional languages like Scala.\n        This fold operation may be applied to partitions individually, and then\n        fold those results into the final result, rather than apply the fold\n        to each element sequentially in some defined ordering. For functions\n        that are not commutative, the result may differ from that of a fold\n        applied to a non-distributed collection.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n        15\n        \"\"\"\n        op = fail_on_stopiteration(op)\n\n        def func(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = op(acc, obj)\n            yield acc\n        # collecting result of mapPartitions here ensures that the copy of\n        # zeroValue provided to each partition is unique from the one provided\n        # to the final reduce call\n        vals = self.mapPartitions(func).collect()\n        return reduce(op, vals, zeroValue)", "summary": "Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given associative function and a neutral \"zero value.\"\n\n        The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        This behaves somewhat differently from fold operations implemented\n        for non-distributed collections in functional languages like Scala.\n        This fold operation may be applied to partitions individually, and then\n        fold those results into the final result, rather than apply the fold\n        to each element sequentially in some defined ordering. For functions\n        that are not commutative, the result may differ from that of a fold\n        applied to a non-distributed collection.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n        15", "name": "fold", "complexity": 2, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L888-L920", "code_lines": 29, "summary_length": 952}
{"code": "def mergeValues(self, iterator):\n        \"\"\" Combine the items by creator and combiner \"\"\"\n        # speedup attribute lookup\n        creator, comb = self.agg.createCombiner, self.agg.mergeValue\n        c, data, pdata, hfun, batch = 0, self.data, self.pdata, self._partition, self.batch\n        limit = self.memory_limit\n\n        for k, v in iterator:\n            d = pdata[hfun(k)] if pdata else data\n            d[k] = comb(d[k], v) if k in d else creator(v)\n\n            c += 1\n            if c >= batch:\n                if get_used_memory() >= limit:\n                    self._spill()\n                    limit = self._next_limit()\n                    batch /= 2\n                    c = 0\n                else:\n                    batch *= 1.5\n\n        if get_used_memory() >= limit:\n            self._spill()", "summary": "Combine the items by creator and combiner", "name": "mergeValues", "complexity": 5, "num_dependencies": 8, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L231-L253", "code_lines": 20, "summary_length": 41}
{"code": "def sortBy(self, keyfunc, ascending=True, numPartitions=None):\n        \"\"\"\n        Sorts this RDD by the given keyfunc\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        \"\"\"\n        return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()", "summary": "Sorts this RDD by the given keyfunc\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]", "name": "sortBy", "complexity": 1, "num_dependencies": 1, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L691-L701", "code_lines": 10, "summary_length": 353}
{"code": "def mergeCombiners(self, iterator, limit=None):\n        \"\"\" Merge (K,V) pair by mergeCombiner \"\"\"\n        if limit is None:\n            limit = self.memory_limit\n        # speedup attribute lookup\n        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size\n        c, data, pdata, batch = 0, self.data, self.pdata, self.batch\n        for k, v in iterator:\n            d = pdata[hfun(k)] if pdata else data\n            d[k] = comb(d[k], v) if k in d else v\n            if not limit:\n                continue\n\n            c += objsize(v)\n            if c > batch:\n                if get_used_memory() > limit:\n                    self._spill()\n                    limit = self._next_limit()\n                    batch /= 2\n                    c = 0\n                else:\n                    batch *= 1.5\n\n        if limit and get_used_memory() >= limit:\n            self._spill()", "summary": "Merge (K,V) pair by mergeCombiner", "name": "mergeCombiners", "complexity": 8, "num_dependencies": 8, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L265-L289", "code_lines": 23, "summary_length": 33}
{"code": "def _spill(self):\n        \"\"\" dump the values into disk \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        if self._file is None:\n            self._open_file()\n\n        used_memory = get_used_memory()\n        pos = self._file.tell()\n        self._ser.dump_stream(self.values, self._file)\n        self.values = []\n        gc.collect()\n        DiskBytesSpilled += self._file.tell() - pos\n        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20", "summary": "dump the values into disk", "name": "_spill", "complexity": 2, "num_dependencies": 8, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L590-L602", "code_lines": 12, "summary_length": 25}
{"code": "def takeOrdered(self, num, key=None):\n        \"\"\"\n        Get the N elements from an RDD ordered in ascending order or as\n        specified by the optional key function.\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n        [1, 2, 3, 4, 5, 6]\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n        [10, 9, 7, 6, 5, 4]\n        \"\"\"\n\n        def merge(a, b):\n            return heapq.nsmallest(num, a + b, key)\n\n        return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)", "summary": "Get the N elements from an RDD ordered in ascending order or as\n        specified by the optional key function.\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n        [1, 2, 3, 4, 5, 6]\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n        [10, 9, 7, 6, 5, 4]", "name": "takeOrdered", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1289-L1306", "code_lines": 14, "summary_length": 495}
{"code": "def from_arrow_type(at):\n    \"\"\" Convert pyarrow type to Spark data type.\n    \"\"\"\n    import pyarrow.types as types\n    if types.is_boolean(at):\n        spark_type = BooleanType()\n    elif types.is_int8(at):\n        spark_type = ByteType()\n    elif types.is_int16(at):\n        spark_type = ShortType()\n    elif types.is_int32(at):\n        spark_type = IntegerType()\n    elif types.is_int64(at):\n        spark_type = LongType()\n    elif types.is_float32(at):\n        spark_type = FloatType()\n    elif types.is_float64(at):\n        spark_type = DoubleType()\n    elif types.is_decimal(at):\n        spark_type = DecimalType(precision=at.precision, scale=at.scale)\n    elif types.is_string(at):\n        spark_type = StringType()\n    elif types.is_binary(at):\n        spark_type = BinaryType()\n    elif types.is_date32(at):\n        spark_type = DateType()\n    elif types.is_timestamp(at):\n        spark_type = TimestampType()\n    elif types.is_list(at):\n        if types.is_timestamp(at.value_type):\n            raise TypeError(\"Unsupported type in conversion from Arrow: \" + str(at))\n        spark_type = ArrayType(from_arrow_type(at.value_type))\n    elif types.is_struct(at):\n        if any(types.is_struct(field.type) for field in at):\n            raise TypeError(\"Nested StructType not supported in conversion from Arrow: \" + str(at))\n        return StructType(\n            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)\n             for field in at])\n    else:\n        raise TypeError(\"Unsupported type in conversion from Arrow: \" + str(at))\n    return spark_type", "summary": "Convert pyarrow type to Spark data type.", "name": "from_arrow_type", "complexity": 17, "num_dependencies": 40, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1634-L1674", "code_lines": 41, "summary_length": 40}
{"code": "def reduceByKey(self, func, numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        Merge the values for each key using an associative and commutative reduce function.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n\n        Output will be partitioned with C{numPartitions} partitions, or\n        the default parallelism level if C{numPartitions} is not specified.\n        Default partitioner is hash-partition.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.reduceByKey(add).collect())\n        [('a', 2), ('b', 1)]\n        \"\"\"\n        return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)", "summary": "Merge the values for each key using an associative and commutative reduce function.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n\n        Output will be partitioned with C{numPartitions} partitions, or\n        the default parallelism level if C{numPartitions} is not specified.\n        Default partitioner is hash-partition.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.reduceByKey(add).collect())\n        [('a', 2), ('b', 1)]", "name": "reduceByKey", "complexity": 1, "num_dependencies": 1, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1611-L1627", "code_lines": 14, "summary_length": 614}
{"code": "def _spill(self):\n        \"\"\"\n        dump already partitioned data into disks.\n\n        It will dump the data in batch for better performance.\n        \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        path = self._get_spill_dir(self.spills)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        used_memory = get_used_memory()\n        if not self.pdata:\n            # The data has not been partitioned, it will iterator the\n            # dataset once, write them into different files, has no\n            # additional memory. It only called when the memory goes\n            # above limit at the first time.\n\n            # open all the files for writing\n            streams = [open(os.path.join(path, str(i)), 'wb')\n                       for i in range(self.partitions)]\n\n            for k, v in self.data.items():\n                h = self._partition(k)\n                # put one item in batch, make it compatible with load_stream\n                # it will increase the memory if dump them in batch\n                self.serializer.dump_stream([(k, v)], streams[h])\n\n            for s in streams:\n                DiskBytesSpilled += s.tell()\n                s.close()\n\n            self.data.clear()\n            self.pdata.extend([{} for i in range(self.partitions)])\n\n        else:\n            for i in range(self.partitions):\n                p = os.path.join(path, str(i))\n                with open(p, \"wb\") as f:\n                    # dump items in batch\n                    self.serializer.dump_stream(iter(self.pdata[i].items()), f)\n                self.pdata[i].clear()\n                DiskBytesSpilled += os.path.getsize(p)\n\n        self.spills += 1\n        gc.collect()  # release the memory as much as possible\n        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20", "summary": "dump already partitioned data into disks.\n\n        It will dump the data in batch for better performance.", "name": "_spill", "complexity": 6, "num_dependencies": 28, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L291-L337", "code_lines": 39, "summary_length": 105}
{"code": "def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n    \"\"\"\n    Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.\n    \"\"\"\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\n\n    if knn_clf is None and model_path is None:\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n\n    # Load a trained KNN model (if one was passed in)\n    if knn_clf is None:\n        with open(model_path, 'rb') as f:\n            knn_clf = pickle.load(f)\n\n    # Load image file and find face locations\n    X_img = face_recognition.load_image_file(X_img_path)\n    X_face_locations = face_recognition.face_locations(X_img)\n\n    # If no faces are found in the image, return an empty result.\n    if len(X_face_locations) == 0:\n        return []\n\n    # Find encodings for faces in the test iamge\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n\n    # Use the KNN model to find the best matches for the test face\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n\n    # Predict classes and remove classifications that aren't within the threshold\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]", "summary": "Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.", "name": "predict", "complexity": 7, "num_dependencies": 16, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L111-L150", "code_lines": 32, "summary_length": 725}
{"code": "def saveAsTextFile(self, path, compressionCodecClass=None):\n        \"\"\"\n        Save this RDD as a text file, using string representations of elements.\n\n        @param path: path to text file\n        @param compressionCodecClass: (None by default) string i.e.\n            \"org.apache.hadoop.io.compress.GzipCodec\"\n\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> tempFile.close()\n        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n        >>> from fileinput import input\n        >>> from glob import glob\n        >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n        '0\\\\n1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n7\\\\n8\\\\n9\\\\n'\n\n        Empty lines are tolerated when saving to text files.\n\n        >>> tempFile2 = NamedTemporaryFile(delete=True)\n        >>> tempFile2.close()\n        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n        >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n        '\\\\n\\\\n\\\\nbar\\\\nfoo\\\\n'\n\n        Using compressionCodecClass\n\n        >>> tempFile3 = NamedTemporaryFile(delete=True)\n        >>> tempFile3.close()\n        >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n        >>> from fileinput import input, hook_compressed\n        >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n        >>> b''.join(result).decode('utf-8')\n        u'bar\\\\nfoo\\\\n'\n        \"\"\"\n        def func(split, iterator):\n            for x in iterator:\n                if not isinstance(x, (unicode, bytes)):\n                    x = unicode(x)\n                if isinstance(x, unicode):\n                    x = x.encode(\"utf-8\")\n                yield x\n        keyed = self.mapPartitionsWithIndex(func)\n        keyed._bypass_serializer = True\n        if compressionCodecClass:\n            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)\n            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)\n        else:\n            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)", "summary": "Save this RDD as a text file, using string representations of elements.\n\n        @param path: path to text file\n        @param compressionCodecClass: (None by default) string i.e.\n            \"org.apache.hadoop.io.compress.GzipCodec\"\n\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> tempFile.close()\n        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n        >>> from fileinput import input\n        >>> from glob import glob\n        >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n        '0\\\\n1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n7\\\\n8\\\\n9\\\\n'\n\n        Empty lines are tolerated when saving to text files.\n\n        >>> tempFile2 = NamedTemporaryFile(delete=True)\n        >>> tempFile2.close()\n        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n        >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n        '\\\\n\\\\n\\\\nbar\\\\nfoo\\\\n'\n\n        Using compressionCodecClass\n\n        >>> tempFile3 = NamedTemporaryFile(delete=True)\n        >>> tempFile3.close()\n        >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n        >>> from fileinput import input, hook_compressed\n        >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n        >>> b''.join(result).decode('utf-8')\n        u'bar\\\\nfoo\\\\n'", "name": "saveAsTextFile", "complexity": 5, "num_dependencies": 12, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1524-L1572", "code_lines": 43, "summary_length": 1413}
{"code": "def face_encodings(face_image, known_face_locations=None, num_jitters=1):\n    \"\"\"\n    Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :return: A list of 128-dimensional face encodings (one for each face in the image)\n    \"\"\"\n    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model=\"small\")\n    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]", "summary": "Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :return: A list of 128-dimensional face encodings (one for each face in the image)", "name": "face_encodings", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L203-L213", "code_lines": 10, "summary_length": 487}
{"code": "def face_distance(face_encodings, face_to_compare):\n    \"\"\"\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n    \"\"\"\n    if len(face_encodings) == 0:\n        return np.empty((0))\n\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)", "summary": "Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array", "name": "face_distance", "complexity": 2, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L63-L75", "code_lines": 11, "summary_length": 394}
{"code": "def aggregate(self, zeroValue, seqOp, combOp):\n        \"\"\"\n        Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given combine functions and a neutral \"zero\n        value.\"\n\n        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        The first function (seqOp) can return a different result type, U, than\n        the type of this RDD. Thus, we need one operation for merging a T into\n        an U and one operation for merging two U\n\n        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n        (10, 4)\n        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n        (0, 0)\n        \"\"\"\n        seqOp = fail_on_stopiteration(seqOp)\n        combOp = fail_on_stopiteration(combOp)\n\n        def func(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = seqOp(acc, obj)\n            yield acc\n        # collecting result of mapPartitions here ensures that the copy of\n        # zeroValue provided to each partition is unique from the one provided\n        # to the final reduce call\n        vals = self.mapPartitions(func).collect()\n        return reduce(combOp, vals, zeroValue)", "summary": "Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given combine functions and a neutral \"zero\n        value.\"\n\n        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        The first function (seqOp) can return a different result type, U, than\n        the type of this RDD. Thus, we need one operation for merging a T into\n        an U and one operation for merging two U\n\n        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n        (10, 4)\n        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n        (0, 0)", "name": "aggregate", "complexity": 2, "num_dependencies": 6, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L922-L955", "code_lines": 30, "summary_length": 838}
{"code": "def ignore_unicode_prefix(f):\n    \"\"\"\n    Ignore the 'u' prefix of string in doc tests, to make it works\n    in both python 2 and 3\n    \"\"\"\n    if sys.version >= '3':\n        # the representation of unicode string in Python 3 does not have prefix 'u',\n        # so remove the prefix 'u' for doc tests\n        literal_re = re.compile(r\"(\\W|^)[uU](['])\", re.UNICODE)\n        f.__doc__ = literal_re.sub(r'\\1\\2', f.__doc__)\n    return f", "summary": "Ignore the 'u' prefix of string in doc tests, to make it works\n    in both python 2 and 3", "name": "ignore_unicode_prefix", "complexity": 2, "num_dependencies": 2, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L150-L160", "code_lines": 11, "summary_length": 89}
{"code": "def _check_series_convert_timestamps_internal(s, timezone):\n    \"\"\"\n    Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for\n    Spark internal storage\n\n    :param s: a pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone\n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    from pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64_dtype(s.dtype):\n        # When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive\n        # timestamp is during the hour when the clock is adjusted backward during due to\n        # daylight saving time (dst).\n        # E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to\n        # 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize\n        # a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either\n        # dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).\n        #\n        # Here we explicit choose to use standard time. This matches the default behavior of\n        # pytz.\n        #\n        # Here are some code to help understand this behavior:\n        # >>> import datetime\n        # >>> import pandas as pd\n        # >>> import pytz\n        # >>>\n        # >>> t = datetime.datetime(2015, 11, 1, 1, 30)\n        # >>> ts = pd.Series([t])\n        # >>> tz = pytz.timezone('America/New_York')\n        # >>>\n        # >>> ts.dt.tz_localize(tz, ambiguous=True)\n        # 0   2015-11-01 01:30:00-04:00\n        # dtype: datetime64[ns, America/New_York]\n        # >>>\n        # >>> ts.dt.tz_localize(tz, ambiguous=False)\n        # 0   2015-11-01 01:30:00-05:00\n        # dtype: datetime64[ns, America/New_York]\n        # >>>\n        # >>> str(tz.localize(t))\n        # '2015-11-01 01:30:00-05:00'\n        tz = timezone or _get_local_timezone()\n        return s.dt.tz_localize(tz, ambiguous=False).dt.tz_convert('UTC')\n    elif is_datetime64tz_dtype(s.dtype):\n        return s.dt.tz_convert('UTC')\n    else:\n        return s", "summary": "Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for\n    Spark internal storage\n\n    :param s: a pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone", "name": "_check_series_convert_timestamps_internal", "complexity": 4, "num_dependencies": 6, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1739-L1789", "code_lines": 49, "summary_length": 328}
{"code": "def _check_series_localize_timestamps(s, timezone):\n    \"\"\"\n    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.\n\n    If the input series is not a timestamp series, then the same series is returned. If the input\n    series is a timestamp series, then a converted series is returned.\n\n    :param s: pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series that have been converted to tz-naive\n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    from pandas.api.types import is_datetime64tz_dtype\n    tz = timezone or _get_local_timezone()\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64tz_dtype(s.dtype):\n        return s.dt.tz_convert(tz).dt.tz_localize(None)\n    else:\n        return s", "summary": "Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.\n\n    If the input series is not a timestamp series, then the same series is returned. If the input\n    series is a timestamp series, then a converted series is returned.\n\n    :param s: pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series that have been converted to tz-naive", "name": "_check_series_localize_timestamps", "complexity": 3, "num_dependencies": 4, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1700-L1720", "code_lines": 18, "summary_length": 436}
{"code": "def _cachedSqlType(cls):\n        \"\"\"\n        Cache the sqlType() into class, because it's heavy used in `toInternal`.\n        \"\"\"\n        if not hasattr(cls, \"_cached_sql_type\"):\n            cls._cached_sql_type = cls.sqlType()\n        return cls._cached_sql_type", "summary": "Cache the sqlType() into class, because it's heavy used in `toInternal`.", "name": "_cachedSqlType", "complexity": 2, "num_dependencies": 2, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L675-L681", "code_lines": 7, "summary_length": 72}
{"code": "def foreach(self, f):\n        \"\"\"\n        Applies a function to all elements of this RDD.\n\n        >>> def f(x): print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n        \"\"\"\n        f = fail_on_stopiteration(f)\n\n        def processPartition(iterator):\n            for x in iterator:\n                f(x)\n            return iter([])\n        self.mapPartitions(processPartition).count()", "summary": "Applies a function to all elements of this RDD.\n\n        >>> def f(x): print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)", "name": "foreach", "complexity": 2, "num_dependencies": 6, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L778-L791", "code_lines": 12, "summary_length": 134}
{"code": "def evaluate(self, dataset):\n        \"\"\"\n        Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`\n        \"\"\"\n        if not isinstance(dataset, DataFrame):\n            raise ValueError(\"dataset must be a DataFrame but got %s.\" % type(dataset))\n        java_lr_summary = self._call_java(\"evaluate\", dataset)\n        return LinearRegressionSummary(java_lr_summary)", "summary": "Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`", "name": "evaluate", "complexity": 2, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L211-L222", "code_lines": 11, "summary_length": 184}
{"code": "def summary(self):\n        \"\"\"\n        Gets summary (e.g. residuals, mse, r-squared ) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.\n        \"\"\"\n        if self.hasSummary:\n            return LinearRegressionTrainingSummary(super(LinearRegressionModel, self).summary)\n        else:\n            raise RuntimeError(\"No training summary available for this %s\" %\n                               self.__class__.__name__)", "summary": "Gets summary (e.g. residuals, mse, r-squared ) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.", "name": "summary", "complexity": 2, "num_dependencies": 2, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L198-L208", "code_lines": 11, "summary_length": 141}
{"code": "def add(self, field, data_type=None, nullable=True, metadata=None):\n        \"\"\"\n        Construct a StructType by adding new elements to it to define the schema. The method accepts\n        either:\n\n            a) A single parameter which is a StructField object.\n            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n               metadata(optional). The data_type parameter may be either a String or a\n               DataType object.\n\n        >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\\\n        ...     StructField(\"f2\", StringType(), True, None)])\n        >>> struct1 == struct2\n        True\n        >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n        >>> struct1 == struct2\n        True\n        >>> struct1 = StructType().add(\"f1\", \"string\", True)\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n        >>> struct1 == struct2\n        True\n\n        :param field: Either the name of the field or a StructField object\n        :param data_type: If present, the DataType of the StructField to create\n        :param nullable: Whether the field to add should be nullable (default True)\n        :param metadata: Any additional metadata (default None)\n        :return: a new updated StructType\n        \"\"\"\n        if isinstance(field, StructField):\n            self.fields.append(field)\n            self.names.append(field.name)\n        else:\n            if isinstance(field, str) and data_type is None:\n                raise ValueError(\"Must specify DataType if passing name of struct_field to create.\")\n\n            if isinstance(data_type, str):\n                data_type_f = _parse_datatype_json_value(data_type)\n            else:\n                data_type_f = data_type\n            self.fields.append(StructField(field, data_type_f, nullable, metadata))\n            self.names.append(field)\n        # Precalculated list of fields that need conversion with fromInternal/toInternal functions\n        self._needConversion = [f.needConversion() for f in self]\n        self._needSerializeAnyField = any(self._needConversion)\n        return self", "summary": "Construct a StructType by adding new elements to it to define the schema. The method accepts\n        either:\n\n            a) A single parameter which is a StructField object.\n            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n               metadata(optional). The data_type parameter may be either a String or a\n               DataType object.\n\n        >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\\\n        ...     StructField(\"f2\", StringType(), True, None)])\n        >>> struct1 == struct2\n        True\n        >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n        >>> struct1 == struct2\n        True\n        >>> struct1 = StructType().add(\"f1\", \"string\", True)\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n        >>> struct1 == struct2\n        True\n\n        :param field: Either the name of the field or a StructField object\n        :param data_type: If present, the DataType of the StructField to create\n        :param nullable: Whether the field to add should be nullable (default True)\n        :param metadata: Any additional metadata (default None)\n        :return: a new updated StructType", "name": "add", "complexity": 5, "num_dependencies": 12, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L491-L537", "code_lines": 43, "summary_length": 1382}
{"code": "def evaluate(self, dataset):\n        \"\"\"\n        Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`\n        \"\"\"\n        if not isinstance(dataset, DataFrame):\n            raise ValueError(\"dataset must be a DataFrame but got %s.\" % type(dataset))\n        java_glr_summary = self._call_java(\"evaluate\", dataset)\n        return GeneralizedLinearRegressionSummary(java_glr_summary)", "summary": "Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`", "name": "evaluate", "complexity": 2, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L1693-L1704", "code_lines": 11, "summary_length": 184}
{"code": "def _parse_memory(s):\n    \"\"\"\n    Parse a memory string in the format supported by Java (e.g. 1g, 200m) and\n    return the value in MiB\n\n    >>> _parse_memory(\"256m\")\n    256\n    >>> _parse_memory(\"2g\")\n    2048\n    \"\"\"\n    units = {'g': 1024, 'm': 1, 't': 1 << 20, 'k': 1.0 / 1024}\n    if s[-1].lower() not in units:\n        raise ValueError(\"invalid format: \" + s)\n    return int(float(s[:-1]) * units[s[-1].lower()])", "summary": "Parse a memory string in the format supported by Java (e.g. 1g, 200m) and\n    return the value in MiB\n\n    >>> _parse_memory(\"256m\")\n    256\n    >>> _parse_memory(\"2g\")\n    2048", "name": "_parse_memory", "complexity": 2, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L125-L138", "code_lines": 13, "summary_length": 177}
{"code": "def _parse_datatype_string(s):\n    \"\"\"\n    Parses the given data type string to a :class:`DataType`. The data type string format equals\n    to :class:`DataType.simpleString`, except that top level struct type can omit\n    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead\n    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name\n    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted\n    string and case-insensitive strings.\n\n    >>> _parse_datatype_string(\"int \")\n    IntegerType\n    >>> _parse_datatype_string(\"INT \")\n    IntegerType\n    >>> _parse_datatype_string(\"a: byte, b: decimal(  16 , 8   ) \")\n    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))\n    >>> _parse_datatype_string(\"a DOUBLE, b STRING\")\n    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))\n    >>> _parse_datatype_string(\"a: array< short>\")\n    StructType(List(StructField(a,ArrayType(ShortType,true),true)))\n    >>> _parse_datatype_string(\" map<string , string > \")\n    MapType(StringType,StringType,true)\n\n    >>> # Error cases\n    >>> _parse_datatype_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    \"\"\"\n    sc = SparkContext._active_spark_context\n\n    def from_ddl_schema(type_str):\n        return _parse_datatype_json_string(\n            sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n\n    def from_ddl_datatype(type_str):\n        return _parse_datatype_json_string(\n            sc._jvm.org.apache.spark.sql.api.python.PythonSQLUtils.parseDataType(type_str).json())\n\n    try:\n        # DDL format, \"fieldname datatype, fieldname datatype\".\n        return from_ddl_schema(s)\n    except Exception as e:\n        try:\n            # For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.\n            return from_ddl_datatype(s)\n        except:\n            try:\n                # For backwards compatibility, \"fieldname: datatype, fieldname: datatype\" case.\n                return from_ddl_datatype(\"struct<%s>\" % s.strip())\n            except:\n                raise e", "summary": "Parses the given data type string to a :class:`DataType`. The data type string format equals\n    to :class:`DataType.simpleString`, except that top level struct type can omit\n    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead\n    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name\n    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted\n    string and case-insensitive strings.\n\n    >>> _parse_datatype_string(\"int \")\n    IntegerType\n    >>> _parse_datatype_string(\"INT \")\n    IntegerType\n    >>> _parse_datatype_string(\"a: byte, b: decimal(  16 , 8   ) \")\n    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))\n    >>> _parse_datatype_string(\"a DOUBLE, b STRING\")\n    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))\n    >>> _parse_datatype_string(\"a: array< short>\")\n    StructType(List(StructField(a,ArrayType(ShortType,true),true)))\n    >>> _parse_datatype_string(\" map<string , string > \")\n    MapType(StringType,StringType,true)\n\n    >>> # Error cases\n    >>> _parse_datatype_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...", "name": "_parse_datatype_string", "complexity": 4, "num_dependencies": 12, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L758-L820", "code_lines": 58, "summary_length": 1756}
{"code": "def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):\n        \"\"\"\n        Returns a sampling rate that guarantees a sample of\n        size >= sampleSizeLowerBound 99.99% of the time.\n\n        How the sampling rate is determined:\n        Let p = num / total, where num is the sample size and total is the\n        total number of data points in the RDD. We're trying to compute\n        q > p such that\n          - when sampling with replacement, we're drawing each data point\n            with prob_i ~ Pois(q), where we want to guarantee\n            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to\n            total), i.e. the failure rate of not having a sufficiently large\n            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient\n            to guarantee 0.9999 success rate for num > 12, but we need a\n            slightly larger q (9 empirically determined).\n          - when sampling without replacement, we're drawing each data point\n            with prob_i ~ Binomial(total, fraction) and our choice of q\n            guarantees 1-delta, or 0.9999 success rate, where success rate is\n            defined the same as in sampling with replacement.\n        \"\"\"\n        fraction = float(sampleSizeLowerBound) / total\n        if withReplacement:\n            numStDev = 5\n            if (sampleSizeLowerBound < 12):\n                numStDev = 9\n            return fraction + numStDev * sqrt(fraction / total)\n        else:\n            delta = 0.00005\n            gamma = - log(delta) / total\n            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))", "summary": "Returns a sampling rate that guarantees a sample of\n        size >= sampleSizeLowerBound 99.99% of the time.\n\n        How the sampling rate is determined:\n        Let p = num / total, where num is the sample size and total is the\n        total number of data points in the RDD. We're trying to compute\n        q > p such that\n          - when sampling with replacement, we're drawing each data point\n            with prob_i ~ Pois(q), where we want to guarantee\n            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to\n            total), i.e. the failure rate of not having a sufficiently large\n            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient\n            to guarantee 0.9999 success rate for num > 12, but we need a\n            slightly larger q (9 empirically determined).\n          - when sampling without replacement, we're drawing each data point\n            with prob_i ~ Binomial(total, fraction) and our choice of q\n            guarantees 1-delta, or 0.9999 success rate, where success rate is\n            defined the same as in sampling with replacement.", "name": "_computeFractionForSampleSize", "complexity": 3, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L521-L551", "code_lines": 30, "summary_length": 1103}
{"code": "def map(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD by applying a function to each element of this RDD.\n\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n        >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n        [('a', 1), ('b', 1), ('c', 1)]\n        \"\"\"\n        def func(_, iterator):\n            return map(fail_on_stopiteration(f), iterator)\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)", "summary": "Return a new RDD by applying a function to each element of this RDD.\n\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n        >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n        [('a', 1), ('b', 1), ('c', 1)]", "name": "map", "complexity": 1, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L317-L327", "code_lines": 10, "summary_length": 214}
{"code": "def histogram(self, buckets):\n        \"\"\"\n        Compute a histogram using the provided buckets. The buckets\n        are all open to the right except for the last which is closed.\n        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n        and 50 we would have a histogram of 1,0,1.\n\n        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n        this can be switched from an O(log n) inseration to O(1) per\n        element (where n is the number of buckets).\n\n        Buckets must be sorted, not contain any duplicates, and have\n        at least two elements.\n\n        If `buckets` is a number, it will generate buckets which are\n        evenly spaced between the minimum and maximum of the RDD. For\n        example, if the min value is 0 and the max is 100, given `buckets`\n        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n        be at least 1. An exception is raised if the RDD contains infinity.\n        If the elements in the RDD do not vary (max == min), a single bucket\n        will be used.\n\n        The return value is a tuple of buckets and histogram.\n\n        >>> rdd = sc.parallelize(range(51))\n        >>> rdd.histogram(2)\n        ([0, 25, 50], [25, 26])\n        >>> rdd.histogram([0, 5, 25, 50])\n        ([0, 5, 25, 50], [5, 20, 26])\n        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n        ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n        >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n        >>> rdd.histogram((\"a\", \"b\", \"c\"))\n        (('a', 'b', 'c'), [2, 2])\n        \"\"\"\n\n        if isinstance(buckets, int):\n            if buckets < 1:\n                raise ValueError(\"number of buckets must be >= 1\")\n\n            # filter out non-comparable elements\n            def comparable(x):\n                if x is None:\n                    return False\n                if type(x) is float and isnan(x):\n                    return False\n                return True\n\n            filtered = self.filter(comparable)\n\n            # faster than stats()\n            def minmax(a, b):\n                return min(a[0], b[0]), max(a[1], b[1])\n            try:\n                minv, maxv = filtered.map(lambda x: (x, x)).reduce(minmax)\n            except TypeError as e:\n                if \" empty \" in str(e):\n                    raise ValueError(\"can not generate buckets from empty RDD\")\n                raise\n\n            if minv == maxv or buckets == 1:\n                return [minv, maxv], [filtered.count()]\n\n            try:\n                inc = (maxv - minv) / buckets\n            except TypeError:\n                raise TypeError(\"Can not generate buckets with non-number in RDD\")\n\n            if isinf(inc):\n                raise ValueError(\"Can not generate buckets with infinite value\")\n\n            # keep them as integer if possible\n            inc = int(inc)\n            if inc * buckets != maxv - minv:\n                inc = (maxv - minv) * 1.0 / buckets\n\n            buckets = [i * inc + minv for i in range(buckets)]\n            buckets.append(maxv)  # fix accumulated error\n            even = True\n\n        elif isinstance(buckets, (list, tuple)):\n            if len(buckets) < 2:\n                raise ValueError(\"buckets should have more than one value\")\n\n            if any(i is None or isinstance(i, float) and isnan(i) for i in buckets):\n                raise ValueError(\"can not have None or NaN in buckets\")\n\n            if sorted(buckets) != list(buckets):\n                raise ValueError(\"buckets should be sorted\")\n\n            if len(set(buckets)) != len(buckets):\n                raise ValueError(\"buckets should not contain duplicated values\")\n\n            minv = buckets[0]\n            maxv = buckets[-1]\n            even = False\n            inc = None\n            try:\n                steps = [buckets[i + 1] - buckets[i] for i in range(len(buckets) - 1)]\n            except TypeError:\n                pass  # objects in buckets do not support '-'\n            else:\n                if max(steps) - min(steps) < 1e-10:  # handle precision errors\n                    even = True\n                    inc = (maxv - minv) / (len(buckets) - 1)\n\n        else:\n            raise TypeError(\"buckets should be a list or tuple or number(int or long)\")\n\n        def histogram(iterator):\n            counters = [0] * len(buckets)\n            for i in iterator:\n                if i is None or (type(i) is float and isnan(i)) or i > maxv or i < minv:\n                    continue\n                t = (int((i - minv) / inc) if even\n                     else bisect.bisect_right(buckets, i) - 1)\n                counters[t] += 1\n            # add last two together\n            last = counters.pop()\n            counters[-1] += last\n            return [counters]\n\n        def mergeCounters(a, b):\n            return [i + j for i, j in zip(a, b)]\n\n        return buckets, self.mapPartitions(histogram).reduce(mergeCounters)", "summary": "Compute a histogram using the provided buckets. The buckets\n        are all open to the right except for the last which is closed.\n        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n        and 50 we would have a histogram of 1,0,1.\n\n        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n        this can be switched from an O(log n) inseration to O(1) per\n        element (where n is the number of buckets).\n\n        Buckets must be sorted, not contain any duplicates, and have\n        at least two elements.\n\n        If `buckets` is a number, it will generate buckets which are\n        evenly spaced between the minimum and maximum of the RDD. For\n        example, if the min value is 0 and the max is 100, given `buckets`\n        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n        be at least 1. An exception is raised if the RDD contains infinity.\n        If the elements in the RDD do not vary (max == min), a single bucket\n        will be used.\n\n        The return value is a tuple of buckets and histogram.\n\n        >>> rdd = sc.parallelize(range(51))\n        >>> rdd.histogram(2)\n        ([0, 25, 50], [25, 26])\n        >>> rdd.histogram([0, 5, 25, 50])\n        ([0, 5, 25, 50], [5, 20, 26])\n        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n        ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n        >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n        >>> rdd.histogram((\"a\", \"b\", \"c\"))\n        (('a', 'b', 'c'), [2, 2])", "name": "histogram", "complexity": 28, "num_dependencies": 11, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1069-L1195", "code_lines": 104, "summary_length": 1590}
{"code": "def mapPartitions(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD by applying a function to each partition of this RDD.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        >>> rdd.mapPartitions(f).collect()\n        [3, 7]\n        \"\"\"\n        def func(s, iterator):\n            return f(iterator)\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)", "summary": "Return a new RDD by applying a function to each partition of this RDD.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        >>> rdd.mapPartitions(f).collect()\n        [3, 7]", "name": "mapPartitions", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L344-L355", "code_lines": 11, "summary_length": 228}
{"code": "def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,\n                       partitionFunc=portable_hash):\n        \"\"\"\n        Aggregate the values of each key, using given combine functions and a neutral\n        \"zero value\". This function can return a different result type, U, than the type\n        of the values in this RDD, V. Thus, we need one operation for merging a V into\n        a U and one operation for merging two U's, The former operation is used for merging\n        values within a partition, and the latter is used for merging values between\n        partitions. To avoid memory allocation, both of these functions are\n        allowed to modify and return their first argument instead of creating a new U.\n        \"\"\"\n        def createZero():\n            return copy.deepcopy(zeroValue)\n\n        return self.combineByKey(\n            lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)", "summary": "Aggregate the values of each key, using given combine functions and a neutral\n        \"zero value\". This function can return a different result type, U, than the type\n        of the values in this RDD, V. Thus, we need one operation for merging a V into\n        a U and one operation for merging two U's, The former operation is used for merging\n        values within a partition, and the latter is used for merging values between\n        partitions. To avoid memory allocation, both of these functions are\n        allowed to modify and return their first argument instead of creating a new U.", "name": "aggregateByKey", "complexity": 1, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1876-L1891", "code_lines": 15, "summary_length": 593}
{"code": "def _check_dataframe_localize_timestamps(pdf, timezone):\n    \"\"\"\n    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone\n\n    :param pdf: pandas.DataFrame\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive\n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    for column, series in pdf.iteritems():\n        pdf[column] = _check_series_localize_timestamps(series, timezone)\n    return pdf", "summary": "Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone\n\n    :param pdf: pandas.DataFrame\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive", "name": "_check_dataframe_localize_timestamps", "complexity": 2, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1723-L1736", "code_lines": 12, "summary_length": 301}
{"code": "def _external_items(self):\n        \"\"\" Return all partitioned items as iterator \"\"\"\n        assert not self.data\n        if any(self.pdata):\n            self._spill()\n        # disable partitioning and spilling when merge combiners from disk\n        self.pdata = []\n\n        try:\n            for i in range(self.partitions):\n                for v in self._merged_items(i):\n                    yield v\n                self.data.clear()\n\n                # remove the merged partition\n                for j in range(self.spills):\n                    path = self._get_spill_dir(j)\n                    os.remove(os.path.join(path, str(i)))\n        finally:\n            self._cleanup()", "summary": "Return all partitioned items as iterator", "name": "_external_items", "complexity": 5, "num_dependencies": 11, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L345-L364", "code_lines": 18, "summary_length": 40}
{"code": "def persist(self, storageLevel=StorageLevel.MEMORY_ONLY):\n        \"\"\"\n        Set this RDD's storage level to persist its values across operations\n        after the first time it is computed. This can only be used to assign\n        a new storage level if the RDD does not have a storage level set yet.\n        If no storage level is specified defaults to (C{MEMORY_ONLY}).\n\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n        >>> rdd.persist().is_cached\n        True\n        \"\"\"\n        self.is_cached = True\n        javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)\n        self._jrdd.persist(javaStorageLevel)\n        return self", "summary": "Set this RDD's storage level to persist its values across operations\n        after the first time it is computed. This can only be used to assign\n        a new storage level if the RDD does not have a storage level set yet.\n        If no storage level is specified defaults to (C{MEMORY_ONLY}).\n\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n        >>> rdd.persist().is_cached\n        True", "name": "persist", "complexity": 1, "num_dependencies": 2, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L231-L245", "code_lines": 14, "summary_length": 394}
{"code": "def _merge_sorted_items(self, index):\n        \"\"\" load a partition from disk, then sort and group by key \"\"\"\n        def load_partition(j):\n            path = self._get_spill_dir(j)\n            p = os.path.join(path, str(index))\n            with open(p, 'rb', 65536) as f:\n                for v in self.serializer.load_stream(f):\n                    yield v\n\n        disk_items = [load_partition(j) for j in range(self.spills)]\n\n        if self._sorted:\n            # all the partitions are already sorted\n            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))\n\n        else:\n            # Flatten the combined values, so it will not consume huge\n            # memory during merging sort.\n            ser = self.flattened_serializer()\n            sorter = ExternalSorter(self.memory_limit, ser)\n            sorted_items = sorter.sorted(itertools.chain(*disk_items),\n                                         key=operator.itemgetter(0))\n        return ((k, vs) for k, vs in GroupByKey(sorted_items))", "summary": "load a partition from disk, then sort and group by key", "name": "_merge_sorted_items", "complexity": 3, "num_dependencies": 20, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L786-L808", "code_lines": 20, "summary_length": 54}
{"code": "def getCheckpointFile(self):\n        \"\"\"\n        Gets the name of the file to which this RDD was checkpointed\n\n        Not defined if RDD is checkpointed locally.\n        \"\"\"\n        checkpointFile = self._jrdd.rdd().getCheckpointFile()\n        if checkpointFile.isDefined():\n            return checkpointFile.get()", "summary": "Gets the name of the file to which this RDD was checkpointed\n\n        Not defined if RDD is checkpointed locally.", "name": "getCheckpointFile", "complexity": 2, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L307-L315", "code_lines": 8, "summary_length": 113}
{"code": "def treeReduce(self, f, depth=2):\n        \"\"\"\n        Reduces the elements of this RDD in a multi-level tree pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeReduce(add)\n        -5\n        >>> rdd.treeReduce(add, 1)\n        -5\n        >>> rdd.treeReduce(add, 2)\n        -5\n        >>> rdd.treeReduce(add, 5)\n        -5\n        >>> rdd.treeReduce(add, 10)\n        -5\n        \"\"\"\n        if depth < 1:\n            raise ValueError(\"Depth cannot be smaller than 1 but got %d.\" % depth)\n\n        zeroValue = None, True  # Use the second entry to indicate whether this is a dummy value.\n\n        def op(x, y):\n            if x[1]:\n                return y\n            elif y[1]:\n                return x\n            else:\n                return f(x[0], y[0]), False\n\n        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)\n        if reduced[1]:\n            raise ValueError(\"Cannot reduce empty RDD.\")\n        return reduced[0]", "summary": "Reduces the elements of this RDD in a multi-level tree pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeReduce(add)\n        -5\n        >>> rdd.treeReduce(add, 1)\n        -5\n        >>> rdd.treeReduce(add, 2)\n        -5\n        >>> rdd.treeReduce(add, 5)\n        -5\n        >>> rdd.treeReduce(add, 10)\n        -5", "name": "treeReduce", "complexity": 5, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L851-L886", "code_lines": 31, "summary_length": 464}
{"code": "def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\n                     numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        Generic function to combine the elements for each key using a custom\n        set of aggregation functions.\n\n        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n        type\" C.\n\n        Users provide three functions:\n\n            - C{createCombiner}, which turns a V into a C (e.g., creates\n              a one-element list)\n            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n              a list)\n            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges\n              the lists)\n\n        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n        modify and return their first argument instead of creating a new C.\n\n        In addition, users can control the partitioning of the output RDD.\n\n        .. note:: V and C can be different -- for example, one might group an RDD of type\n            (Int, Int) into an RDD of type (Int, List[Int]).\n\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n        >>> def to_list(a):\n        ...     return [a]\n        ...\n        >>> def append(a, b):\n        ...     a.append(b)\n        ...     return a\n        ...\n        >>> def extend(a, b):\n        ...     a.extend(b)\n        ...     return a\n        ...\n        >>> sorted(x.combineByKey(to_list, append, extend).collect())\n        [('a', [1, 2]), ('b', [1])]\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        serializer = self.ctx.serializer\n        memory = self._memory_limit()\n        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)\n\n        def combineLocally(iterator):\n            merger = ExternalMerger(agg, memory * 0.9, serializer)\n            merger.mergeValues(iterator)\n            return merger.items()\n\n        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)\n        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)\n\n        def _mergeCombiners(iterator):\n            merger = ExternalMerger(agg, memory, serializer)\n            merger.mergeCombiners(iterator)\n            return merger.items()\n\n        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)", "summary": "Generic function to combine the elements for each key using a custom\n        set of aggregation functions.\n\n        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n        type\" C.\n\n        Users provide three functions:\n\n            - C{createCombiner}, which turns a V into a C (e.g., creates\n              a one-element list)\n            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n              a list)\n            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges\n              the lists)\n\n        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n        modify and return their first argument instead of creating a new C.\n\n        In addition, users can control the partitioning of the output RDD.\n\n        .. note:: V and C can be different -- for example, one might group an RDD of type\n            (Int, Int) into an RDD of type (Int, List[Int]).\n\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n        >>> def to_list(a):\n        ...     return [a]\n        ...\n        >>> def append(a, b):\n        ...     a.append(b)\n        ...     return a\n        ...\n        >>> def extend(a, b):\n        ...     a.extend(b)\n        ...     return a\n        ...\n        >>> sorted(x.combineByKey(to_list, append, extend).collect())\n        [('a', [1, 2]), ('b', [1])]", "name": "combineByKey", "complexity": 2, "num_dependencies": 18, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1813-L1874", "code_lines": 50, "summary_length": 1381}
{"code": "def summary(self):\n        \"\"\"\n        Gets summary (e.g. residuals, deviance, pValues) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.\n        \"\"\"\n        if self.hasSummary:\n            return GeneralizedLinearRegressionTrainingSummary(\n                super(GeneralizedLinearRegressionModel, self).summary)\n        else:\n            raise RuntimeError(\"No training summary available for this %s\" %\n                               self.__class__.__name__)", "summary": "Gets summary (e.g. residuals, deviance, pValues) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.", "name": "summary", "complexity": 2, "num_dependencies": 2, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L1679-L1690", "code_lines": 12, "summary_length": 143}
{"code": "def to_arrow_type(dt):\n    \"\"\" Convert Spark data type to pyarrow type\n    \"\"\"\n    import pyarrow as pa\n    if type(dt) == BooleanType:\n        arrow_type = pa.bool_()\n    elif type(dt) == ByteType:\n        arrow_type = pa.int8()\n    elif type(dt) == ShortType:\n        arrow_type = pa.int16()\n    elif type(dt) == IntegerType:\n        arrow_type = pa.int32()\n    elif type(dt) == LongType:\n        arrow_type = pa.int64()\n    elif type(dt) == FloatType:\n        arrow_type = pa.float32()\n    elif type(dt) == DoubleType:\n        arrow_type = pa.float64()\n    elif type(dt) == DecimalType:\n        arrow_type = pa.decimal128(dt.precision, dt.scale)\n    elif type(dt) == StringType:\n        arrow_type = pa.string()\n    elif type(dt) == BinaryType:\n        arrow_type = pa.binary()\n    elif type(dt) == DateType:\n        arrow_type = pa.date32()\n    elif type(dt) == TimestampType:\n        # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read\n        arrow_type = pa.timestamp('us', tz='UTC')\n    elif type(dt) == ArrayType:\n        if type(dt.elementType) in [StructType, TimestampType]:\n            raise TypeError(\"Unsupported type in conversion to Arrow: \" + str(dt))\n        arrow_type = pa.list_(to_arrow_type(dt.elementType))\n    elif type(dt) == StructType:\n        if any(type(field.dataType) == StructType for field in dt):\n            raise TypeError(\"Nested StructType not supported in conversion to Arrow\")\n        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)\n                  for field in dt]\n        arrow_type = pa.struct(fields)\n    else:\n        raise TypeError(\"Unsupported type in conversion to Arrow: \" + str(dt))\n    return arrow_type", "summary": "Convert Spark data type to pyarrow type", "name": "to_arrow_type", "complexity": 17, "num_dependencies": 39, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1581-L1622", "code_lines": 42, "summary_length": 39}
{"code": "def max(self, key=None):\n        \"\"\"\n        Find the maximum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n        >>> rdd.max()\n        43.0\n        >>> rdd.max(key=str)\n        5.0\n        \"\"\"\n        if key is None:\n            return self.reduce(max)\n        return self.reduce(lambda a, b: max(a, b, key=key))", "summary": "Find the maximum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n        >>> rdd.max()\n        43.0\n        >>> rdd.max(key=str)\n        5.0", "name": "max", "complexity": 2, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1009-L1023", "code_lines": 13, "summary_length": 235}
{"code": "def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n        \"\"\"\n        Sorts this RDD, which is assumed to consist of (key, value) pairs.\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortByKey().first()\n        ('1', 3)\n        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        memory = self._memory_limit()\n        serializer = self._jrdd_deserializer\n\n        def sortPartition(iterator):\n            sort = ExternalSorter(memory * 0.9, serializer).sorted\n            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n\n        if numPartitions == 1:\n            if self.getNumPartitions() > 1:\n                self = self.coalesce(1)\n            return self.mapPartitions(sortPartition, True)\n\n        # first compute the boundary of each part via sampling: we want to partition\n        # the key-space into bins such that the bins have roughly the same\n        # number of (key, value) pairs falling into them\n        rddSize = self.count()\n        if not rddSize:\n            return self  # empty RDD\n        maxSampleSize = numPartitions * 20.0  # constant from Spark's RangePartitioner\n        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)\n        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()\n        samples = sorted(samples, key=keyfunc)\n\n        # we have numPartitions many parts but one of the them has\n        # an implicit boundary\n        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)]\n                  for i in range(0, numPartitions - 1)]\n\n        def rangePartitioner(k):\n            p = bisect.bisect_left(bounds, keyfunc(k))\n            if ascending:\n                return p\n            else:\n                return numPartitions - 1 - p\n\n        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)", "summary": "Sorts this RDD, which is assumed to consist of (key, value) pairs.\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortByKey().first()\n        ('1', 3)\n        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]", "name": "sortByKey", "complexity": 6, "num_dependencies": 24, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L635-L689", "code_lines": 47, "summary_length": 789}
{"code": "def randomSplit(self, weights, seed=None):\n        \"\"\"\n        Randomly splits this RDD with the provided weights.\n\n        :param weights: weights for splits, will be normalized if they don't sum to 1\n        :param seed: random seed\n        :return: split RDDs in a list\n\n        >>> rdd = sc.parallelize(range(500), 1)\n        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n        >>> len(rdd1.collect() + rdd2.collect())\n        500\n        >>> 150 < rdd1.count() < 250\n        True\n        >>> 250 < rdd2.count() < 350\n        True\n        \"\"\"\n        s = float(sum(weights))\n        cweights = [0.0]\n        for w in weights:\n            cweights.append(cweights[-1] + w / s)\n        if seed is None:\n            seed = random.randint(0, 2 ** 32 - 1)\n        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)\n                for lb, ub in zip(cweights, cweights[1:])]", "summary": "Randomly splits this RDD with the provided weights.\n\n        :param weights: weights for splits, will be normalized if they don't sum to 1\n        :param seed: random seed\n        :return: split RDDs in a list\n\n        >>> rdd = sc.parallelize(range(500), 1)\n        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n        >>> len(rdd1.collect() + rdd2.collect())\n        500\n        >>> 150 < rdd1.count() < 250\n        True\n        >>> 250 < rdd2.count() < 350\n        True", "name": "randomSplit", "complexity": 3, "num_dependencies": 6, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L438-L462", "code_lines": 23, "summary_length": 472}
{"code": "def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n    \"\"\"\n    Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n         <person1>/\n            <somename1>.jpeg\n            <somename2>.jpeg\n            ...\n         <person2>/\n            <somename1>.jpeg\n            <somename2>.jpeg\n         ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.\n    \"\"\"\n    X = []\n    y = []\n\n    # Loop through each person in the training set\n    for class_dir in os.listdir(train_dir):\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n            continue\n\n        # Loop through each training image for the current person\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n            image = face_recognition.load_image_file(img_path)\n            face_bounding_boxes = face_recognition.face_locations(image)\n\n            if len(face_bounding_boxes) != 1:\n                # If there are no people (or too many people) in a training image, skip the image.\n                if verbose:\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n            else:\n                # Add face encoding for current image to the training set\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n                y.append(class_dir)\n\n    # Determine how many neighbors to use for weighting in the KNN classifier\n    if n_neighbors is None:\n        n_neighbors = int(round(math.sqrt(len(X))))\n        if verbose:\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\n\n    # Create and train the KNN classifier\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n    knn_clf.fit(X, y)\n\n    # Save the trained KNN classifier\n    if model_save_path is not None:\n        with open(model_save_path, 'wb') as f:\n            pickle.dump(knn_clf, f)\n\n    return knn_clf", "summary": "Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n         <person1>/\n            <somename1>.jpeg\n            <somename2>.jpeg\n            ...\n         <person2>/\n            <somename1>.jpeg\n            <somename2>.jpeg\n         ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.", "name": "train", "complexity": 9, "num_dependencies": 23, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L46-L108", "code_lines": 52, "summary_length": 878}
{"code": "def load_image_file(file, mode='RGB'):\n    \"\"\"\n    Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array\n    \"\"\"\n    im = PIL.Image.open(file)\n    if mode:\n        im = im.convert(mode)\n    return np.array(im)", "summary": "Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array", "name": "load_image_file", "complexity": 2, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L78-L89", "code_lines": 11, "summary_length": 281}
{"code": "def face_landmarks(face_image, face_locations=None, model=\"large\"):\n    \"\"\"\n    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)\n    \"\"\"\n    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n\n    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n    if model == 'large':\n        return [{\n            \"chin\": points[0:17],\n            \"left_eyebrow\": points[17:22],\n            \"right_eyebrow\": points[22:27],\n            \"nose_bridge\": points[27:31],\n            \"nose_tip\": points[31:36],\n            \"left_eye\": points[36:42],\n            \"right_eye\": points[42:48],\n            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n        } for points in landmarks_as_tuples]\n    elif model == 'small':\n        return [{\n            \"nose_tip\": [points[4]],\n            \"left_eye\": points[2:4],\n            \"right_eye\": points[0:2],\n        } for points in landmarks_as_tuples]\n    else:\n        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")", "summary": "Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)", "name": "face_landmarks", "complexity": 3, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L168-L200", "code_lines": 31, "summary_length": 416}
{"code": "def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):\n        \"\"\"\n        Aggregates the elements of this RDD in a multi-level tree\n        pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeAggregate(0, add, add)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 1)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 2)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 5)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 10)\n        -5\n        \"\"\"\n        if depth < 1:\n            raise ValueError(\"Depth cannot be smaller than 1 but got %d.\" % depth)\n\n        if self.getNumPartitions() == 0:\n            return zeroValue\n\n        def aggregatePartition(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = seqOp(acc, obj)\n            yield acc\n\n        partiallyAggregated = self.mapPartitions(aggregatePartition)\n        numPartitions = partiallyAggregated.getNumPartitions()\n        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)\n        # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree\n        # aggregation.\n        while numPartitions > scale + numPartitions / scale:\n            numPartitions /= scale\n            curNumPartitions = int(numPartitions)\n\n            def mapPartition(i, iterator):\n                for obj in iterator:\n                    yield (i % curNumPartitions, obj)\n\n            partiallyAggregated = partiallyAggregated \\\n                .mapPartitionsWithIndex(mapPartition) \\\n                .reduceByKey(combOp, curNumPartitions) \\\n                .values()\n\n        return partiallyAggregated.reduce(combOp)", "summary": "Aggregates the elements of this RDD in a multi-level tree\n        pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeAggregate(0, add, add)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 1)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 2)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 5)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 10)\n        -5", "name": "treeAggregate", "complexity": 6, "num_dependencies": 13, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L957-L1007", "code_lines": 43, "summary_length": 530}
{"code": "def saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,\n                               keyConverter=None, valueConverter=None, conf=None):\n        \"\"\"\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n        will be inferred if not specified. Keys and values are converted for output using either\n        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n\n        :param path: path to Hadoop file\n        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n               (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n        :param keyClass: fully qualified classname of key Writable class\n               (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n        :param valueClass: fully qualified classname of value Writable class\n               (e.g. \"org.apache.hadoop.io.Text\", None by default)\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)\n        :param conf: Hadoop job configuration, passed in as a dict (None by default)\n        \"\"\"\n        jconf = self.ctx._dictToJavaMap(conf)\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path,\n                                                       outputFormatClass,\n                                                       keyClass, valueClass,\n                                                       keyConverter, valueConverter, jconf)", "summary": "Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n        will be inferred if not specified. Keys and values are converted for output using either\n        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n\n        :param path: path to Hadoop file\n        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n               (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n        :param keyClass: fully qualified classname of key Writable class\n               (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n        :param valueClass: fully qualified classname of value Writable class\n               (e.g. \"org.apache.hadoop.io.Text\", None by default)\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)\n        :param conf: Hadoop job configuration, passed in as a dict (None by default)", "name": "saveAsNewAPIHadoopFile", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1414-L1440", "code_lines": 26, "summary_length": 1242}
{"code": "def sum(self):\n        \"\"\"\n        Add up the elements in this RDD.\n\n        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n        6.0\n        \"\"\"\n        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)", "summary": "Add up the elements in this RDD.\n\n        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n        6.0", "name": "sum", "complexity": 1, "num_dependencies": 1, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1041-L1048", "code_lines": 7, "summary_length": 95}
{"code": "def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,\n                                           ascending=True, keyfunc=lambda x: x):\n        \"\"\"\n        Repartition the RDD according to the given partitioner and, within each resulting partition,\n        sort records by their keys.\n\n        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n        >>> rdd2.glom().collect()\n        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        memory = _parse_memory(self.ctx._conf.get(\"spark.python.worker.memory\", \"512m\"))\n        serializer = self._jrdd_deserializer\n\n        def sortPartition(iterator):\n            sort = ExternalSorter(memory * 0.9, serializer).sorted\n            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))\n\n        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)", "summary": "Repartition the RDD according to the given partitioner and, within each resulting partition,\n        sort records by their keys.\n\n        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n        >>> rdd2.glom().collect()\n        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]", "name": "repartitionAndSortWithinPartitions", "complexity": 2, "num_dependencies": 10, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L612-L633", "code_lines": 18, "summary_length": 391}
{"code": "def top(self, num, key=None):\n        \"\"\"\n        Get the top N elements from an RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        .. note:: It returns the list sorted in descending order.\n\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n        [12]\n        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n        [6, 5]\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n        [4, 3, 2]\n        \"\"\"\n        def topIterator(iterator):\n            yield heapq.nlargest(num, iterator, key=key)\n\n        def merge(a, b):\n            return heapq.nlargest(num, a + b, key=key)\n\n        return self.mapPartitions(topIterator).reduce(merge)", "summary": "Get the top N elements from an RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        .. note:: It returns the list sorted in descending order.\n\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n        [12]\n        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n        [6, 5]\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n        [4, 3, 2]", "name": "top", "complexity": 1, "num_dependencies": 5, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1265-L1287", "code_lines": 18, "summary_length": 481}
{"code": "def _has_nulltype(dt):\n    \"\"\" Return whether there is NullType in `dt` or not \"\"\"\n    if isinstance(dt, StructType):\n        return any(_has_nulltype(f.dataType) for f in dt.fields)\n    elif isinstance(dt, ArrayType):\n        return _has_nulltype((dt.elementType))\n    elif isinstance(dt, MapType):\n        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)\n    else:\n        return isinstance(dt, NullType)", "summary": "Return whether there is NullType in `dt` or not", "name": "_has_nulltype", "complexity": 5, "num_dependencies": 9, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1068-L1077", "code_lines": 10, "summary_length": 47}
{"code": "def pipe(self, command, env=None, checkCode=False):\n        \"\"\"\n        Return an RDD created by piping elements to a forked external process.\n\n        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n        [u'1', u'2', u'', u'3']\n\n        :param checkCode: whether or not to check the return value of the shell command.\n        \"\"\"\n        if env is None:\n            env = dict()\n\n        def func(iterator):\n            pipe = Popen(\n                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)\n\n            def pipe_objs(out):\n                for obj in iterator:\n                    s = unicode(obj).rstrip('\\n') + '\\n'\n                    out.write(s.encode('utf-8'))\n                out.close()\n            Thread(target=pipe_objs, args=[pipe.stdin]).start()\n\n            def check_return_code():\n                pipe.wait()\n                if checkCode and pipe.returncode:\n                    raise Exception(\"Pipe function `%s' exited \"\n                                    \"with error code %d\" % (command, pipe.returncode))\n                else:\n                    for i in range(0):\n                        yield i\n            return (x.rstrip(b'\\n').decode('utf-8') for x in\n                    chain(iter(pipe.stdout.readline, b''), check_return_code()))\n        return self.mapPartitions(func)", "summary": "Return an RDD created by piping elements to a forked external process.\n\n        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n        [u'1', u'2', u'', u'3']\n\n        :param checkCode: whether or not to check the return value of the shell command.", "name": "pipe", "complexity": 6, "num_dependencies": 37, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L743-L776", "code_lines": 29, "summary_length": 263}
{"code": "def distinct(self, numPartitions=None):\n        \"\"\"\n        Return a new RDD containing the distinct elements in this RDD.\n\n        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n        [1, 2, 3]\n        \"\"\"\n        return self.map(lambda x: (x, None)) \\\n                   .reduceByKey(lambda x, _: x, numPartitions) \\\n                   .map(lambda x: x[0])", "summary": "Return a new RDD containing the distinct elements in this RDD.\n\n        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n        [1, 2, 3]", "name": "distinct", "complexity": 1, "num_dependencies": 1, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L407-L416", "code_lines": 9, "summary_length": 151}
{"code": "def cartesian(self, other):\n        \"\"\"\n        Return the Cartesian product of this RDD and another one, that is, the\n        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n        C{b} is in C{other}.\n\n        >>> rdd = sc.parallelize([1, 2])\n        >>> sorted(rdd.cartesian(rdd).collect())\n        [(1, 1), (1, 2), (2, 1), (2, 2)]\n        \"\"\"\n        # Due to batching, we can't use the Java cartesian method.\n        deserializer = CartesianDeserializer(self._jrdd_deserializer,\n                                             other._jrdd_deserializer)\n        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)", "summary": "Return the Cartesian product of this RDD and another one, that is, the\n        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n        C{b} is in C{other}.\n\n        >>> rdd = sc.parallelize([1, 2])\n        >>> sorted(rdd.cartesian(rdd).collect())\n        [(1, 1), (1, 2), (2, 1), (2, 2)]", "name": "cartesian", "complexity": 1, "num_dependencies": 3, "language": "python", "source_url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L716-L729", "code_lines": 13, "summary_length": 307}
